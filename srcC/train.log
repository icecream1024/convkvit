[2023-08-08 15:55:11,780][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2023-08-08 15:55:11,786][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2023-08-08 15:55:11,877][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2023-08-08 15:55:11,890][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2023-08-08 15:55:11,906][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 92, in _call_target
    return _target_(*args, **kwargs)
TypeError: __init__() missing 1 required positional argument: 'num_classes'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 53, in train
    model: LightningModule = hydra.utils.instantiate(cfg.model)
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 226, in instantiate
    return instantiate_node(
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 342, in instantiate_node
    value = instantiate_node(
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 347, in instantiate_node
    return _call_target(_target_, partial, args, kwargs, full_key)
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 97, in _call_target
    raise InstantiationException(msg) from e
hydra.errors.InstantiationException: Error in call to target 'srcC.models.components.net.network':
TypeError("__init__() missing 1 required positional argument: 'num_classes'")
full_key: model.net
[2023-08-08 15:55:11,908][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2023-08-08_15-55-11
[2023-08-08 15:58:37,888][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2023-08-08 15:58:37,894][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2023-08-08 15:58:37,979][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2023-08-08 15:58:37,985][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2023-08-08 15:58:38,007][__main__][INFO] - Instantiating callbacks...
[2023-08-08 15:58:38,008][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2023-08-08 15:58:38,013][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2023-08-08 15:58:38,015][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2023-08-08 15:58:38,016][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2023-08-08 15:58:38,018][__main__][INFO] - Instantiating loggers...
[2023-08-08 15:58:38,019][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2023-08-08 15:59:00,699][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2023-08-08 15:59:01,612][__main__][INFO] - Logging hyperparameters!
[2023-08-08 15:59:01,615][__main__][INFO] - Starting training!
[2023-08-08 16:00:09,805][__main__][INFO] - Starting testing!
[2023-08-08 16:00:17,241][__main__][INFO] - Best ckpt path: D:\data\CIFAR10_logs\logs\train\runs\2023-08-08_15-58-37\checkpoints\epoch_001.ckpt
[2023-08-08 16:00:17,243][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2023-08-08_15-58-37
[2023-08-08 16:00:17,243][src.utils.utils][INFO] - Closing wandb!
[2023-08-08 16:00:24,415][src.utils.utils][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-01-30 11:29:43,701][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-01-30 11:29:43,708][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-01-30 11:29:43,779][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-01-30 11:29:43,784][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-01-30 11:29:43,808][__main__][INFO] - Instantiating callbacks...
[2024-01-30 11:29:43,808][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-01-30 11:29:43,812][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-01-30 11:29:43,813][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-01-30 11:29:43,813][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-01-30 11:29:43,816][__main__][INFO] - Instantiating loggers...
[2024-01-30 11:29:43,816][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-01-30 11:30:13,229][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-01-30 11:30:13,819][__main__][INFO] - Logging hyperparameters!
[2024-01-30 11:30:13,822][__main__][INFO] - Starting training!
[2024-01-30 11:32:34,831][__main__][INFO] - Starting testing!
[2024-01-30 11:32:40,004][__main__][INFO] - Best ckpt path: D:\data\CIFAR10_logs\logs\train\runs\2024-01-30_11-29-43\checkpoints\epoch_005.ckpt
[2024-01-30 11:32:40,005][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-01-30_11-29-43
[2024-01-30 11:32:40,005][src.utils.utils][INFO] - Closing wandb!
[2024-01-30 11:32:50,557][src.utils.utils][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-01-30 11:33:03,914][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-01-30 11:33:03,921][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-01-30 11:33:03,991][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-01-30 11:33:03,995][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-01-30 11:33:04,011][__main__][INFO] - Instantiating callbacks...
[2024-01-30 11:33:04,011][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-01-30 11:33:04,014][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-01-30 11:33:04,015][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-01-30 11:33:04,016][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-01-30 11:33:04,017][__main__][INFO] - Instantiating loggers...
[2024-01-30 11:33:04,018][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-01-30 11:33:31,687][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-01-30 11:33:32,265][__main__][INFO] - Logging hyperparameters!
[2024-01-30 11:33:32,268][__main__][INFO] - Starting training!
[2024-01-30 11:35:14,674][__main__][INFO] - Starting testing!
[2024-01-30 11:35:19,645][__main__][INFO] - Best ckpt path: D:\data\CIFAR10_logs\logs\train\runs\2024-01-30_11-33-03\checkpoints\epoch_001.ckpt
[2024-01-30 11:35:19,647][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-01-30_11-33-03
[2024-01-30 11:35:19,647][src.utils.utils][INFO] - Closing wandb!
[2024-01-30 11:35:30,647][src.utils.utils][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-01-30 13:17:51,208][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-01-30 13:17:51,215][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-01-30 13:17:51,291][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-01-30 13:17:51,297][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-01-30 13:17:51,354][__main__][INFO] - Instantiating callbacks...
[2024-01-30 13:17:51,354][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-01-30 13:17:51,357][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-01-30 13:17:51,359][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-01-30 13:17:51,360][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-01-30 13:17:51,361][__main__][INFO] - Instantiating loggers...
[2024-01-30 13:17:51,362][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-01-30 13:18:29,163][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-01-30 13:18:29,859][__main__][INFO] - Logging hyperparameters!
[2024-01-30 13:18:29,863][__main__][INFO] - Starting training!
[2024-01-30 13:18:38,118][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 949, in _run
    self.strategy.setup(self)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\single_device.py", line 75, in setup
    super().setup(trainer)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 149, in setup
    self.setup_optimizers(trainer)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 139, in setup_optimizers
    self.optimizers, self.lr_scheduler_configs = _init_optimizers_and_lr_schedulers(self.lightning_module)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\optimizer.py", line 168, in _init_optimizers_and_lr_schedulers
    optim_conf = call._call_lightning_module_hook(model.trainer, "configure_optimizers", pl_module=model)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 144, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 188, in configure_optimizers
    optimizer = self.hparams.optimizer(params=self.parameters())
TypeError: __init__() got an unexpected keyword argument 'lr'
[2024-01-30 13:18:38,124][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-01-30_13-17-51
[2024-01-30 13:18:38,125][src.utils.utils][INFO] - Closing wandb!
[2024-01-30 13:20:40,237][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-01-30 13:20:40,248][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-01-30 13:20:40,368][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-01-30 13:20:40,374][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-01-30 13:20:40,399][__main__][INFO] - Instantiating callbacks...
[2024-01-30 13:20:40,400][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-01-30 13:20:40,409][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-01-30 13:20:40,412][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-01-30 13:20:40,413][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-01-30 13:20:40,415][__main__][INFO] - Instantiating loggers...
[2024-01-30 13:20:40,416][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-01-30 13:21:09,612][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-01-30 13:21:10,239][__main__][INFO] - Logging hyperparameters!
[2024-01-30 13:21:10,242][__main__][INFO] - Starting training!
[2024-01-30 13:21:15,012][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 949, in _run
    self.strategy.setup(self)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\single_device.py", line 75, in setup
    super().setup(trainer)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 149, in setup
    self.setup_optimizers(trainer)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 139, in setup_optimizers
    self.optimizers, self.lr_scheduler_configs = _init_optimizers_and_lr_schedulers(self.lightning_module)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\optimizer.py", line 168, in _init_optimizers_and_lr_schedulers
    optim_conf = call._call_lightning_module_hook(model.trainer, "configure_optimizers", pl_module=model)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 144, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 188, in configure_optimizers
    optimizer = self.hparams.optimizer(params=self.parameters())
TypeError: __init__() got an unexpected keyword argument 'lr'
[2024-01-30 13:21:15,016][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-01-30_13-20-40
[2024-01-30 13:21:15,017][src.utils.utils][INFO] - Closing wandb!
[2024-01-30 13:22:01,870][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-01-30 13:22:01,881][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-01-30 13:22:02,071][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-01-30 13:22:02,090][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-01-30 13:22:02,134][__main__][INFO] - Instantiating callbacks...
[2024-01-30 13:22:02,136][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-01-30 13:22:02,145][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-01-30 13:22:02,150][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-01-30 13:22:02,151][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-01-30 13:22:02,157][__main__][INFO] - Instantiating loggers...
[2024-01-30 13:22:02,158][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-01-30 13:22:35,140][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-01-30 13:22:35,655][__main__][INFO] - Logging hyperparameters!
[2024-01-30 13:22:35,659][__main__][INFO] - Starting training!
[2024-01-30 13:22:40,348][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 949, in _run
    self.strategy.setup(self)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\single_device.py", line 75, in setup
    super().setup(trainer)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 149, in setup
    self.setup_optimizers(trainer)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 139, in setup_optimizers
    self.optimizers, self.lr_scheduler_configs = _init_optimizers_and_lr_schedulers(self.lightning_module)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\optimizer.py", line 168, in _init_optimizers_and_lr_schedulers
    optim_conf = call._call_lightning_module_hook(model.trainer, "configure_optimizers", pl_module=model)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 144, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 188, in configure_optimizers
    optimizer = self.hparams.optimizer(params=self.parameters())
TypeError: __init__() got an unexpected keyword argument 'lr'
[2024-01-30 13:22:40,351][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-01-30_13-22-01
[2024-01-30 13:22:40,352][src.utils.utils][INFO] - Closing wandb!
[2024-01-30 13:24:01,170][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-01-30 13:24:01,180][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-01-30 13:24:01,285][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-01-30 13:24:01,290][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-01-30 13:24:01,313][__main__][INFO] - Instantiating callbacks...
[2024-01-30 13:24:01,313][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-01-30 13:24:01,321][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-01-30 13:24:01,323][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-01-30 13:24:01,324][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-01-30 13:24:01,325][__main__][INFO] - Instantiating loggers...
[2024-01-30 13:24:01,327][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-01-30 13:24:30,795][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-01-30 13:24:31,393][__main__][INFO] - Logging hyperparameters!
[2024-01-30 13:24:31,397][__main__][INFO] - Starting training!
[2024-01-30 13:24:36,409][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 949, in _run
    self.strategy.setup(self)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\single_device.py", line 75, in setup
    super().setup(trainer)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 149, in setup
    self.setup_optimizers(trainer)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 139, in setup_optimizers
    self.optimizers, self.lr_scheduler_configs = _init_optimizers_and_lr_schedulers(self.lightning_module)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\optimizer.py", line 168, in _init_optimizers_and_lr_schedulers
    optim_conf = call._call_lightning_module_hook(model.trainer, "configure_optimizers", pl_module=model)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 144, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 188, in configure_optimizers
    optimizer = self.hparams.optimizer(params=self.parameters())
TypeError: __init__() got an unexpected keyword argument 'lr'
[2024-01-30 13:24:36,413][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-01-30_13-24-01
[2024-01-30 13:24:36,413][src.utils.utils][INFO] - Closing wandb!
[2024-01-30 13:25:48,281][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-01-30 13:25:48,289][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-01-30 13:25:48,378][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-01-30 13:25:48,384][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-01-30 13:25:48,405][__main__][INFO] - Instantiating callbacks...
[2024-01-30 13:25:48,405][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-01-30 13:25:48,411][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-01-30 13:25:48,413][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-01-30 13:25:48,414][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-01-30 13:25:48,416][__main__][INFO] - Instantiating loggers...
[2024-01-30 13:25:48,416][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-01-30 13:26:19,405][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-01-30 13:26:20,003][__main__][INFO] - Logging hyperparameters!
[2024-01-30 13:26:20,007][__main__][INFO] - Starting training!
[2024-01-30 13:26:24,641][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 949, in _run
    self.strategy.setup(self)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\single_device.py", line 75, in setup
    super().setup(trainer)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 149, in setup
    self.setup_optimizers(trainer)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 139, in setup_optimizers
    self.optimizers, self.lr_scheduler_configs = _init_optimizers_and_lr_schedulers(self.lightning_module)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\optimizer.py", line 168, in _init_optimizers_and_lr_schedulers
    optim_conf = call._call_lightning_module_hook(model.trainer, "configure_optimizers", pl_module=model)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 144, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 188, in configure_optimizers
    optimizer = self.hparams.optimizer(params=self.parameters(), lr_min=1e-5, lr_max=1e-2)
TypeError: __init__() got an unexpected keyword argument 'lr'
[2024-01-30 13:26:24,645][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-01-30_13-25-48
[2024-01-30 13:26:24,645][src.utils.utils][INFO] - Closing wandb!
[2024-01-30 13:27:39,677][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-01-30 13:27:39,685][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-01-30 13:27:39,776][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-01-30 13:27:39,786][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-01-30 13:27:39,820][__main__][INFO] - Instantiating callbacks...
[2024-01-30 13:27:39,820][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-01-30 13:27:39,826][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-01-30 13:27:39,829][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-01-30 13:27:39,830][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-01-30 13:27:39,831][__main__][INFO] - Instantiating loggers...
[2024-01-30 13:27:39,832][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-01-30 13:28:10,870][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-01-30 13:28:11,341][__main__][INFO] - Logging hyperparameters!
[2024-01-30 13:28:11,343][__main__][INFO] - Starting training!
[2024-01-30 13:28:16,873][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 949, in _run
    self.strategy.setup(self)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\single_device.py", line 75, in setup
    super().setup(trainer)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 149, in setup
    self.setup_optimizers(trainer)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 139, in setup_optimizers
    self.optimizers, self.lr_scheduler_configs = _init_optimizers_and_lr_schedulers(self.lightning_module)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\optimizer.py", line 168, in _init_optimizers_and_lr_schedulers
    optim_conf = call._call_lightning_module_hook(model.trainer, "configure_optimizers", pl_module=model)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 144, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 188, in configure_optimizers
    optimizer = self.hparams.optimizer(params=self.parameters())
TypeError: __init__() got an unexpected keyword argument 'lr'
[2024-01-30 13:28:16,877][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-01-30_13-27-39
[2024-01-30 13:28:16,878][src.utils.utils][INFO] - Closing wandb!
[2024-01-30 13:30:20,386][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-01-30 13:30:20,393][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-01-30 13:30:20,496][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-01-30 13:30:20,501][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-01-30 13:30:20,520][__main__][INFO] - Instantiating callbacks...
[2024-01-30 13:30:20,521][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-01-30 13:30:20,526][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-01-30 13:30:20,529][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-01-30 13:30:20,530][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-01-30 13:30:20,531][__main__][INFO] - Instantiating loggers...
[2024-01-30 13:30:20,531][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-01-30 13:30:50,794][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-01-30 13:30:51,259][__main__][INFO] - Logging hyperparameters!
[2024-01-30 13:30:51,261][__main__][INFO] - Starting training!
[2024-01-30 13:30:55,957][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 949, in _run
    self.strategy.setup(self)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\single_device.py", line 75, in setup
    super().setup(trainer)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 149, in setup
    self.setup_optimizers(trainer)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 139, in setup_optimizers
    self.optimizers, self.lr_scheduler_configs = _init_optimizers_and_lr_schedulers(self.lightning_module)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\optimizer.py", line 168, in _init_optimizers_and_lr_schedulers
    optim_conf = call._call_lightning_module_hook(model.trainer, "configure_optimizers", pl_module=model)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 144, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 188, in configure_optimizers
    optimizer = self.hparams.optimizer(params=self.parameters(), lr_min=1e-5, lr_max=1e-2)
TypeError: __init__() got an unexpected keyword argument 'lr'
[2024-01-30 13:30:55,960][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-01-30_13-30-20
[2024-01-30 13:30:55,961][src.utils.utils][INFO] - Closing wandb!
[2024-01-30 13:31:35,819][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-01-30 13:31:35,827][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-01-30 13:31:35,917][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-01-30 13:31:35,922][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-01-30 13:31:35,941][__main__][INFO] - Instantiating callbacks...
[2024-01-30 13:31:35,941][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-01-30 13:31:35,946][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-01-30 13:31:35,946][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-01-30 13:31:35,947][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-01-30 13:31:35,949][__main__][INFO] - Instantiating loggers...
[2024-01-30 13:31:35,949][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-01-30 13:32:04,309][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-01-30 13:32:05,012][__main__][INFO] - Logging hyperparameters!
[2024-01-30 13:32:05,014][__main__][INFO] - Starting training!
[2024-01-30 13:32:09,697][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 949, in _run
    self.strategy.setup(self)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\single_device.py", line 75, in setup
    super().setup(trainer)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 149, in setup
    self.setup_optimizers(trainer)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 139, in setup_optimizers
    self.optimizers, self.lr_scheduler_configs = _init_optimizers_and_lr_schedulers(self.lightning_module)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\optimizer.py", line 168, in _init_optimizers_and_lr_schedulers
    optim_conf = call._call_lightning_module_hook(model.trainer, "configure_optimizers", pl_module=model)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 144, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 201, in configure_optimizers
    optimizer = self.hparams.optimizer(params=self.parameters())
TypeError: __init__() got an unexpected keyword argument 'lr'
[2024-01-30 13:32:09,700][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-01-30_13-31-35
[2024-01-30 13:32:09,701][src.utils.utils][INFO] - Closing wandb!
[2024-01-30 13:33:11,194][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-01-30 13:33:11,203][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-01-30 13:33:11,319][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-01-30 13:33:11,327][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-01-30 13:33:11,358][__main__][INFO] - Instantiating callbacks...
[2024-01-30 13:33:11,358][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-01-30 13:33:11,366][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-01-30 13:33:11,370][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-01-30 13:33:11,371][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-01-30 13:33:11,373][__main__][INFO] - Instantiating loggers...
[2024-01-30 13:33:11,374][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-01-30 13:33:40,071][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-01-30 13:33:40,702][__main__][INFO] - Logging hyperparameters!
[2024-01-30 13:33:40,705][__main__][INFO] - Starting training!
[2024-01-30 13:33:45,441][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 949, in _run
    self.strategy.setup(self)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\single_device.py", line 75, in setup
    super().setup(trainer)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 149, in setup
    self.setup_optimizers(trainer)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 139, in setup_optimizers
    self.optimizers, self.lr_scheduler_configs = _init_optimizers_and_lr_schedulers(self.lightning_module)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\optimizer.py", line 168, in _init_optimizers_and_lr_schedulers
    optim_conf = call._call_lightning_module_hook(model.trainer, "configure_optimizers", pl_module=model)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 144, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 201, in configure_optimizers
    optimizer = self.hparams.optimizer(params=self.parameters())
TypeError: __init__() got an unexpected keyword argument 'lr'
[2024-01-30 13:33:45,444][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-01-30_13-33-11
[2024-01-30 13:33:45,445][src.utils.utils][INFO] - Closing wandb!
[2024-01-30 13:34:36,082][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-01-30 13:34:36,089][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-01-30 13:34:36,184][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-01-30 13:34:36,189][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-01-30 13:34:36,208][__main__][INFO] - Instantiating callbacks...
[2024-01-30 13:34:36,209][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-01-30 13:34:36,213][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-01-30 13:34:36,216][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-01-30 13:34:36,217][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-01-30 13:34:36,218][__main__][INFO] - Instantiating loggers...
[2024-01-30 13:34:36,218][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-01-30 13:35:10,662][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-01-30 13:35:11,426][__main__][INFO] - Logging hyperparameters!
[2024-01-30 13:35:11,429][__main__][INFO] - Starting training!
[2024-01-30 13:35:16,623][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 949, in _run
    self.strategy.setup(self)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\single_device.py", line 75, in setup
    super().setup(trainer)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 149, in setup
    self.setup_optimizers(trainer)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 139, in setup_optimizers
    self.optimizers, self.lr_scheduler_configs = _init_optimizers_and_lr_schedulers(self.lightning_module)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\optimizer.py", line 168, in _init_optimizers_and_lr_schedulers
    optim_conf = call._call_lightning_module_hook(model.trainer, "configure_optimizers", pl_module=model)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 144, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 201, in configure_optimizers
    optimizer = self.hparams.optimizer(params=self.parameters())
TypeError: __init__() got an unexpected keyword argument 'lr'
[2024-01-30 13:35:16,627][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-01-30_13-34-35
[2024-01-30 13:35:16,627][src.utils.utils][INFO] - Closing wandb!
[2024-01-30 13:36:34,676][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-01-30 13:36:34,683][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-01-30 13:36:34,759][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-01-30 13:36:34,764][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-01-30 13:36:34,785][__main__][INFO] - Instantiating callbacks...
[2024-01-30 13:36:34,785][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-01-30 13:36:34,789][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-01-30 13:36:34,790][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-01-30 13:36:34,791][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-01-30 13:36:34,791][__main__][INFO] - Instantiating loggers...
[2024-01-30 13:36:34,793][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-01-30 13:37:02,707][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-01-30 13:37:03,350][__main__][INFO] - Logging hyperparameters!
[2024-01-30 13:37:03,353][__main__][INFO] - Starting training!
[2024-01-30 13:37:32,459][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1016, in _run_stage
    self.fit_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 202, in run
    self.on_advance_end()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 377, in on_advance_end
    self.epoch_loop.update_lr_schedulers("epoch", update_plateau_schedulers=not self.restarting)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 300, in update_lr_schedulers
    self._update_learning_rates(interval=interval, update_plateau_schedulers=update_plateau_schedulers)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 350, in _update_learning_rates
    call._call_lightning_module_hook(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 144, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\module.py", line 1216, in lr_scheduler_step
    scheduler.step(metric)
  File "D:\Anaconda3\lib\site-packages\torch\optim\lr_scheduler.py", line 1031, in step
    self._last_lr = [group['lr'] for group in self.optimizer.param_groups]
  File "D:\Anaconda3\lib\site-packages\torch\optim\lr_scheduler.py", line 1031, in <listcomp>
    self._last_lr = [group['lr'] for group in self.optimizer.param_groups]
KeyError: 'lr'
[2024-01-30 13:37:32,465][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-01-30_13-36-34
[2024-01-30 13:37:32,465][src.utils.utils][INFO] - Closing wandb!
[2024-01-30 13:38:48,326][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-01-30 13:38:48,336][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-01-30 13:38:48,448][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-01-30 13:38:48,453][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-01-30 13:38:48,474][__main__][INFO] - Instantiating callbacks...
[2024-01-30 13:38:48,474][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-01-30 13:38:48,478][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-01-30 13:38:48,480][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-01-30 13:38:48,481][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-01-30 13:38:48,481][__main__][INFO] - Instantiating loggers...
[2024-01-30 13:38:48,483][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-01-30 13:39:23,385][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-01-30 13:39:24,013][__main__][INFO] - Logging hyperparameters!
[2024-01-30 13:39:24,016][__main__][INFO] - Starting training!
[2024-01-30 13:39:48,860][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1016, in _run_stage
    self.fit_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 202, in run
    self.on_advance_end()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 377, in on_advance_end
    self.epoch_loop.update_lr_schedulers("epoch", update_plateau_schedulers=not self.restarting)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 300, in update_lr_schedulers
    self._update_learning_rates(interval=interval, update_plateau_schedulers=update_plateau_schedulers)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 350, in _update_learning_rates
    call._call_lightning_module_hook(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 144, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\module.py", line 1216, in lr_scheduler_step
    scheduler.step(metric)
  File "D:\Anaconda3\lib\site-packages\torch\optim\lr_scheduler.py", line 1031, in step
    self._last_lr = [group['lr'] for group in self.optimizer.param_groups]
  File "D:\Anaconda3\lib\site-packages\torch\optim\lr_scheduler.py", line 1031, in <listcomp>
    self._last_lr = [group['lr'] for group in self.optimizer.param_groups]
KeyError: 'lr'
[2024-01-30 13:39:48,864][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-01-30_13-38-48
[2024-01-30 13:39:48,865][src.utils.utils][INFO] - Closing wandb!
[2024-01-30 13:41:16,878][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-01-30 13:41:16,888][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-01-30 13:41:16,986][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-01-30 13:41:16,991][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-01-30 13:41:17,010][__main__][INFO] - Instantiating callbacks...
[2024-01-30 13:41:17,010][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-01-30 13:41:17,016][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-01-30 13:41:17,017][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-01-30 13:41:17,018][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-01-30 13:41:17,019][__main__][INFO] - Instantiating loggers...
[2024-01-30 13:41:17,019][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-01-30 13:41:45,734][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-01-30 13:41:46,354][__main__][INFO] - Logging hyperparameters!
[2024-01-30 13:41:46,357][__main__][INFO] - Starting training!
[2024-01-30 13:41:50,959][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 949, in _run
    self.strategy.setup(self)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\single_device.py", line 75, in setup
    super().setup(trainer)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 149, in setup
    self.setup_optimizers(trainer)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 139, in setup_optimizers
    self.optimizers, self.lr_scheduler_configs = _init_optimizers_and_lr_schedulers(self.lightning_module)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\optimizer.py", line 168, in _init_optimizers_and_lr_schedulers
    optim_conf = call._call_lightning_module_hook(model.trainer, "configure_optimizers", pl_module=model)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 144, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 201, in configure_optimizers
    optimizer = self.hparams.optimizer(params=self.parameters(), lr=1e-5)  # 
TypeError: __init__() got an unexpected keyword argument 'lr'
[2024-01-30 13:41:50,965][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-01-30_13-41-16
[2024-01-30 13:41:50,965][src.utils.utils][INFO] - Closing wandb!
[2024-01-30 13:44:01,687][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-01-30 13:44:01,695][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-01-30 13:44:01,772][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-01-30 13:44:01,777][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-01-30 13:44:01,799][__main__][INFO] - Instantiating callbacks...
[2024-01-30 13:44:01,800][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-01-30 13:44:01,803][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-01-30 13:44:01,805][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-01-30 13:44:01,806][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-01-30 13:44:01,808][__main__][INFO] - Instantiating loggers...
[2024-01-30 13:44:01,808][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-01-30 13:44:33,577][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-01-30 13:44:34,005][__main__][INFO] - Logging hyperparameters!
[2024-01-30 13:44:34,009][__main__][INFO] - Starting training!
[2024-01-30 13:46:47,565][__main__][INFO] - Starting testing!
[2024-01-30 13:46:53,101][__main__][INFO] - Best ckpt path: D:\data\CIFAR10_logs\logs\train\runs\2024-01-30_13-44-01\checkpoints\epoch_004.ckpt
[2024-01-30 13:46:53,103][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-01-30_13-44-01
[2024-01-30 13:46:53,105][src.utils.utils][INFO] - Closing wandb!
[2024-01-30 13:47:03,275][src.utils.utils][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-01-30 13:54:17,859][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-01-30 13:54:17,867][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-01-30 13:54:17,945][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-01-30 13:54:17,950][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-01-30 13:54:17,969][__main__][INFO] - Instantiating callbacks...
[2024-01-30 13:54:17,969][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-01-30 13:54:17,974][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-01-30 13:54:17,975][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-01-30 13:54:17,976][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-01-30 13:54:17,977][__main__][INFO] - Instantiating loggers...
[2024-01-30 13:54:17,977][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-01-30 13:54:48,728][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-01-30 13:54:49,672][__main__][INFO] - Logging hyperparameters!
[2024-01-30 13:54:49,677][__main__][INFO] - Starting training!
[2024-01-30 13:54:57,336][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1016, in _run_stage
    self.fit_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 201, in run
    self.advance()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 354, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 133, in run
    self.advance(data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 218, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 185, in run
    self._optimizer_step(kwargs.get("batch_idx", 0), closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 260, in _optimizer_step
    call._call_lightning_module_hook(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 144, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\module.py", line 1256, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\optimizer.py", line 155, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 225, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\plugins\precision\precision_plugin.py", line 114, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\optim\optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\optim\Optimizer.py", line 51, in step
    angle = torch.dot(grad, prev_grad) / (torch.norm(grad) * torch.norm(prev_grad) + group['eps'])
RuntimeError: 1D tensors expected, but got 4D and 4D tensors
[2024-01-30 13:54:57,345][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-01-30_13-54-17
[2024-01-30 13:54:57,346][src.utils.utils][INFO] - Closing wandb!
[2024-01-30 13:55:50,951][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-01-30 13:55:50,958][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-01-30 13:55:51,052][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-01-30 13:55:51,057][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-01-30 13:55:51,077][__main__][INFO] - Instantiating callbacks...
[2024-01-30 13:55:51,077][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-01-30 13:55:51,082][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-01-30 13:55:51,083][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-01-30 13:55:51,083][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-01-30 13:55:51,084][__main__][INFO] - Instantiating loggers...
[2024-01-30 13:55:51,084][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-01-30 13:56:20,114][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-01-30 13:56:20,782][__main__][INFO] - Logging hyperparameters!
[2024-01-30 13:56:20,784][__main__][INFO] - Starting training!
[2024-01-30 13:56:28,029][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1016, in _run_stage
    self.fit_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 201, in run
    self.advance()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 354, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 133, in run
    self.advance(data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 218, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 185, in run
    self._optimizer_step(kwargs.get("batch_idx", 0), closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 260, in _optimizer_step
    call._call_lightning_module_hook(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 144, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\module.py", line 1256, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\optimizer.py", line 155, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 225, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\plugins\precision\precision_plugin.py", line 114, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\optim\optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\optim\Optimizer.py", line 80, in step
    p.data.addcdiv_(-step_size, momentum_term, denom)
RuntimeError: The size of tensor a (3) must match the size of tensor b (432) at non-singleton dimension 3
[2024-01-30 13:56:28,034][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-01-30_13-55-50
[2024-01-30 13:56:28,034][src.utils.utils][INFO] - Closing wandb!
[2024-01-30 13:57:19,583][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-01-30 13:57:19,591][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-01-30 13:57:19,712][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-01-30 13:57:19,720][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-01-30 13:57:19,740][__main__][INFO] - Instantiating callbacks...
[2024-01-30 13:57:19,741][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-01-30 13:57:19,745][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-01-30 13:57:19,745][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-01-30 13:57:19,746][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-01-30 13:57:19,747][__main__][INFO] - Instantiating loggers...
[2024-01-30 13:57:19,747][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-01-30 13:57:52,528][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-01-30 13:57:53,839][__main__][INFO] - Logging hyperparameters!
[2024-01-30 13:57:53,843][__main__][INFO] - Starting training!
[2024-01-30 13:58:02,241][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1016, in _run_stage
    self.fit_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 201, in run
    self.advance()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 354, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 133, in run
    self.advance(data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 218, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 185, in run
    self._optimizer_step(kwargs.get("batch_idx", 0), closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 260, in _optimizer_step
    call._call_lightning_module_hook(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 144, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\module.py", line 1256, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\optimizer.py", line 155, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 225, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\plugins\precision\precision_plugin.py", line 114, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\optim\optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\optim\Optimizer.py", line 80, in step
    p.data.addcdiv_(-step_size, momentum_term, denom)
RuntimeError: The size of tensor a (3) must match the size of tensor b (432) at non-singleton dimension 3
[2024-01-30 13:58:02,246][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-01-30_13-57-19
[2024-01-30 13:58:02,246][src.utils.utils][INFO] - Closing wandb!
[2024-01-30 13:58:57,031][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-01-30 13:58:57,040][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-01-30 13:58:57,127][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-01-30 13:58:57,132][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-01-30 13:58:57,158][__main__][INFO] - Instantiating callbacks...
[2024-01-30 13:58:57,159][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-01-30 13:58:57,165][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-01-30 13:58:57,168][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-01-30 13:58:57,169][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-01-30 13:58:57,170][__main__][INFO] - Instantiating loggers...
[2024-01-30 13:58:57,171][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-01-30 13:59:30,298][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-01-30 13:59:30,820][__main__][INFO] - Logging hyperparameters!
[2024-01-30 13:59:30,823][__main__][INFO] - Starting training!
[2024-01-30 13:59:38,750][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1016, in _run_stage
    self.fit_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 201, in run
    self.advance()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 354, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 133, in run
    self.advance(data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 218, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 185, in run
    self._optimizer_step(kwargs.get("batch_idx", 0), closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 260, in _optimizer_step
    call._call_lightning_module_hook(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 144, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\module.py", line 1256, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\optimizer.py", line 155, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 225, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\plugins\precision\precision_plugin.py", line 114, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\optim\optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\optim\Optimizer.py", line 80, in step
    p.data.addcdiv_(-step_size, momentum_term, denom)
RuntimeError: The size of tensor a (3) must match the size of tensor b (432) at non-singleton dimension 3
[2024-01-30 13:59:38,755][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-01-30_13-58-56
[2024-01-30 13:59:38,755][src.utils.utils][INFO] - Closing wandb!
[2024-01-30 14:00:42,895][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-01-30 14:00:42,902][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-01-30 14:00:42,979][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-01-30 14:00:42,983][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-01-30 14:00:43,004][__main__][INFO] - Instantiating callbacks...
[2024-01-30 14:00:43,005][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-01-30 14:00:43,009][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-01-30 14:00:43,011][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-01-30 14:00:43,012][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-01-30 14:00:43,013][__main__][INFO] - Instantiating loggers...
[2024-01-30 14:00:43,014][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-01-30 14:01:18,370][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-01-30 14:01:19,881][__main__][INFO] - Logging hyperparameters!
[2024-01-30 14:01:19,884][__main__][INFO] - Starting training!
[2024-01-30 14:02:48,838][__main__][INFO] - Starting testing!
[2024-01-30 14:07:48,999][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-01-30 14:07:49,008][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-01-30 14:07:49,103][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-01-30 14:07:49,110][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-01-30 14:07:49,133][__main__][INFO] - Instantiating callbacks...
[2024-01-30 14:07:49,134][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-01-30 14:07:49,138][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-01-30 14:07:49,140][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-01-30 14:07:49,141][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-01-30 14:07:49,142][__main__][INFO] - Instantiating loggers...
[2024-01-30 14:07:49,143][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-01-30 14:08:23,182][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-01-30 14:08:23,797][__main__][INFO] - Logging hyperparameters!
[2024-01-30 14:08:23,801][__main__][INFO] - Starting training!
[2024-01-30 14:10:51,369][__main__][INFO] - Starting testing!
[2024-01-30 14:10:57,335][__main__][INFO] - Best ckpt path: D:\data\CIFAR10_logs\logs\train\runs\2024-01-30_14-07-48\checkpoints\epoch_004.ckpt
[2024-01-30 14:10:57,336][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-01-30_14-07-48
[2024-01-30 14:10:57,337][src.utils.utils][INFO] - Closing wandb!
[2024-01-30 14:11:08,242][src.utils.utils][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-01-30 14:13:28,666][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-01-30 14:13:28,674][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-01-30 14:13:28,757][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-01-30 14:13:28,762][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-01-30 14:13:28,784][__main__][INFO] - Instantiating callbacks...
[2024-01-30 14:13:28,785][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-01-30 14:13:28,790][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-01-30 14:13:28,792][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-01-30 14:13:28,793][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-01-30 14:13:28,794][__main__][INFO] - Instantiating loggers...
[2024-01-30 14:13:28,795][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-01-30 14:14:05,065][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-01-30 14:14:05,934][__main__][INFO] - Logging hyperparameters!
[2024-01-30 14:14:05,939][__main__][INFO] - Starting training!
[2024-01-30 14:17:13,689][__main__][INFO] - Starting testing!
[2024-01-30 14:17:19,464][__main__][INFO] - Best ckpt path: D:\data\CIFAR10_logs\logs\train\runs\2024-01-30_14-13-28\checkpoints\epoch_004.ckpt
[2024-01-30 14:17:19,465][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-01-30_14-13-28
[2024-01-30 14:17:19,466][src.utils.utils][INFO] - Closing wandb!
[2024-01-30 14:17:31,082][src.utils.utils][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-01-30 14:19:16,619][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-01-30 14:19:16,630][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-01-30 14:19:16,730][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-01-30 14:19:16,734][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-01-30 14:19:16,759][__main__][INFO] - Instantiating callbacks...
[2024-01-30 14:19:16,759][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-01-30 14:19:16,764][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-01-30 14:19:16,765][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-01-30 14:19:16,767][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-01-30 14:19:16,769][__main__][INFO] - Instantiating loggers...
[2024-01-30 14:19:16,769][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-01-30 14:19:46,649][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-01-30 14:19:47,249][__main__][INFO] - Logging hyperparameters!
[2024-01-30 14:19:47,252][__main__][INFO] - Starting training!
[2024-01-30 14:22:12,200][__main__][INFO] - Starting testing!
[2024-01-30 14:22:18,815][__main__][INFO] - Best ckpt path: D:\data\CIFAR10_logs\logs\train\runs\2024-01-30_14-19-16\checkpoints\epoch_003.ckpt
[2024-01-30 14:22:18,816][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-01-30_14-19-16
[2024-01-30 14:22:18,818][src.utils.utils][INFO] - Closing wandb!
[2024-01-30 14:22:30,773][src.utils.utils][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-01-30 14:22:43,875][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-01-30 14:22:43,883][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-01-30 14:22:43,968][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-01-30 14:22:43,973][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-01-30 14:22:43,990][__main__][INFO] - Instantiating callbacks...
[2024-01-30 14:22:43,990][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-01-30 14:22:43,994][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-01-30 14:22:43,996][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-01-30 14:22:43,997][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-01-30 14:22:44,000][__main__][INFO] - Instantiating loggers...
[2024-01-30 14:22:44,000][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-01-30 14:23:17,188][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-01-30 14:23:17,930][__main__][INFO] - Logging hyperparameters!
[2024-01-30 14:23:17,936][__main__][INFO] - Starting training!
[2024-01-30 14:25:43,000][__main__][INFO] - Starting testing!
[2024-01-30 14:25:48,348][__main__][INFO] - Best ckpt path: D:\data\CIFAR10_logs\logs\train\runs\2024-01-30_14-22-43\checkpoints\epoch_005.ckpt
[2024-01-30 14:25:48,350][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-01-30_14-22-43
[2024-01-30 14:25:48,350][src.utils.utils][INFO] - Closing wandb!
[2024-01-30 14:25:59,452][src.utils.utils][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-01-30 14:28:07,871][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-01-30 14:28:07,878][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-01-30 14:28:07,966][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-01-30 14:28:07,971][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-01-30 14:28:07,989][__main__][INFO] - Instantiating callbacks...
[2024-01-30 14:28:07,990][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-01-30 14:28:07,994][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-01-30 14:28:07,996][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-01-30 14:28:07,996][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-01-30 14:28:07,998][__main__][INFO] - Instantiating loggers...
[2024-01-30 14:28:07,998][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-01-30 14:28:37,224][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-01-30 14:28:37,864][__main__][INFO] - Logging hyperparameters!
[2024-01-30 14:28:37,868][__main__][INFO] - Starting training!
[2024-01-30 14:32:28,788][__main__][INFO] - Starting testing!
[2024-01-30 14:32:33,788][__main__][INFO] - Best ckpt path: D:\data\CIFAR10_logs\logs\train\runs\2024-01-30_14-28-07\checkpoints\epoch_009.ckpt
[2024-01-30 14:32:33,791][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-01-30_14-28-07
[2024-01-30 14:32:33,791][src.utils.utils][INFO] - Closing wandb!
[2024-01-30 14:32:46,434][src.utils.utils][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-01-30 14:39:24,035][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-01-30 14:39:24,047][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-01-30 14:39:24,212][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-01-30 14:39:24,229][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-01-30 14:39:24,285][__main__][INFO] - Instantiating callbacks...
[2024-01-30 14:39:24,286][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-01-30 14:39:24,294][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-01-30 14:39:24,301][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-01-30 14:39:24,302][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-01-30 14:39:24,305][__main__][INFO] - Instantiating loggers...
[2024-01-30 14:39:24,305][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-01-30 14:39:55,206][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-01-30 14:39:55,825][__main__][INFO] - Logging hyperparameters!
[2024-01-30 14:39:55,830][__main__][INFO] - Starting training!
[2024-01-30 14:43:44,542][__main__][INFO] - Starting testing!
[2024-01-30 14:43:50,280][__main__][INFO] - Best ckpt path: D:\data\CIFAR10_logs\logs\train\runs\2024-01-30_14-39-23\checkpoints\epoch_009.ckpt
[2024-01-30 14:43:50,281][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-01-30_14-39-23
[2024-01-30 14:43:50,282][src.utils.utils][INFO] - Closing wandb!
[2024-01-30 14:44:01,127][src.utils.utils][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-01-30 15:18:06,971][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-01-30 15:18:06,977][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-01-30 15:18:07,056][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-01-30 15:18:07,061][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-01-30 15:18:07,079][__main__][INFO] - Instantiating callbacks...
[2024-01-30 15:18:07,080][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-01-30 15:18:07,084][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-01-30 15:18:07,087][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-01-30 15:18:07,088][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-01-30 15:18:07,089][__main__][INFO] - Instantiating loggers...
[2024-01-30 15:18:07,090][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-01-30 15:18:36,227][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-01-30 15:18:36,892][__main__][INFO] - Logging hyperparameters!
[2024-01-30 15:18:36,895][__main__][INFO] - Starting training!
[2024-01-30 15:22:54,418][__main__][INFO] - Starting testing!
[2024-01-30 15:23:02,888][__main__][INFO] - Best ckpt path: D:\data\CIFAR10_logs\logs\train\runs\2024-01-30_15-18-06\checkpoints\epoch_006.ckpt
[2024-01-30 15:23:02,890][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-01-30_15-18-06
[2024-01-30 15:23:02,890][src.utils.utils][INFO] - Closing wandb!
[2024-01-30 15:23:13,287][src.utils.utils][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-01-30 15:25:31,888][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-01-30 15:25:31,895][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-01-30 15:25:31,983][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-01-30 15:25:31,987][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-01-30 15:25:32,008][__main__][INFO] - Instantiating callbacks...
[2024-01-30 15:25:32,009][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-01-30 15:25:32,014][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-01-30 15:25:32,015][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-01-30 15:25:32,017][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-01-30 15:25:32,018][__main__][INFO] - Instantiating loggers...
[2024-01-30 15:25:32,018][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-01-30 15:26:11,956][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-01-30 15:26:12,896][__main__][INFO] - Logging hyperparameters!
[2024-01-30 15:26:12,900][__main__][INFO] - Starting training!
[2024-01-30 15:30:44,029][__main__][INFO] - Starting testing!
[2024-01-30 15:30:49,768][__main__][INFO] - Best ckpt path: D:\data\CIFAR10_logs\logs\train\runs\2024-01-30_15-25-31\checkpoints\epoch_008.ckpt
[2024-01-30 15:30:49,770][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-01-30_15-25-31
[2024-01-30 15:30:49,770][src.utils.utils][INFO] - Closing wandb!
[2024-01-30 15:31:01,140][src.utils.utils][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-01-30 15:34:04,992][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-01-30 15:34:05,000][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-01-30 15:34:05,085][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-01-30 15:34:05,089][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-01-30 15:34:05,106][__main__][INFO] - Instantiating callbacks...
[2024-01-30 15:34:05,107][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-01-30 15:34:05,112][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-01-30 15:34:05,114][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-01-30 15:34:05,114][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-01-30 15:34:05,116][__main__][INFO] - Instantiating loggers...
[2024-01-30 15:34:05,116][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-01-30 15:34:16,223][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-01-30_15-34-04
[2024-01-30 15:35:22,396][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-01-30 15:35:22,404][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-01-30 15:35:22,488][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-01-30 15:35:22,492][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-01-30 15:35:22,511][__main__][INFO] - Instantiating callbacks...
[2024-01-30 15:35:22,511][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-01-30 15:35:22,515][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-01-30 15:35:22,516][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-01-30 15:35:22,516][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-01-30 15:35:22,518][__main__][INFO] - Instantiating loggers...
[2024-01-30 15:35:22,518][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-01-30 15:35:50,009][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-01-30 15:35:50,473][__main__][INFO] - Logging hyperparameters!
[2024-01-30 15:35:50,476][__main__][INFO] - Starting training!
[2024-01-30 15:41:01,416][__main__][INFO] - Starting testing!
[2024-01-30 15:41:08,490][__main__][INFO] - Best ckpt path: D:\data\CIFAR10_logs\logs\train\runs\2024-01-30_15-35-22\checkpoints\epoch_006.ckpt
[2024-01-30 15:41:08,492][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-01-30_15-35-22
[2024-01-30 15:41:08,492][src.utils.utils][INFO] - Closing wandb!
[2024-01-30 15:41:19,166][src.utils.utils][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-01-30 15:56:49,404][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-01-30 15:56:49,411][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-01-30 15:56:49,507][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-01-30 15:56:49,512][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-01-30 15:56:49,534][__main__][INFO] - Instantiating callbacks...
[2024-01-30 15:56:49,534][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-01-30 15:56:49,541][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-01-30 15:56:49,542][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-01-30 15:56:49,542][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-01-30 15:56:49,544][__main__][INFO] - Instantiating loggers...
[2024-01-30 15:56:49,544][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-01-30 15:57:19,102][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-01-30 15:57:19,715][__main__][INFO] - Logging hyperparameters!
[2024-01-30 15:57:19,718][__main__][INFO] - Starting training!
[2024-01-30 16:02:54,069][__main__][INFO] - Starting testing!
[2024-01-30 16:02:59,744][__main__][INFO] - Best ckpt path: D:\data\CIFAR10_logs\logs\train\runs\2024-01-30_15-56-49\checkpoints\epoch_007.ckpt
[2024-01-30 16:02:59,746][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-01-30_15-56-49
[2024-01-30 16:02:59,746][src.utils.utils][INFO] - Closing wandb!
[2024-01-30 16:03:12,659][src.utils.utils][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-01-30 16:04:07,304][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-01-30 16:04:07,313][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-01-30 16:04:07,430][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-01-30 16:04:07,437][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-01-30 16:04:07,464][__main__][INFO] - Instantiating callbacks...
[2024-01-30 16:04:07,465][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-01-30 16:04:07,472][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-01-30 16:04:07,475][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-01-30 16:04:07,476][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-01-30 16:04:07,479][__main__][INFO] - Instantiating loggers...
[2024-01-30 16:04:07,480][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-01-30 16:04:40,543][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-01-30 16:04:41,306][__main__][INFO] - Logging hyperparameters!
[2024-01-30 16:04:41,310][__main__][INFO] - Starting training!
[2024-01-30 16:10:36,277][__main__][INFO] - Starting testing!
[2024-01-30 16:10:43,676][__main__][INFO] - Best ckpt path: D:\data\CIFAR10_logs\logs\train\runs\2024-01-30_16-04-07\checkpoints\epoch_010.ckpt
[2024-01-30 16:10:43,679][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-01-30_16-04-07
[2024-01-30 16:10:43,679][src.utils.utils][INFO] - Closing wandb!
[2024-01-30 16:10:55,934][src.utils.utils][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-01-30 16:15:32,385][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-01-30 16:15:32,394][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-01-30 16:15:32,509][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-01-30 16:15:32,515][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-01-30 16:15:32,540][__main__][INFO] - Instantiating callbacks...
[2024-01-30 16:15:32,541][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-01-30 16:15:32,548][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-01-30 16:15:32,550][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-01-30 16:15:32,551][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-01-30 16:15:32,553][__main__][INFO] - Instantiating loggers...
[2024-01-30 16:15:32,553][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-01-30 16:16:08,970][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-01-30 16:16:09,637][__main__][INFO] - Logging hyperparameters!
[2024-01-30 16:16:09,640][__main__][INFO] - Starting training!
[2024-01-30 16:21:50,385][__main__][INFO] - Starting testing!
[2024-01-30 16:21:57,772][__main__][INFO] - Best ckpt path: D:\data\CIFAR10_logs\logs\train\runs\2024-01-30_16-15-32\checkpoints\epoch_007.ckpt
[2024-01-30 16:21:57,774][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-01-30_16-15-32
[2024-01-30 16:21:57,775][src.utils.utils][INFO] - Closing wandb!
[2024-01-30 16:22:10,330][src.utils.utils][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-01-30 16:22:55,015][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-01-30 16:22:55,023][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-01-30 16:22:55,125][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-01-30 16:22:55,129][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-01-30 16:22:55,148][__main__][INFO] - Instantiating callbacks...
[2024-01-30 16:22:55,148][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-01-30 16:22:55,152][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-01-30 16:22:55,153][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-01-30 16:22:55,155][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-01-30 16:22:55,156][__main__][INFO] - Instantiating loggers...
[2024-01-30 16:22:55,156][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-01-30 16:23:31,712][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-01-30 16:23:32,770][__main__][INFO] - Logging hyperparameters!
[2024-01-30 16:23:32,777][__main__][INFO] - Starting training!
[2024-01-30 16:30:11,336][__main__][INFO] - Starting testing!
[2024-01-30 16:30:17,074][__main__][INFO] - Best ckpt path: D:\data\CIFAR10_logs\logs\train\runs\2024-01-30_16-22-54\checkpoints\epoch_007.ckpt
[2024-01-30 16:30:17,076][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-01-30_16-22-54
[2024-01-30 16:30:17,077][src.utils.utils][INFO] - Closing wandb!
[2024-01-30 16:30:27,779][src.utils.utils][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-01-30 16:32:07,908][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-01-30 16:32:07,915][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-01-30 16:32:08,014][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-01-30 16:32:08,019][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-01-30 16:32:08,039][__main__][INFO] - Instantiating callbacks...
[2024-01-30 16:32:08,040][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-01-30 16:32:08,044][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-01-30 16:32:08,046][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-01-30 16:32:08,047][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-01-30 16:32:08,048][__main__][INFO] - Instantiating loggers...
[2024-01-30 16:32:08,048][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-01-30 16:32:40,460][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-01-30 16:32:41,205][__main__][INFO] - Logging hyperparameters!
[2024-01-30 16:32:41,208][__main__][INFO] - Starting training!
[2024-01-30 16:39:06,456][__main__][INFO] - Starting testing!
[2024-01-30 16:39:22,846][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-01-30 16:39:22,853][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-01-30 16:39:22,945][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-01-30 16:39:22,950][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-01-30 16:39:22,969][__main__][INFO] - Instantiating callbacks...
[2024-01-30 16:39:22,969][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-01-30 16:39:22,975][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-01-30 16:39:22,977][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-01-30 16:39:22,978][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-01-30 16:39:22,979][__main__][INFO] - Instantiating loggers...
[2024-01-30 16:39:22,979][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-01-30 16:39:56,738][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-01-30 16:39:57,371][__main__][INFO] - Logging hyperparameters!
[2024-01-30 16:39:57,375][__main__][INFO] - Starting training!
[2024-01-30 16:41:52,428][__main__][INFO] - Starting testing!
[2024-01-30 16:41:58,697][__main__][INFO] - Best ckpt path: D:\data\CIFAR10_logs\logs\train\runs\2024-01-30_16-39-22\checkpoints\epoch_000.ckpt
[2024-01-30 16:41:58,699][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-01-30_16-39-22
[2024-01-30 16:41:58,699][src.utils.utils][INFO] - Closing wandb!
[2024-01-30 16:42:13,066][src.utils.utils][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-01-30 17:00:28,600][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-01-30 17:00:28,610][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-01-30 17:00:28,720][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-01-30 17:00:28,726][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-01-30 17:00:28,808][__main__][INFO] - Instantiating callbacks...
[2024-01-30 17:00:28,809][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-01-30 17:00:28,818][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-01-30 17:00:28,822][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-01-30 17:00:28,823][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-01-30 17:00:28,826][__main__][INFO] - Instantiating loggers...
[2024-01-30 17:00:28,831][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-01-30 17:01:06,206][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-01-30 17:01:06,819][__main__][INFO] - Logging hyperparameters!
[2024-01-30 17:01:06,822][__main__][INFO] - Starting training!
[2024-01-30 17:06:19,392][__main__][INFO] - Starting testing!
[2024-01-30 17:06:25,693][__main__][INFO] - Best ckpt path: D:\data\CIFAR10_logs\logs\train\runs\2024-01-30_17-00-28\checkpoints\epoch_007.ckpt
[2024-01-30 17:06:25,695][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-01-30_17-00-28
[2024-01-30 17:06:25,695][src.utils.utils][INFO] - Closing wandb!
[2024-01-30 17:06:46,348][src.utils.utils][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-01-30 17:06:46,394][urllib3.connectionpool][WARNING] - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProxyError('Cannot connect to proxy.', NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x0000016EFA932580>: Failed to establish a new connection: [WinError 10061] '))': /api/4504800232407040/envelope/
[2024-01-30 17:55:48,653][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-01-30 17:55:48,663][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-01-30 17:55:48,784][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-01-30 17:55:48,793][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-01-30 17:55:48,831][__main__][INFO] - Instantiating callbacks...
[2024-01-30 17:55:48,832][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-01-30 17:55:48,842][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-01-30 17:55:48,845][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-01-30 17:55:48,845][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-01-30 17:55:48,847][__main__][INFO] - Instantiating loggers...
[2024-01-30 17:55:48,848][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-01-30 17:56:20,912][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-01-30 17:56:21,575][__main__][INFO] - Logging hyperparameters!
[2024-01-30 17:56:21,579][__main__][INFO] - Starting training!
[2024-01-30 18:00:34,410][__main__][INFO] - Starting testing!
[2024-01-30 18:00:40,747][__main__][INFO] - Best ckpt path: D:\data\CIFAR10_logs\logs\train\runs\2024-01-30_17-55-48\checkpoints\epoch_007.ckpt
[2024-01-30 18:00:40,749][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-01-30_17-55-48
[2024-01-30 18:00:40,750][src.utils.utils][INFO] - Closing wandb!
[2024-01-30 18:00:50,398][src.utils.utils][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-01-30 18:12:19,787][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-01-30 18:12:19,794][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-01-30 18:12:19,870][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-01-30 18:12:19,875][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-01-30 18:12:19,897][__main__][INFO] - Instantiating callbacks...
[2024-01-30 18:12:19,900][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-01-30 18:12:19,905][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-01-30 18:12:19,907][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-01-30 18:12:19,907][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-01-30 18:12:19,909][__main__][INFO] - Instantiating loggers...
[2024-01-30 18:12:19,909][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-01-30 18:12:27,590][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-01-30_18-12-19
[2024-01-30 18:12:50,786][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-01-30 18:12:50,799][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-01-30 18:12:50,900][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-01-30 18:12:50,907][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-01-30 18:12:50,932][__main__][INFO] - Instantiating callbacks...
[2024-01-30 18:12:50,932][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-01-30 18:12:50,939][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-01-30 18:12:50,940][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-01-30 18:12:50,942][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-01-30 18:12:50,944][__main__][INFO] - Instantiating loggers...
[2024-01-30 18:12:50,944][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-01-30 18:13:37,494][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-01-30 18:13:38,132][__main__][INFO] - Logging hyperparameters!
[2024-01-30 18:13:38,135][__main__][INFO] - Starting training!
[2024-01-30 18:20:13,288][__main__][INFO] - Starting testing!
[2024-01-30 18:20:18,724][__main__][INFO] - Best ckpt path: D:\data\CIFAR10_logs\logs\train\runs\2024-01-30_18-12-50\checkpoints\epoch_007.ckpt
[2024-01-30 18:20:18,726][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-01-30_18-12-50
[2024-01-30 18:20:18,726][src.utils.utils][INFO] - Closing wandb!
[2024-01-30 18:20:29,056][src.utils.utils][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-01-30 18:30:18,055][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-01-30 18:30:18,062][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-01-30 18:30:18,147][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-01-30 18:30:18,151][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-01-30 18:30:18,179][__main__][INFO] - Instantiating callbacks...
[2024-01-30 18:30:18,180][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-01-30 18:30:18,183][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-01-30 18:30:18,184][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-01-30 18:30:18,185][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-01-30 18:30:18,186][__main__][INFO] - Instantiating loggers...
[2024-01-30 18:30:18,186][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-01-30 18:30:47,085][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-01-30 18:30:47,712][__main__][INFO] - Logging hyperparameters!
[2024-01-30 18:30:47,715][__main__][INFO] - Starting training!
[2024-01-30 18:30:53,492][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1014, in _run_stage
    self._run_sanity_check()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1043, in _run_sanity_check
    val_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\utilities.py", line 177, in _decorator
    return loop_run(self, *args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\evaluation_loop.py", line 115, in run
    self._evaluation_step(batch, batch_idx, dataloader_idx)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\evaluation_loop.py", line 375, in _evaluation_step
    output = call._call_strategy_hook(trainer, hook_name, *step_kwargs.values())
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 291, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 379, in validation_step
    return self.model.validation_step(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 143, in validation_step
    loss, preds, targets = self.model_step(batch)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 107, in model_step
    logits = self.forward(x)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 84, in forward
    return self.net(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\net.py", line 96, in forward
    return self.model(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\container.py", line 217, in forward
    input = module(input)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: mat1 and mat2 shapes cannot be multiplied (128x3072 and 784x256)
[2024-01-30 18:30:53,500][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-01-30_18-30-17
[2024-01-30 18:30:53,502][src.utils.utils][INFO] - Closing wandb!
[2024-01-30 18:34:08,715][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-01-30 18:34:08,722][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-01-30 18:34:08,807][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-01-30 18:34:08,812][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-01-30 18:34:08,833][__main__][INFO] - Instantiating callbacks...
[2024-01-30 18:34:08,833][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-01-30 18:34:08,837][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-01-30 18:34:08,839][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-01-30 18:34:08,840][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-01-30 18:34:08,841][__main__][INFO] - Instantiating loggers...
[2024-01-30 18:34:08,841][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-01-30 18:34:37,771][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-01-30 18:34:38,383][__main__][INFO] - Logging hyperparameters!
[2024-01-30 18:34:38,386][__main__][INFO] - Starting training!
[2024-01-30 18:41:40,642][__main__][INFO] - Starting testing!
[2024-01-30 18:41:48,435][__main__][INFO] - Best ckpt path: D:\data\CIFAR10_logs\logs\train\runs\2024-01-30_18-34-08\checkpoints\epoch_007.ckpt
[2024-01-30 18:41:48,438][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-01-30_18-34-08
[2024-01-30 18:41:48,438][src.utils.utils][INFO] - Closing wandb!
[2024-01-30 18:41:59,837][src.utils.utils][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-01-30 18:47:45,637][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-01-30 18:47:45,646][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-01-30 18:47:45,749][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-01-30 18:47:45,754][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-01-30 18:47:45,772][__main__][INFO] - Instantiating callbacks...
[2024-01-30 18:47:45,773][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-01-30 18:47:45,778][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-01-30 18:47:45,780][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-01-30 18:47:45,780][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-01-30 18:47:45,782][__main__][INFO] - Instantiating loggers...
[2024-01-30 18:47:45,782][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-01-30 18:48:14,583][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-01-30 18:48:15,058][__main__][INFO] - Logging hyperparameters!
[2024-01-30 18:48:15,061][__main__][INFO] - Starting training!
[2024-01-30 18:54:38,844][__main__][INFO] - Starting testing!
[2024-01-30 18:54:44,620][__main__][INFO] - Best ckpt path: D:\data\CIFAR10_logs\logs\train\runs\2024-01-30_18-47-45\checkpoints\epoch_008.ckpt
[2024-01-30 18:54:44,622][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-01-30_18-47-45
[2024-01-30 18:54:44,622][src.utils.utils][INFO] - Closing wandb!
[2024-01-30 18:54:56,282][src.utils.utils][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-01-30 19:22:23,708][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-01-30 19:22:23,715][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-01-30 19:22:23,807][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-01-30 19:22:23,815][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-01-30 19:22:23,838][__main__][INFO] - Instantiating callbacks...
[2024-01-30 19:22:23,839][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-01-30 19:22:23,843][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-01-30 19:22:23,845][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-01-30 19:22:23,846][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-01-30 19:22:23,847][__main__][INFO] - Instantiating loggers...
[2024-01-30 19:22:23,847][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-01-30 19:23:05,212][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-01-30 19:23:05,957][__main__][INFO] - Logging hyperparameters!
[2024-01-30 19:23:05,962][__main__][INFO] - Starting training!
[2024-01-30 19:25:22,012][__main__][INFO] - Starting testing!
[2024-01-31 09:58:07,227][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-01-31 09:58:07,235][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-01-31 09:58:07,313][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-01-31 09:58:07,319][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-01-31 09:58:07,366][__main__][INFO] - Instantiating callbacks...
[2024-01-31 09:58:07,366][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-01-31 09:58:07,371][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-01-31 09:58:07,372][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-01-31 09:58:07,373][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-01-31 09:58:07,374][__main__][INFO] - Instantiating loggers...
[2024-01-31 09:58:07,375][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-01-31 09:58:34,108][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-01-31 09:58:34,644][__main__][INFO] - Logging hyperparameters!
[2024-01-31 09:58:34,647][__main__][INFO] - Starting training!
[2024-01-31 10:03:33,912][__main__][INFO] - Starting testing!
[2024-01-31 10:03:39,322][__main__][INFO] - Best ckpt path: D:\data\CIFAR10_logs\logs\train\runs\2024-01-31_09-58-07\checkpoints\epoch_012.ckpt
[2024-01-31 10:03:39,323][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-01-31_09-58-07
[2024-01-31 10:03:39,324][src.utils.utils][INFO] - Closing wandb!
[2024-01-31 10:04:00,624][src.utils.utils][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-01-31 10:04:20,308][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-01-31 10:04:20,316][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-01-31 10:04:20,402][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-01-31 10:04:20,407][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-01-31 10:04:20,428][__main__][INFO] - Instantiating callbacks...
[2024-01-31 10:04:20,428][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-01-31 10:04:20,434][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-01-31 10:04:20,436][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-01-31 10:04:20,437][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-01-31 10:04:20,438][__main__][INFO] - Instantiating loggers...
[2024-01-31 10:04:20,439][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-01-31 10:04:58,263][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-01-31 10:04:58,899][__main__][INFO] - Logging hyperparameters!
[2024-01-31 10:04:58,903][__main__][INFO] - Starting training!
[2024-01-31 10:11:14,672][__main__][INFO] - Starting testing!
[2024-01-31 10:11:19,809][__main__][INFO] - Best ckpt path: D:\data\CIFAR10_logs\logs\train\runs\2024-01-31_10-04-20\checkpoints\epoch_014.ckpt
[2024-01-31 10:11:19,811][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-01-31_10-04-20
[2024-01-31 10:11:19,812][src.utils.utils][INFO] - Closing wandb!
[2024-01-31 10:11:44,934][src.utils.utils][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-01-31 10:13:26,494][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-01-31 10:13:26,501][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-01-31 10:13:26,633][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-01-31 10:13:26,641][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-01-31 10:13:26,672][__main__][INFO] - Instantiating callbacks...
[2024-01-31 10:13:26,672][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-01-31 10:13:26,679][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-01-31 10:13:26,682][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-01-31 10:13:26,684][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-01-31 10:13:26,686][__main__][INFO] - Instantiating loggers...
[2024-01-31 10:13:26,686][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-01-31 10:13:57,731][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-01-31 10:13:58,426][__main__][INFO] - Logging hyperparameters!
[2024-01-31 10:13:58,431][__main__][INFO] - Starting training!
[2024-01-31 10:15:50,215][__main__][INFO] - Starting testing!
[2024-01-31 10:15:56,346][__main__][INFO] - Best ckpt path: D:\data\CIFAR10_logs\logs\train\runs\2024-01-31_10-13-26\checkpoints\epoch_003.ckpt
[2024-01-31 10:15:56,348][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-01-31_10-13-26
[2024-01-31 10:15:56,348][src.utils.utils][INFO] - Closing wandb!
[2024-01-31 10:16:34,229][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-01-31 10:16:34,236][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-01-31 10:16:34,314][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-01-31 10:16:34,319][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-01-31 10:16:34,337][__main__][INFO] - Instantiating callbacks...
[2024-01-31 10:16:34,337][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-01-31 10:16:34,342][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-01-31 10:16:34,343][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-01-31 10:16:34,344][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-01-31 10:16:34,346][__main__][INFO] - Instantiating loggers...
[2024-01-31 10:16:34,346][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-01-31 10:17:04,306][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-01-31 10:17:04,850][__main__][INFO] - Logging hyperparameters!
[2024-01-31 10:17:04,854][__main__][INFO] - Starting training!
[2024-01-31 10:24:17,709][__main__][INFO] - Starting testing!
[2024-01-31 10:24:23,656][__main__][INFO] - Best ckpt path: D:\data\CIFAR10_logs\logs\train\runs\2024-01-31_10-16-34\checkpoints\epoch_010.ckpt
[2024-01-31 10:24:23,657][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-01-31_10-16-34
[2024-01-31 10:24:23,658][src.utils.utils][INFO] - Closing wandb!
[2024-01-31 10:24:48,584][src.utils.utils][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-01-31 10:53:11,654][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-01-31 10:53:11,661][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-01-31 10:53:11,769][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-01-31 10:53:11,774][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-01-31 10:53:11,793][__main__][INFO] - Instantiating callbacks...
[2024-01-31 10:53:11,793][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-01-31 10:53:11,798][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-01-31 10:53:11,800][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-01-31 10:53:11,800][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-01-31 10:53:11,801][__main__][INFO] - Instantiating loggers...
[2024-01-31 10:53:11,802][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-01-31 10:53:46,453][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-01-31 10:53:47,082][__main__][INFO] - Logging hyperparameters!
[2024-01-31 10:53:47,086][__main__][INFO] - Starting training!
[2024-01-31 10:53:51,642][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 949, in _run
    self.strategy.setup(self)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\single_device.py", line 75, in setup
    super().setup(trainer)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 149, in setup
    self.setup_optimizers(trainer)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 139, in setup_optimizers
    self.optimizers, self.lr_scheduler_configs = _init_optimizers_and_lr_schedulers(self.lightning_module)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\optimizer.py", line 168, in _init_optimizers_and_lr_schedulers
    optim_conf = call._call_lightning_module_hook(model.trainer, "configure_optimizers", pl_module=model)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 144, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 188, in configure_optimizers
    optimizer = self.hparams.optimizer(params=self.parameters())
TypeError: __init__() got an unexpected keyword argument 'weight_decay'
[2024-01-31 10:53:51,648][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-01-31_10-53-11
[2024-01-31 10:53:51,649][src.utils.utils][INFO] - Closing wandb!
[2024-01-31 10:54:45,147][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-01-31 10:54:45,156][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-01-31 10:54:45,250][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-01-31 10:54:45,255][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-01-31 10:54:45,274][__main__][INFO] - Instantiating callbacks...
[2024-01-31 10:54:45,276][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-01-31 10:54:45,281][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-01-31 10:54:45,284][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-01-31 10:54:45,285][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-01-31 10:54:45,287][__main__][INFO] - Instantiating loggers...
[2024-01-31 10:54:45,288][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-01-31 10:55:15,735][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-01-31 10:55:16,228][__main__][INFO] - Logging hyperparameters!
[2024-01-31 10:55:16,231][__main__][INFO] - Starting training!
[2024-01-31 10:55:20,811][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 949, in _run
    self.strategy.setup(self)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\single_device.py", line 75, in setup
    super().setup(trainer)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 149, in setup
    self.setup_optimizers(trainer)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 139, in setup_optimizers
    self.optimizers, self.lr_scheduler_configs = _init_optimizers_and_lr_schedulers(self.lightning_module)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\optimizer.py", line 168, in _init_optimizers_and_lr_schedulers
    optim_conf = call._call_lightning_module_hook(model.trainer, "configure_optimizers", pl_module=model)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 144, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 188, in configure_optimizers
    optimizer = self.hparams.optimizer(params=self.parameters())
TypeError: __init__() got an unexpected keyword argument 'params'
[2024-01-31 10:55:20,815][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-01-31_10-54-44
[2024-01-31 10:55:20,816][src.utils.utils][INFO] - Closing wandb!
[2024-01-31 10:56:43,851][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-01-31 10:56:43,860][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-01-31 10:56:43,942][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-01-31 10:56:43,947][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-01-31 10:56:43,964][__main__][INFO] - Instantiating callbacks...
[2024-01-31 10:56:43,965][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-01-31 10:56:43,970][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-01-31 10:56:43,971][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-01-31 10:56:43,972][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-01-31 10:56:43,973][__main__][INFO] - Instantiating loggers...
[2024-01-31 10:56:43,973][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-01-31 10:57:18,110][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-01-31 10:57:18,797][__main__][INFO] - Logging hyperparameters!
[2024-01-31 10:57:18,805][__main__][INFO] - Starting training!
[2024-01-31 10:57:23,945][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 949, in _run
    self.strategy.setup(self)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\single_device.py", line 75, in setup
    super().setup(trainer)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 149, in setup
    self.setup_optimizers(trainer)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 139, in setup_optimizers
    self.optimizers, self.lr_scheduler_configs = _init_optimizers_and_lr_schedulers(self.lightning_module)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\optimizer.py", line 168, in _init_optimizers_and_lr_schedulers
    optim_conf = call._call_lightning_module_hook(model.trainer, "configure_optimizers", pl_module=model)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 144, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 190, in configure_optimizers
    scheduler = self.hparams.scheduler(optimizer=optimizer)
  File "D:\Anaconda3\lib\site-packages\torch\optim\lr_scheduler.py", line 973, in __init__
    raise TypeError('{} is not an Optimizer'.format(
TypeError: NesterovOptimizer is not an Optimizer
[2024-01-31 10:57:23,950][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-01-31_10-56-43
[2024-01-31 10:57:23,950][src.utils.utils][INFO] - Closing wandb!
[2024-01-31 10:58:25,692][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-01-31 10:58:25,702][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-01-31 10:58:25,790][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-01-31 10:58:25,795][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-01-31 10:58:25,815][__main__][INFO] - Instantiating callbacks...
[2024-01-31 10:58:25,815][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-01-31 10:58:25,819][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-01-31 10:58:25,821][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-01-31 10:58:25,822][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-01-31 10:58:25,823][__main__][INFO] - Instantiating loggers...
[2024-01-31 10:58:25,823][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-01-31 10:58:55,462][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-01-31 10:58:56,105][__main__][INFO] - Logging hyperparameters!
[2024-01-31 10:58:56,108][__main__][INFO] - Starting training!
[2024-01-31 10:59:03,178][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1016, in _run_stage
    self.fit_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 201, in run
    self.advance()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 354, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 133, in run
    self.advance(data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 218, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 185, in run
    self._optimizer_step(kwargs.get("batch_idx", 0), closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 260, in _optimizer_step
    call._call_lightning_module_hook(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 144, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\module.py", line 1256, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\optimizer.py", line 155, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 225, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\plugins\precision\precision_plugin.py", line 114, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\optim\optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\optim\ZW.py", line 98, in step
    p.add_(buf, alpha=-group['lr'])
RuntimeError: a leaf Variable that requires grad is being used in an in-place operation.
[2024-01-31 10:59:03,185][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-01-31_10-58-25
[2024-01-31 10:59:03,185][src.utils.utils][INFO] - Closing wandb!
[2024-01-31 11:00:04,350][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-01-31 11:00:04,366][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-01-31 11:00:04,500][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-01-31 11:00:04,507][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-01-31 11:00:04,631][__main__][INFO] - Instantiating callbacks...
[2024-01-31 11:00:04,632][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-01-31 11:00:04,641][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-01-31 11:00:04,646][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-01-31 11:00:04,665][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-01-31 11:00:04,667][__main__][INFO] - Instantiating loggers...
[2024-01-31 11:00:04,668][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-01-31 11:00:42,236][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-01-31 11:00:42,890][__main__][INFO] - Logging hyperparameters!
[2024-01-31 11:00:42,893][__main__][INFO] - Starting training!
[2024-01-31 11:00:50,695][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1016, in _run_stage
    self.fit_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 201, in run
    self.advance()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 354, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 133, in run
    self.advance(data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 218, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 185, in run
    self._optimizer_step(kwargs.get("batch_idx", 0), closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 260, in _optimizer_step
    call._call_lightning_module_hook(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 144, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\module.py", line 1256, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\optimizer.py", line 155, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 225, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\plugins\precision\precision_plugin.py", line 114, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\optim\optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\optim\ZW.py", line 99, in step
    p.add_(momentum, alpha=-group['lr'])
RuntimeError: a leaf Variable that requires grad is being used in an in-place operation.
[2024-01-31 11:00:50,699][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-01-31_11-00-04
[2024-01-31 11:00:50,700][src.utils.utils][INFO] - Closing wandb!
[2024-01-31 11:01:52,597][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-01-31 11:01:52,605][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-01-31 11:01:52,686][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-01-31 11:01:52,691][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-01-31 11:01:52,709][__main__][INFO] - Instantiating callbacks...
[2024-01-31 11:01:52,710][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-01-31 11:01:52,714][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-01-31 11:01:52,715][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-01-31 11:01:52,715][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-01-31 11:01:52,717][__main__][INFO] - Instantiating loggers...
[2024-01-31 11:01:52,717][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-01-31 11:02:21,592][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-01-31 11:02:22,503][__main__][INFO] - Logging hyperparameters!
[2024-01-31 11:02:22,508][__main__][INFO] - Starting training!
[2024-01-31 11:02:29,672][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1016, in _run_stage
    self.fit_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 201, in run
    self.advance()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 354, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 133, in run
    self.advance(data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 218, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 185, in run
    self._optimizer_step(kwargs.get("batch_idx", 0), closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 260, in _optimizer_step
    call._call_lightning_module_hook(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 144, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\module.py", line 1256, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\optimizer.py", line 155, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 225, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\plugins\precision\precision_plugin.py", line 114, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\optim\optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\optim\ZW.py", line 99, in step
    p.add_(momentum, alpha=-group['lr'])
RuntimeError: a leaf Variable that requires grad is being used in an in-place operation.
[2024-01-31 11:02:29,678][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-01-31_11-01-52
[2024-01-31 11:02:29,678][src.utils.utils][INFO] - Closing wandb!
[2024-01-31 11:03:19,782][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-01-31 11:03:19,792][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-01-31 11:03:19,914][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-01-31 11:03:19,920][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-01-31 11:03:19,941][__main__][INFO] - Instantiating callbacks...
[2024-01-31 11:03:19,941][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-01-31 11:03:19,945][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-01-31 11:03:19,947][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-01-31 11:03:19,948][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-01-31 11:03:19,950][__main__][INFO] - Instantiating loggers...
[2024-01-31 11:03:19,951][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-01-31 11:04:00,842][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-01-31 11:04:01,545][__main__][INFO] - Logging hyperparameters!
[2024-01-31 11:04:01,549][__main__][INFO] - Starting training!
[2024-01-31 11:04:09,501][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1016, in _run_stage
    self.fit_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 201, in run
    self.advance()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 354, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 133, in run
    self.advance(data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 218, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 185, in run
    self._optimizer_step(kwargs.get("batch_idx", 0), closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 260, in _optimizer_step
    call._call_lightning_module_hook(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 144, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\module.py", line 1256, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\optimizer.py", line 155, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 225, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\plugins\precision\precision_plugin.py", line 114, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\optim\optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\optim\ZW.py", line 98, in step
    p.add_(momentum, alpha=-group['lr'])
RuntimeError: a leaf Variable that requires grad is being used in an in-place operation.
[2024-01-31 11:04:09,507][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-01-31_11-03-19
[2024-01-31 11:04:09,508][src.utils.utils][INFO] - Closing wandb!
[2024-01-31 11:05:43,375][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-01-31 11:05:43,388][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-01-31 11:05:43,486][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-01-31 11:05:43,492][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-01-31 11:05:43,515][__main__][INFO] - Instantiating callbacks...
[2024-01-31 11:05:43,516][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-01-31 11:05:43,521][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-01-31 11:05:43,523][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-01-31 11:05:43,524][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-01-31 11:05:43,525][__main__][INFO] - Instantiating loggers...
[2024-01-31 11:05:43,526][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-01-31 11:06:53,485][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 92, in _call_target
    return _target_(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loggers\wandb.py", line 358, in __init__
    _ = self.experiment
  File "D:\Anaconda3\lib\site-packages\lightning\fabric\loggers\logger.py", line 114, in experiment
    return fn(self)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loggers\wandb.py", line 406, in experiment
    self._experiment = wandb.init(**self._wandb_init)
  File "D:\Anaconda3\lib\site-packages\wandb\sdk\wandb_init.py", line 1166, in init
    raise e
  File "D:\Anaconda3\lib\site-packages\wandb\sdk\wandb_init.py", line 1147, in init
    run = wi.init()
  File "D:\Anaconda3\lib\site-packages\wandb\sdk\wandb_init.py", line 762, in init
    raise error
wandb.errors.CommError: Run initialization has timed out after 60.0 sec. 
Please refer to the documentation for additional information: https://docs.wandb.ai/guides/track/tracking-faq#initstarterror-error-communicating-with-wandb-process-

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 59, in train
    logger: List[Logger] = utils.instantiate_loggers(cfg.get("logger"))
  File "D:\pycharmproject\template\src\utils\instantiators.py", line 48, in instantiate_loggers
    logger.append(hydra.utils.instantiate(lg_conf))
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 226, in instantiate
    return instantiate_node(
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 347, in instantiate_node
    return _call_target(_target_, partial, args, kwargs, full_key)
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 97, in _call_target
    raise InstantiationException(msg) from e
hydra.errors.InstantiationException: Error in call to target 'lightning.pytorch.loggers.wandb.WandbLogger':
CommError('Run initialization has timed out after 60.0 sec. \nPlease refer to the documentation for additional information: https://docs.wandb.ai/guides/track/tracking-faq#initstarterror-error-communicating-with-wandb-process-')
full_key: logger.wandb
[2024-01-31 11:06:53,489][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-01-31_11-05-43
[2024-01-31 11:06:53,857][urllib3.connectionpool][WARNING] - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProxyError('Cannot connect to proxy.', ConnectionResetError(10054, '', None, 10054, None))': /api/4504800232407040/store/
[2024-01-31 11:06:56,246][urllib3.connectionpool][WARNING] - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProxyError('Cannot connect to proxy.', ConnectionResetError(10054, '', None, 10054, None))': /api/4504800232407040/store/
[2024-01-31 11:06:58,618][urllib3.connectionpool][WARNING] - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProxyError('Cannot connect to proxy.', ConnectionResetError(10054, '', None, 10054, None))': /api/4504800232407040/store/
[2024-01-31 11:07:47,509][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-01-31 11:07:47,519][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-01-31 11:07:47,604][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-01-31 11:07:47,611][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-01-31 11:07:47,637][__main__][INFO] - Instantiating callbacks...
[2024-01-31 11:07:47,638][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-01-31 11:07:47,642][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-01-31 11:07:47,643][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-01-31 11:07:47,644][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-01-31 11:07:47,648][__main__][INFO] - Instantiating loggers...
[2024-01-31 11:07:47,649][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-01-31 11:08:23,764][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-01-31 11:08:24,520][__main__][INFO] - Logging hyperparameters!
[2024-01-31 11:08:24,523][__main__][INFO] - Starting training!
[2024-01-31 11:08:33,135][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1016, in _run_stage
    self.fit_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 201, in run
    self.advance()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 354, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 133, in run
    self.advance(data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 218, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 185, in run
    self._optimizer_step(kwargs.get("batch_idx", 0), closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 260, in _optimizer_step
    call._call_lightning_module_hook(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 144, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\module.py", line 1256, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\optimizer.py", line 155, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 225, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\plugins\precision\precision_plugin.py", line 114, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\optim\optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\optim\ZW.py", line 99, in step
    p.add_(momentum, alpha=-group['lr'])
RuntimeError: a leaf Variable that requires grad is being used in an in-place operation.
[2024-01-31 11:08:33,141][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-01-31_11-07-47
[2024-01-31 11:08:33,141][src.utils.utils][INFO] - Closing wandb!
[2024-01-31 13:55:30,891][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-01-31 13:55:30,918][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-01-31 13:55:31,120][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-01-31 13:55:31,132][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-01-31 13:55:31,215][__main__][INFO] - Instantiating callbacks...
[2024-01-31 13:55:31,215][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-01-31 13:55:31,228][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-01-31 13:55:31,232][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-01-31 13:55:31,235][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-01-31 13:55:31,241][__main__][INFO] - Instantiating loggers...
[2024-01-31 13:55:31,242][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-01-31 13:56:09,270][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-01-31 13:56:10,140][__main__][INFO] - Logging hyperparameters!
[2024-01-31 13:56:10,145][__main__][INFO] - Starting training!
[2024-01-31 13:56:22,958][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1016, in _run_stage
    self.fit_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 201, in run
    self.advance()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 354, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 133, in run
    self.advance(data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 218, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 185, in run
    self._optimizer_step(kwargs.get("batch_idx", 0), closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 260, in _optimizer_step
    call._call_lightning_module_hook(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 144, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\module.py", line 1256, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\optimizer.py", line 155, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 225, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\plugins\precision\precision_plugin.py", line 114, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\optim\optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\optim\ZW.py", line 99, in step
    p.add_(momentum, alpha=-group['lr'])
RuntimeError: a leaf Variable that requires grad is being used in an in-place operation.
[2024-01-31 13:56:22,967][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-01-31_13-55-30
[2024-01-31 13:56:22,967][src.utils.utils][INFO] - Closing wandb!
[2024-01-31 13:59:41,950][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-01-31 13:59:41,958][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-01-31 13:59:42,068][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-01-31 13:59:42,075][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-01-31 13:59:42,113][__main__][INFO] - Instantiating callbacks...
[2024-01-31 13:59:42,114][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-01-31 13:59:42,124][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-01-31 13:59:42,128][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-01-31 13:59:42,129][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-01-31 13:59:42,132][__main__][INFO] - Instantiating loggers...
[2024-01-31 13:59:42,132][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-01-31 14:00:17,521][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-01-31 14:00:18,291][__main__][INFO] - Logging hyperparameters!
[2024-01-31 14:00:18,294][__main__][INFO] - Starting training!
[2024-01-31 14:00:25,811][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1016, in _run_stage
    self.fit_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 201, in run
    self.advance()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 354, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 133, in run
    self.advance(data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 218, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 185, in run
    self._optimizer_step(kwargs.get("batch_idx", 0), closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 260, in _optimizer_step
    call._call_lightning_module_hook(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 144, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\module.py", line 1256, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\optimizer.py", line 155, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 225, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\plugins\precision\precision_plugin.py", line 114, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\optim\optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\optim\ZW.py", line 97, in step
    p.add_(momentum, alpha=-group['lr'])
RuntimeError: a leaf Variable that requires grad is being used in an in-place operation.
[2024-01-31 14:00:25,818][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-01-31_13-59-41
[2024-01-31 14:00:25,818][src.utils.utils][INFO] - Closing wandb!
[2024-01-31 14:01:53,448][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-01-31 14:01:53,462][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-01-31 14:01:53,577][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-01-31 14:01:53,585][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-01-31 14:01:53,608][__main__][INFO] - Instantiating callbacks...
[2024-01-31 14:01:53,608][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-01-31 14:01:53,614][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-01-31 14:01:53,616][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-01-31 14:01:53,617][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-01-31 14:01:53,621][__main__][INFO] - Instantiating loggers...
[2024-01-31 14:01:53,621][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-01-31 14:02:38,647][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-01-31 14:02:39,340][__main__][INFO] - Logging hyperparameters!
[2024-01-31 14:02:39,343][__main__][INFO] - Starting training!
[2024-01-31 14:02:46,578][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1016, in _run_stage
    self.fit_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 201, in run
    self.advance()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 354, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 133, in run
    self.advance(data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 218, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 185, in run
    self._optimizer_step(kwargs.get("batch_idx", 0), closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 260, in _optimizer_step
    call._call_lightning_module_hook(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 144, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\module.py", line 1256, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\optimizer.py", line 155, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 225, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\plugins\precision\precision_plugin.py", line 114, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\optim\optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\optim\ZW.py", line 97, in step
    p.add_(momentum, alpha=-group['lr'])
RuntimeError: a leaf Variable that requires grad is being used in an in-place operation.
[2024-01-31 14:02:46,585][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-01-31_14-01-53
[2024-01-31 14:02:46,585][src.utils.utils][INFO] - Closing wandb!
[2024-01-31 14:03:26,274][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-01-31 14:03:26,282][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-01-31 14:03:26,380][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-01-31 14:03:26,386][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-01-31 14:03:26,413][__main__][INFO] - Instantiating callbacks...
[2024-01-31 14:03:26,413][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-01-31 14:03:26,418][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-01-31 14:03:26,421][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-01-31 14:03:26,423][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-01-31 14:03:26,424][__main__][INFO] - Instantiating loggers...
[2024-01-31 14:03:26,425][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-01-31 14:04:00,114][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-01-31 14:04:00,807][__main__][INFO] - Logging hyperparameters!
[2024-01-31 14:04:00,809][__main__][INFO] - Starting training!
[2024-01-31 14:05:49,703][__main__][INFO] - Starting testing!
[2024-02-01 21:08:08,175][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-01 21:08:08,192][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-01 21:08:08,283][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-01 21:08:08,289][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-01 21:08:08,355][__main__][INFO] - Instantiating callbacks...
[2024-02-01 21:08:08,355][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-01 21:08:08,357][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-01 21:08:08,362][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-01 21:08:08,362][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-01 21:08:08,365][__main__][INFO] - Instantiating loggers...
[2024-02-01 21:08:08,365][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-01 21:08:56,866][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-01 21:08:57,605][__main__][INFO] - Logging hyperparameters!
[2024-02-01 21:08:57,613][__main__][INFO] - Starting training!
[2024-02-01 21:19:54,604][__main__][INFO] - Starting testing!
[2024-02-01 21:20:02,107][__main__][INFO] - Best ckpt path: D:\data\CIFAR10_logs\logs\train\runs\2024-02-01_21-08-08\checkpoints\epoch_008.ckpt
[2024-02-01 21:20:02,109][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-02-01_21-08-08
[2024-02-01 21:20:02,110][src.utils.utils][INFO] - Closing wandb!
[2024-02-01 21:20:28,513][src.utils.utils][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-02-01 21:50:47,650][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-01 21:50:47,663][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-01 21:50:47,823][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-01 21:50:47,831][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-01 21:50:47,858][__main__][INFO] - Instantiating callbacks...
[2024-02-01 21:50:47,858][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-01 21:50:47,865][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-01 21:50:47,868][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-01 21:50:47,868][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-01 21:50:47,873][__main__][INFO] - Instantiating loggers...
[2024-02-01 21:50:47,873][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-01 21:51:19,298][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-01 21:51:19,935][__main__][INFO] - Logging hyperparameters!
[2024-02-01 21:51:19,938][__main__][INFO] - Starting training!
[2024-02-01 22:00:20,843][__main__][INFO] - Starting testing!
[2024-02-01 22:00:26,919][__main__][INFO] - Best ckpt path: D:\data\CIFAR10_logs\logs\train\runs\2024-02-01_21-50-47\checkpoints\epoch_007.ckpt
[2024-02-01 22:00:26,924][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-02-01_21-50-47
[2024-02-01 22:00:26,926][src.utils.utils][INFO] - Closing wandb!
[2024-02-01 22:01:01,427][src.utils.utils][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-02-03 11:14:56,617][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-03 11:14:56,627][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-03 11:14:56,715][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-03 11:14:56,721][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-03 11:14:56,777][__main__][INFO] - Instantiating callbacks...
[2024-02-03 11:14:56,778][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-03 11:14:56,783][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-03 11:14:56,784][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-03 11:14:56,785][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-03 11:14:56,786][__main__][INFO] - Instantiating loggers...
[2024-02-03 11:14:56,787][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-03 11:15:45,607][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-03 11:15:46,402][__main__][INFO] - Logging hyperparameters!
[2024-02-03 11:15:46,408][__main__][INFO] - Starting training!
[2024-02-03 11:24:53,190][__main__][INFO] - Starting testing!
[2024-02-03 11:25:00,598][__main__][INFO] - Best ckpt path: D:\data\CIFAR10_logs\logs\train\runs\2024-02-03_11-14-56\checkpoints\epoch_008.ckpt
[2024-02-03 11:25:00,599][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-02-03_11-14-56
[2024-02-03 11:25:00,600][src.utils.utils][INFO] - Closing wandb!
[2024-02-03 11:25:22,913][src.utils.utils][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-02-03 20:23:22,529][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-03 20:23:22,536][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-03 20:23:22,619][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-03 20:23:22,625][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-03 20:23:22,717][__main__][INFO] - Instantiating callbacks...
[2024-02-03 20:23:22,717][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-03 20:23:22,721][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-03 20:23:22,723][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-03 20:23:22,723][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-03 20:23:22,724][__main__][INFO] - Instantiating loggers...
[2024-02-03 20:23:22,724][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-03 20:23:52,474][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-03 20:23:53,114][__main__][INFO] - Logging hyperparameters!
[2024-02-03 20:23:53,117][__main__][INFO] - Starting training!
[2024-02-03 20:24:05,097][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1016, in _run_stage
    self.fit_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 201, in run
    self.advance()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 354, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 133, in run
    self.advance(data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 218, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 185, in run
    self._optimizer_step(kwargs.get("batch_idx", 0), closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 260, in _optimizer_step
    call._call_lightning_module_hook(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 144, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\module.py", line 1256, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\optimizer.py", line 155, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 225, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\plugins\precision\precision_plugin.py", line 114, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\optim\optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\optim\ZW.py", line 66, in step
    x = torch.tensor(len(curvature_avg), requires_grad=True)
RuntimeError: Only Tensors of floating point and complex dtype can require gradients
[2024-02-03 20:24:05,105][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-02-03_20-23-22
[2024-02-03 20:24:05,106][src.utils.utils][INFO] - Closing wandb!
[2024-02-03 20:25:58,143][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-03 20:25:58,153][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-03 20:25:58,249][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-03 20:25:58,254][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-03 20:25:58,277][__main__][INFO] - Instantiating callbacks...
[2024-02-03 20:25:58,277][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-03 20:25:58,283][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-03 20:25:58,285][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-03 20:25:58,286][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-03 20:25:58,288][__main__][INFO] - Instantiating loggers...
[2024-02-03 20:25:58,288][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-03 20:26:29,286][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-03 20:26:29,966][__main__][INFO] - Logging hyperparameters!
[2024-02-03 20:26:29,969][__main__][INFO] - Starting training!
[2024-02-03 20:26:36,165][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1016, in _run_stage
    self.fit_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 201, in run
    self.advance()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 354, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 133, in run
    self.advance(data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 218, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 185, in run
    self._optimizer_step(kwargs.get("batch_idx", 0), closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 260, in _optimizer_step
    call._call_lightning_module_hook(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 144, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\module.py", line 1256, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\optimizer.py", line 155, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 225, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\plugins\precision\precision_plugin.py", line 114, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\optim\optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\optim\ZW.py", line 67, in step
    derivative_approximation = torch.autograd.grad(y_values, x)[0].item()
  File "D:\Anaconda3\lib\site-packages\torch\autograd\__init__.py", line 303, in grad
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
[2024-02-03 20:26:36,170][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-02-03_20-25-58
[2024-02-03 20:26:36,170][src.utils.utils][INFO] - Closing wandb!
[2024-02-03 20:28:08,878][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-03 20:28:08,885][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-03 20:28:08,976][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-03 20:28:08,980][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-03 20:28:08,998][__main__][INFO] - Instantiating callbacks...
[2024-02-03 20:28:08,999][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-03 20:28:09,003][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-03 20:28:09,005][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-03 20:28:09,005][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-03 20:28:09,006][__main__][INFO] - Instantiating loggers...
[2024-02-03 20:28:09,007][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-03 20:28:36,267][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-03 20:28:36,888][__main__][INFO] - Logging hyperparameters!
[2024-02-03 20:28:36,890][__main__][INFO] - Starting training!
[2024-02-03 20:28:42,981][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1016, in _run_stage
    self.fit_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 201, in run
    self.advance()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 354, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 133, in run
    self.advance(data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 218, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 185, in run
    self._optimizer_step(kwargs.get("batch_idx", 0), closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 260, in _optimizer_step
    call._call_lightning_module_hook(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 144, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\module.py", line 1256, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\optimizer.py", line 155, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 225, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\plugins\precision\precision_plugin.py", line 114, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\optim\optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\optim\ZW.py", line 68, in step
    derivative_approximation = torch.autograd.grad(y_values, x)[0].item()
  File "D:\Anaconda3\lib\site-packages\torch\autograd\__init__.py", line 288, in grad
    grad_outputs_ = _make_grads(t_outputs, grad_outputs_, is_grads_batched=is_grads_batched)
  File "D:\Anaconda3\lib\site-packages\torch\autograd\__init__.py", line 88, in _make_grads
    raise RuntimeError("grad can be implicitly created only for scalar outputs")
RuntimeError: grad can be implicitly created only for scalar outputs
[2024-02-03 20:28:42,985][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-02-03_20-28-08
[2024-02-03 20:28:42,986][src.utils.utils][INFO] - Closing wandb!
[2024-02-03 20:30:26,373][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-03 20:30:26,381][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-03 20:30:26,468][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-03 20:30:26,472][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-03 20:30:26,491][__main__][INFO] - Instantiating callbacks...
[2024-02-03 20:30:26,491][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-03 20:30:26,495][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-03 20:30:26,496][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-03 20:30:26,497][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-03 20:30:26,498][__main__][INFO] - Instantiating loggers...
[2024-02-03 20:30:26,498][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-03 20:30:55,678][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-03 20:30:56,329][__main__][INFO] - Logging hyperparameters!
[2024-02-03 20:30:56,332][__main__][INFO] - Starting training!
[2024-02-03 20:31:02,381][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1016, in _run_stage
    self.fit_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 201, in run
    self.advance()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 354, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 133, in run
    self.advance(data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 218, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 185, in run
    self._optimizer_step(kwargs.get("batch_idx", 0), closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 260, in _optimizer_step
    call._call_lightning_module_hook(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 144, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\module.py", line 1256, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\optimizer.py", line 155, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 225, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\plugins\precision\precision_plugin.py", line 114, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\optim\optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\optim\ZW.py", line 70, in step
    derivative_approximation = torch.autograd.grad(interpolated_value, x)[0].item()
  File "D:\Anaconda3\lib\site-packages\torch\autograd\__init__.py", line 288, in grad
    grad_outputs_ = _make_grads(t_outputs, grad_outputs_, is_grads_batched=is_grads_batched)
  File "D:\Anaconda3\lib\site-packages\torch\autograd\__init__.py", line 88, in _make_grads
    raise RuntimeError("grad can be implicitly created only for scalar outputs")
RuntimeError: grad can be implicitly created only for scalar outputs
[2024-02-03 20:31:02,385][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-02-03_20-30-26
[2024-02-03 20:31:02,386][src.utils.utils][INFO] - Closing wandb!
[2024-02-03 20:33:06,456][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-03 20:33:06,463][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-03 20:33:06,545][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-03 20:33:06,550][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-03 20:33:06,568][__main__][INFO] - Instantiating callbacks...
[2024-02-03 20:33:06,568][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-03 20:33:06,572][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-03 20:33:06,573][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-03 20:33:06,574][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-03 20:33:06,575][__main__][INFO] - Instantiating loggers...
[2024-02-03 20:33:06,575][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-03 20:33:36,215][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-03 20:33:36,983][__main__][INFO] - Logging hyperparameters!
[2024-02-03 20:33:36,985][__main__][INFO] - Starting training!
[2024-02-03 20:33:44,791][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1016, in _run_stage
    self.fit_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 201, in run
    self.advance()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 354, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 133, in run
    self.advance(data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 218, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 185, in run
    self._optimizer_step(kwargs.get("batch_idx", 0), closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 260, in _optimizer_step
    call._call_lightning_module_hook(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 144, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\module.py", line 1256, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\optimizer.py", line 155, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 225, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\plugins\precision\precision_plugin.py", line 114, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\optim\optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\optim\ZW.py", line 60, in step
    p.data.add_(-step_size * derivative_approximation, grad)
TypeError: add_() takes 1 positional argument but 2 were given
[2024-02-03 20:33:44,796][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-02-03_20-33-06
[2024-02-03 20:33:44,797][src.utils.utils][INFO] - Closing wandb!
[2024-02-03 20:34:23,984][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-03 20:34:23,992][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-03 20:34:24,083][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-03 20:34:24,088][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-03 20:34:24,107][__main__][INFO] - Instantiating callbacks...
[2024-02-03 20:34:24,108][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-03 20:34:24,113][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-03 20:34:24,115][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-03 20:34:24,115][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-03 20:34:24,117][__main__][INFO] - Instantiating loggers...
[2024-02-03 20:34:24,117][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-03 20:34:51,101][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-03 20:34:51,540][__main__][INFO] - Logging hyperparameters!
[2024-02-03 20:34:51,544][__main__][INFO] - Starting training!
[2024-02-03 20:34:57,589][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1016, in _run_stage
    self.fit_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 201, in run
    self.advance()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 354, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 133, in run
    self.advance(data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 218, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 185, in run
    self._optimizer_step(kwargs.get("batch_idx", 0), closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 260, in _optimizer_step
    call._call_lightning_module_hook(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 144, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\module.py", line 1256, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\optimizer.py", line 155, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 225, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\plugins\precision\precision_plugin.py", line 114, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\optim\optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\optim\ZW.py", line 62, in step
    p.data.add_(-step_size * derivative_approximation, grad)
TypeError: add_() takes 1 positional argument but 2 were given
[2024-02-03 20:34:57,592][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-02-03_20-34-23
[2024-02-03 20:34:57,594][src.utils.utils][INFO] - Closing wandb!
[2024-02-03 20:35:48,668][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-03 20:35:48,677][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-03 20:35:48,756][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-03 20:35:48,761][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-03 20:35:48,776][__main__][INFO] - Instantiating callbacks...
[2024-02-03 20:35:48,776][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-03 20:35:48,780][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-03 20:35:48,782][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-03 20:35:48,782][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-03 20:35:48,784][__main__][INFO] - Instantiating loggers...
[2024-02-03 20:35:48,784][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-03 20:36:17,893][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-03 20:36:18,467][__main__][INFO] - Logging hyperparameters!
[2024-02-03 20:36:18,471][__main__][INFO] - Starting training!
[2024-02-03 20:36:24,547][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1016, in _run_stage
    self.fit_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 201, in run
    self.advance()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 354, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 133, in run
    self.advance(data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 218, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 185, in run
    self._optimizer_step(kwargs.get("batch_idx", 0), closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 260, in _optimizer_step
    call._call_lightning_module_hook(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 144, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\module.py", line 1256, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\optimizer.py", line 155, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 225, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\plugins\precision\precision_plugin.py", line 114, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\optim\optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\optim\ZW.py", line 62, in step
    p.data.add_(-step_size * derivative_approximation, grad)
TypeError: add_() takes 1 positional argument but 2 were given
[2024-02-03 20:36:24,551][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-02-03_20-35-48
[2024-02-03 20:36:24,551][src.utils.utils][INFO] - Closing wandb!
[2024-02-03 20:37:09,003][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-03 20:37:09,009][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-03 20:37:09,081][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-03 20:37:09,085][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-03 20:37:09,103][__main__][INFO] - Instantiating callbacks...
[2024-02-03 20:37:09,103][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-03 20:37:09,107][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-03 20:37:09,108][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-03 20:37:09,110][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-03 20:37:09,111][__main__][INFO] - Instantiating loggers...
[2024-02-03 20:37:09,111][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-03 20:37:38,946][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-03 20:37:39,370][__main__][INFO] - Logging hyperparameters!
[2024-02-03 20:37:39,373][__main__][INFO] - Starting training!
[2024-02-03 20:38:45,543][__main__][INFO] - Starting testing!
[2024-02-04 16:24:19,874][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-04 16:24:19,890][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-04 16:24:20,047][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-04 16:24:20,058][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-04 16:24:20,141][__main__][INFO] - Instantiating callbacks...
[2024-02-04 16:24:20,141][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-04 16:24:20,152][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-04 16:24:20,154][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-04 16:24:20,155][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-04 16:24:20,158][__main__][INFO] - Instantiating loggers...
[2024-02-04 16:24:20,159][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-04 16:25:02,622][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-04 16:25:03,543][__main__][INFO] - Logging hyperparameters!
[2024-02-04 16:25:03,548][__main__][INFO] - Starting training!
[2024-02-04 16:25:19,695][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1016, in _run_stage
    self.fit_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 201, in run
    self.advance()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 354, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 133, in run
    self.advance(data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 218, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 185, in run
    self._optimizer_step(kwargs.get("batch_idx", 0), closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 260, in _optimizer_step
    call._call_lightning_module_hook(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 144, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\module.py", line 1256, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\optimizer.py", line 155, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 225, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\plugins\precision\precision_plugin.py", line 114, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\optim\optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\optim\ZW.py", line 64, in step
    p.data.addcdiv_(-step_size, corrected_exp_avg, denom)
TypeError: addcdiv_() takes 2 positional arguments but 3 were given
[2024-02-04 16:25:19,712][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-02-04_16-24-19
[2024-02-04 16:25:19,713][src.utils.utils][INFO] - Closing wandb!
[2024-02-04 16:27:04,351][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-04 16:27:04,358][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-04 16:27:04,436][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-04 16:27:04,441][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-04 16:27:04,459][__main__][INFO] - Instantiating callbacks...
[2024-02-04 16:27:04,459][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-04 16:27:04,466][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-04 16:27:04,468][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-04 16:27:04,469][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-04 16:27:04,471][__main__][INFO] - Instantiating loggers...
[2024-02-04 16:27:04,471][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-04 16:27:37,085][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-04 16:27:37,942][__main__][INFO] - Logging hyperparameters!
[2024-02-04 16:27:37,949][__main__][INFO] - Starting training!
[2024-02-04 16:27:49,961][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1016, in _run_stage
    self.fit_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 201, in run
    self.advance()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 354, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 133, in run
    self.advance(data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 218, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 185, in run
    self._optimizer_step(kwargs.get("batch_idx", 0), closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 260, in _optimizer_step
    call._call_lightning_module_hook(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 144, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\module.py", line 1256, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\optimizer.py", line 155, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 225, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\plugins\precision\precision_plugin.py", line 114, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\optim\optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\optim\ZW.py", line 64, in step
    p.data.addcdiv_(-step_size, corrected_exp_avg, denom)
TypeError: addcdiv_() takes 2 positional arguments but 3 were given
[2024-02-04 16:27:49,968][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-02-04_16-27-04
[2024-02-04 16:27:49,969][src.utils.utils][INFO] - Closing wandb!
[2024-02-04 16:28:31,578][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-04 16:28:31,585][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-04 16:28:31,691][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-04 16:28:31,697][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-04 16:28:31,715][__main__][INFO] - Instantiating callbacks...
[2024-02-04 16:28:31,715][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-04 16:28:31,720][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-04 16:28:31,722][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-04 16:28:31,724][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-04 16:28:31,725][__main__][INFO] - Instantiating loggers...
[2024-02-04 16:28:31,725][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-04 16:29:02,707][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-04 16:29:03,422][__main__][INFO] - Logging hyperparameters!
[2024-02-04 16:29:03,425][__main__][INFO] - Starting training!
[2024-02-04 16:29:09,820][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1016, in _run_stage
    self.fit_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 201, in run
    self.advance()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 354, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 133, in run
    self.advance(data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 218, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 185, in run
    self._optimizer_step(kwargs.get("batch_idx", 0), closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 260, in _optimizer_step
    call._call_lightning_module_hook(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 144, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\module.py", line 1256, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\optimizer.py", line 155, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 225, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\plugins\precision\precision_plugin.py", line 114, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\optim\optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\optim\ZW.py", line 64, in step
    p.data.addcdiv_(-step_size, corrected_exp_avg, denom)
TypeError: addcdiv_() takes 2 positional arguments but 3 were given
[2024-02-04 16:29:09,825][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-02-04_16-28-31
[2024-02-04 16:29:09,825][src.utils.utils][INFO] - Closing wandb!
[2024-02-04 16:29:59,994][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-04 16:30:00,016][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-04 16:30:00,162][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-04 16:30:00,168][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-04 16:30:00,188][__main__][INFO] - Instantiating callbacks...
[2024-02-04 16:30:00,190][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-04 16:30:00,196][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-04 16:30:00,198][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-04 16:30:00,199][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-04 16:30:00,200][__main__][INFO] - Instantiating loggers...
[2024-02-04 16:30:00,200][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-04 16:30:33,030][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-04 16:30:34,570][__main__][INFO] - Logging hyperparameters!
[2024-02-04 16:30:34,581][__main__][INFO] - Starting training!
[2024-02-04 16:30:47,496][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1016, in _run_stage
    self.fit_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 201, in run
    self.advance()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 354, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 133, in run
    self.advance(data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 218, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 185, in run
    self._optimizer_step(kwargs.get("batch_idx", 0), closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 260, in _optimizer_step
    call._call_lightning_module_hook(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 144, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\module.py", line 1256, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\optimizer.py", line 155, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 225, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\plugins\precision\precision_plugin.py", line 114, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\optim\optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\optim\ZW.py", line 64, in step
    p.data.addcdiv_(-step_size, corrected_exp_avg, denom)
TypeError: addcdiv_() takes 2 positional arguments but 3 were given
[2024-02-04 16:30:47,511][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-02-04_16-29-59
[2024-02-04 16:30:47,512][src.utils.utils][INFO] - Closing wandb!
[2024-02-04 16:33:23,854][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-04 16:33:23,861][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-04 16:33:23,955][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-04 16:33:23,962][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-04 16:33:23,982][__main__][INFO] - Instantiating callbacks...
[2024-02-04 16:33:23,984][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-04 16:33:23,988][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-04 16:33:23,989][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-04 16:33:23,990][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-04 16:33:23,991][__main__][INFO] - Instantiating loggers...
[2024-02-04 16:33:23,991][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-04 16:33:53,844][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-04 16:33:54,385][__main__][INFO] - Logging hyperparameters!
[2024-02-04 16:33:54,388][__main__][INFO] - Starting training!
[2024-02-04 16:34:55,902][__main__][INFO] - Starting testing!
[2024-02-04 20:18:49,173][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-04 20:18:49,182][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-04 20:18:49,269][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-04 20:18:49,277][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-04 20:18:49,363][__main__][INFO] - Instantiating callbacks...
[2024-02-04 20:18:49,363][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-04 20:18:49,366][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-04 20:18:49,371][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-04 20:18:49,371][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-04 20:18:49,371][__main__][INFO] - Instantiating loggers...
[2024-02-04 20:18:49,371][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-04 20:19:26,061][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-04 20:19:26,827][__main__][INFO] - Logging hyperparameters!
[2024-02-04 20:19:26,833][__main__][INFO] - Starting training!
[2024-02-04 20:20:35,549][__main__][INFO] - Starting testing!
[2024-02-04 20:22:11,396][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-04 20:22:11,406][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-04 20:22:11,516][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-04 20:22:11,522][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-04 20:22:11,550][__main__][INFO] - Instantiating callbacks...
[2024-02-04 20:22:11,550][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-04 20:22:11,559][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-04 20:22:11,561][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-04 20:22:11,562][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-04 20:22:11,562][__main__][INFO] - Instantiating loggers...
[2024-02-04 20:22:11,562][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-04 20:22:47,831][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-04 20:22:48,616][__main__][INFO] - Logging hyperparameters!
[2024-02-04 20:22:48,621][__main__][INFO] - Starting training!
[2024-02-04 20:23:53,326][__main__][INFO] - Starting testing!
[2024-02-04 20:24:59,378][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-04 20:24:59,391][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-04 20:24:59,499][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-04 20:24:59,508][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-04 20:24:59,544][__main__][INFO] - Instantiating callbacks...
[2024-02-04 20:24:59,544][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-04 20:24:59,550][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-04 20:24:59,553][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-04 20:24:59,554][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-04 20:24:59,556][__main__][INFO] - Instantiating loggers...
[2024-02-04 20:24:59,556][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-04 20:25:40,488][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-04 20:25:41,045][__main__][INFO] - Logging hyperparameters!
[2024-02-04 20:25:41,052][__main__][INFO] - Starting training!
[2024-02-04 20:26:44,948][__main__][INFO] - Starting testing!
[2024-02-04 20:27:23,595][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-04 20:27:23,609][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-04 20:27:23,708][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-04 20:27:23,716][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-04 20:27:23,749][__main__][INFO] - Instantiating callbacks...
[2024-02-04 20:27:23,749][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-04 20:27:23,755][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-04 20:27:23,758][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-04 20:27:23,758][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-04 20:27:23,758][__main__][INFO] - Instantiating loggers...
[2024-02-04 20:27:23,758][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-04 20:28:02,470][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-04 20:28:03,207][__main__][INFO] - Logging hyperparameters!
[2024-02-04 20:28:03,210][__main__][INFO] - Starting training!
[2024-02-04 20:29:12,485][__main__][INFO] - Starting testing!
[2024-02-04 20:29:53,915][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-04 20:29:53,927][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-04 20:29:54,009][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-04 20:29:54,017][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-04 20:29:54,046][__main__][INFO] - Instantiating callbacks...
[2024-02-04 20:29:54,048][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-04 20:29:54,054][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-04 20:29:54,056][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-04 20:29:54,056][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-04 20:29:54,056][__main__][INFO] - Instantiating loggers...
[2024-02-04 20:29:54,056][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-04 20:30:31,128][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-04 20:30:31,844][__main__][INFO] - Logging hyperparameters!
[2024-02-04 20:30:31,847][__main__][INFO] - Starting training!
[2024-02-04 20:31:32,485][__main__][INFO] - Starting testing!
[2024-02-04 20:32:37,804][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-04 20:32:37,812][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-04 20:32:37,886][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-04 20:32:37,902][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-04 20:32:37,928][__main__][INFO] - Instantiating callbacks...
[2024-02-04 20:32:37,934][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-04 20:32:37,936][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-04 20:32:37,940][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-04 20:32:37,940][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-04 20:32:37,940][__main__][INFO] - Instantiating loggers...
[2024-02-04 20:32:37,940][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-04 20:33:14,917][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-04 20:33:15,674][__main__][INFO] - Logging hyperparameters!
[2024-02-04 20:33:15,679][__main__][INFO] - Starting training!
[2024-02-04 20:34:13,693][__main__][INFO] - Starting testing!
[2024-02-04 20:34:51,258][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-04 20:34:51,273][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-04 20:34:51,364][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-04 20:34:51,369][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-04 20:34:51,389][__main__][INFO] - Instantiating callbacks...
[2024-02-04 20:34:51,389][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-04 20:34:51,399][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-04 20:34:51,401][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-04 20:34:51,403][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-04 20:34:51,405][__main__][INFO] - Instantiating loggers...
[2024-02-04 20:34:51,405][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-04 20:35:33,663][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-04 20:35:34,413][__main__][INFO] - Logging hyperparameters!
[2024-02-04 20:35:34,419][__main__][INFO] - Starting training!
[2024-02-04 20:35:46,068][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1016, in _run_stage
    self.fit_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 201, in run
    self.advance()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 354, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 133, in run
    self.advance(data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 218, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 185, in run
    self._optimizer_step(kwargs.get("batch_idx", 0), closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 260, in _optimizer_step
    call._call_lightning_module_hook(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 144, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\module.py", line 1256, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\optimizer.py", line 155, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 225, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\plugins\precision\precision_plugin.py", line 114, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\optim\optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\optim\ZW.py", line 53, in step
    self.update_learning_rate()
  File "D:\pycharmproject\template\srcC\optim\ZW.py", line 75, in update_learning_rate
    group['lr'] = self.lr
AttributeError: 'AWD' object has no attribute 'lr'
[2024-02-04 20:35:46,079][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-02-04_20-34-51
[2024-02-04 20:35:46,079][src.utils.utils][INFO] - Closing wandb!
[2024-02-04 20:36:57,174][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-04 20:36:57,182][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-04 20:36:57,264][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-04 20:36:57,273][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-04 20:36:57,300][__main__][INFO] - Instantiating callbacks...
[2024-02-04 20:36:57,300][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-04 20:36:57,304][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-04 20:36:57,304][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-04 20:36:57,309][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-04 20:36:57,309][__main__][INFO] - Instantiating loggers...
[2024-02-04 20:36:57,309][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-04 20:37:33,756][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-04 20:37:34,684][__main__][INFO] - Logging hyperparameters!
[2024-02-04 20:37:34,687][__main__][INFO] - Starting training!
[2024-02-04 20:40:03,570][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-04 20:40:03,584][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-04 20:40:03,727][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-04 20:40:03,739][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-04 20:40:03,802][__main__][INFO] - Instantiating callbacks...
[2024-02-04 20:40:03,802][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-04 20:40:03,811][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-04 20:40:03,813][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-04 20:40:03,814][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-04 20:40:03,818][__main__][INFO] - Instantiating loggers...
[2024-02-04 20:40:03,819][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-04 20:40:48,392][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-04 20:40:49,742][__main__][INFO] - Logging hyperparameters!
[2024-02-04 20:40:49,745][__main__][INFO] - Starting training!
[2024-02-04 20:41:38,375][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-04 20:41:38,384][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-04 20:41:38,539][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-04 20:41:38,555][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-04 20:41:38,632][__main__][INFO] - Instantiating callbacks...
[2024-02-04 20:41:38,632][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-04 20:41:38,642][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-04 20:41:38,647][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-04 20:41:38,647][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-04 20:41:38,652][__main__][INFO] - Instantiating loggers...
[2024-02-04 20:41:38,652][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-04 20:42:30,714][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-04 20:42:31,364][__main__][INFO] - Logging hyperparameters!
[2024-02-04 20:42:31,365][__main__][INFO] - Starting training!
[2024-02-04 20:43:36,505][__main__][INFO] - Starting testing!
[2024-02-04 20:44:24,441][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-04 20:44:24,441][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-04 20:44:24,563][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-04 20:44:24,567][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-04 20:44:24,591][__main__][INFO] - Instantiating callbacks...
[2024-02-04 20:44:24,596][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-04 20:44:24,601][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-04 20:44:24,601][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-04 20:44:24,601][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-04 20:44:24,605][__main__][INFO] - Instantiating loggers...
[2024-02-04 20:44:24,605][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-04 20:45:14,574][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-04 20:45:15,177][__main__][INFO] - Logging hyperparameters!
[2024-02-04 20:45:15,181][__main__][INFO] - Starting training!
[2024-02-04 20:46:10,959][__main__][INFO] - Starting testing!
[2024-02-04 20:47:01,211][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-04 20:47:01,219][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-04 20:47:01,334][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-04 20:47:01,343][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-04 20:47:01,384][__main__][INFO] - Instantiating callbacks...
[2024-02-04 20:47:01,384][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-04 20:47:01,392][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-04 20:47:01,392][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-04 20:47:01,392][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-04 20:47:01,392][__main__][INFO] - Instantiating loggers...
[2024-02-04 20:47:01,392][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-04 20:47:34,368][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-04 20:47:35,148][__main__][INFO] - Logging hyperparameters!
[2024-02-04 20:47:35,153][__main__][INFO] - Starting training!
[2024-02-04 20:48:31,829][__main__][INFO] - Starting testing!
[2024-02-04 20:49:12,951][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-04 20:49:12,964][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-04 20:49:13,069][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-04 20:49:13,079][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-04 20:49:13,111][__main__][INFO] - Instantiating callbacks...
[2024-02-04 20:49:13,113][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-04 20:49:13,115][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-04 20:49:13,120][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-04 20:49:13,120][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-04 20:49:13,123][__main__][INFO] - Instantiating loggers...
[2024-02-04 20:49:13,124][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-04 20:49:48,404][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-04 20:49:49,314][__main__][INFO] - Logging hyperparameters!
[2024-02-04 20:49:49,318][__main__][INFO] - Starting training!
[2024-02-04 20:50:44,040][__main__][INFO] - Starting testing!
[2024-02-04 20:52:19,388][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-04 20:52:19,396][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-04 20:52:19,510][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-04 20:52:19,522][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-04 20:52:19,561][__main__][INFO] - Instantiating callbacks...
[2024-02-04 20:52:19,561][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-04 20:52:19,570][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-04 20:52:19,573][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-04 20:52:19,574][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-04 20:52:19,574][__main__][INFO] - Instantiating loggers...
[2024-02-04 20:52:19,574][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-04 20:52:59,724][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-04 20:53:00,496][__main__][INFO] - Logging hyperparameters!
[2024-02-04 20:53:00,499][__main__][INFO] - Starting training!
[2024-02-04 20:53:08,893][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1016, in _run_stage
    self.fit_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 201, in run
    self.advance()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 354, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 133, in run
    self.advance(data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 218, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 185, in run
    self._optimizer_step(kwargs.get("batch_idx", 0), closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 260, in _optimizer_step
    call._call_lightning_module_hook(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 144, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\module.py", line 1256, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\optimizer.py", line 155, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 225, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\plugins\precision\precision_plugin.py", line 114, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\optim\optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\optim\ZW.py", line 76, in step
    self.update_learning_rate()
  File "D:\pycharmproject\template\srcC\optim\ZW.py", line 101, in update_learning_rate
    self.exp_avg_sq_biased.mul_(self.alpha).add_(1 - self.alpha, exp_avg_sq_hat)
NameError: name 'exp_avg_sq_hat' is not defined
[2024-02-04 20:53:08,906][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-02-04_20-52-19
[2024-02-04 20:53:08,906][src.utils.utils][INFO] - Closing wandb!
[2024-02-04 20:55:22,364][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-04 20:55:22,371][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-04 20:55:22,471][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-04 20:55:22,475][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-04 20:55:22,494][__main__][INFO] - Instantiating callbacks...
[2024-02-04 20:55:22,494][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-04 20:55:22,498][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-04 20:55:22,498][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-04 20:55:22,498][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-04 20:55:22,503][__main__][INFO] - Instantiating loggers...
[2024-02-04 20:55:22,503][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-04 20:55:54,393][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-04 20:55:54,936][__main__][INFO] - Logging hyperparameters!
[2024-02-04 20:55:54,939][__main__][INFO] - Starting training!
[2024-02-04 20:57:07,559][__main__][INFO] - Starting testing!
[2024-02-04 20:58:01,028][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-04 20:58:01,037][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-04 20:58:01,145][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-04 20:58:01,147][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-04 20:58:01,170][__main__][INFO] - Instantiating callbacks...
[2024-02-04 20:58:01,170][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-04 20:58:01,176][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-04 20:58:01,178][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-04 20:58:01,179][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-04 20:58:01,180][__main__][INFO] - Instantiating loggers...
[2024-02-04 20:58:01,180][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-04 20:58:28,273][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-04 20:58:28,902][__main__][INFO] - Logging hyperparameters!
[2024-02-04 20:58:28,906][__main__][INFO] - Starting training!
[2024-02-04 21:00:10,460][__main__][INFO] - Starting testing!
[2024-02-04 21:02:29,876][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-04 21:02:29,886][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-04 21:02:29,997][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-04 21:02:30,002][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-04 21:02:30,029][__main__][INFO] - Instantiating callbacks...
[2024-02-04 21:02:30,029][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-04 21:02:30,037][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-04 21:02:30,039][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-04 21:02:30,041][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-04 21:02:30,041][__main__][INFO] - Instantiating loggers...
[2024-02-04 21:02:30,041][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-04 21:03:06,565][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-04 21:03:07,373][__main__][INFO] - Logging hyperparameters!
[2024-02-04 21:03:07,376][__main__][INFO] - Starting training!
[2024-02-04 21:04:52,562][__main__][INFO] - Starting testing!
[2024-02-04 21:05:32,858][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-04 21:05:32,873][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-04 21:05:33,046][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-04 21:05:33,051][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-04 21:05:33,067][__main__][INFO] - Instantiating callbacks...
[2024-02-04 21:05:33,067][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-04 21:05:33,076][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-04 21:05:33,076][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-04 21:05:33,076][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-04 21:05:33,081][__main__][INFO] - Instantiating loggers...
[2024-02-04 21:05:33,081][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-04 21:06:04,958][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-04 21:06:05,559][__main__][INFO] - Logging hyperparameters!
[2024-02-04 21:06:05,565][__main__][INFO] - Starting training!
[2024-02-04 21:07:36,660][__main__][INFO] - Starting testing!
[2024-02-04 21:10:07,231][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-04 21:10:07,237][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-04 21:10:07,329][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-04 21:10:07,334][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-04 21:10:07,358][__main__][INFO] - Instantiating callbacks...
[2024-02-04 21:10:07,359][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-04 21:10:07,364][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-04 21:10:07,365][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-04 21:10:07,366][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-04 21:10:07,368][__main__][INFO] - Instantiating loggers...
[2024-02-04 21:10:07,368][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-04 21:10:41,232][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-04 21:10:41,908][__main__][INFO] - Logging hyperparameters!
[2024-02-04 21:10:41,913][__main__][INFO] - Starting training!
[2024-02-04 21:11:37,126][__main__][INFO] - Starting testing!
[2024-02-04 21:15:19,218][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-04 21:15:19,227][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-04 21:15:19,303][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-04 21:15:19,308][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-04 21:15:19,326][__main__][INFO] - Instantiating callbacks...
[2024-02-04 21:15:19,327][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-04 21:15:19,332][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-04 21:15:19,333][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-04 21:15:19,333][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-04 21:15:19,334][__main__][INFO] - Instantiating loggers...
[2024-02-04 21:15:19,335][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-04 21:15:49,280][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-04 21:15:49,988][__main__][INFO] - Logging hyperparameters!
[2024-02-04 21:15:49,992][__main__][INFO] - Starting training!
[2024-02-04 21:16:51,670][__main__][INFO] - Starting testing!
[2024-02-04 21:19:01,939][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-04 21:19:01,946][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-04 21:19:02,024][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-04 21:19:02,028][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-04 21:19:02,047][__main__][INFO] - Instantiating callbacks...
[2024-02-04 21:19:02,048][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-04 21:19:02,051][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-04 21:19:02,052][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-04 21:19:02,052][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-04 21:19:02,053][__main__][INFO] - Instantiating loggers...
[2024-02-04 21:19:02,054][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-04 21:19:33,374][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-04 21:19:33,952][__main__][INFO] - Logging hyperparameters!
[2024-02-04 21:19:33,964][__main__][INFO] - Starting training!
[2024-02-04 21:20:40,760][__main__][INFO] - Starting testing!
[2024-02-04 21:26:48,160][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-04 21:26:48,167][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-04 21:26:48,247][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-04 21:26:48,252][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-04 21:26:48,269][__main__][INFO] - Instantiating callbacks...
[2024-02-04 21:26:48,270][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-04 21:26:48,273][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-04 21:26:48,274][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-04 21:26:48,275][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-04 21:26:48,275][__main__][INFO] - Instantiating loggers...
[2024-02-04 21:26:48,277][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-04 21:27:18,711][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-04 21:27:19,299][__main__][INFO] - Logging hyperparameters!
[2024-02-04 21:27:19,307][__main__][INFO] - Starting training!
[2024-02-04 21:28:25,877][__main__][INFO] - Starting testing!
[2024-02-04 21:47:51,509][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-04 21:47:51,518][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-04 21:47:51,621][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-04 21:47:51,629][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-04 21:47:51,653][__main__][INFO] - Instantiating callbacks...
[2024-02-04 21:47:51,654][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-04 21:47:51,660][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-04 21:47:51,662][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-04 21:47:51,663][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-04 21:47:51,665][__main__][INFO] - Instantiating loggers...
[2024-02-04 21:47:51,666][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-04 21:48:32,511][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-04 21:48:33,144][__main__][INFO] - Logging hyperparameters!
[2024-02-04 21:48:33,147][__main__][INFO] - Starting training!
[2024-02-04 21:49:25,113][__main__][INFO] - Starting testing!
[2024-02-04 21:50:05,312][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-04 21:50:05,316][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-04 21:50:05,393][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-04 21:50:05,398][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-04 21:50:05,417][__main__][INFO] - Instantiating callbacks...
[2024-02-04 21:50:05,417][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-04 21:50:05,424][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-04 21:50:05,428][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-04 21:50:05,428][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-04 21:50:05,428][__main__][INFO] - Instantiating loggers...
[2024-02-04 21:50:05,428][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-04 21:50:32,719][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-04 21:50:33,389][__main__][INFO] - Logging hyperparameters!
[2024-02-04 21:50:33,395][__main__][INFO] - Starting training!
[2024-02-04 21:51:25,329][__main__][INFO] - Starting testing!
[2024-02-04 21:52:21,449][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-04 21:52:21,457][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-04 21:52:21,543][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-04 21:52:21,545][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-04 21:52:21,565][__main__][INFO] - Instantiating callbacks...
[2024-02-04 21:52:21,569][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-04 21:52:21,573][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-04 21:52:21,573][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-04 21:52:21,573][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-04 21:52:21,573][__main__][INFO] - Instantiating loggers...
[2024-02-04 21:52:21,573][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-04 21:52:52,221][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-04 21:52:52,821][__main__][INFO] - Logging hyperparameters!
[2024-02-04 21:52:52,823][__main__][INFO] - Starting training!
[2024-02-04 21:53:33,809][__main__][INFO] - Starting testing!
[2024-02-04 22:02:11,979][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-04 22:02:11,983][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-04 22:02:12,055][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-04 22:02:12,061][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-04 22:02:12,081][__main__][INFO] - Instantiating callbacks...
[2024-02-04 22:02:12,081][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-04 22:02:12,083][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-04 22:02:12,083][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-04 22:02:12,087][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-04 22:02:12,087][__main__][INFO] - Instantiating loggers...
[2024-02-04 22:02:12,087][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-04 22:02:40,060][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-04 22:02:40,674][__main__][INFO] - Logging hyperparameters!
[2024-02-04 22:02:40,678][__main__][INFO] - Starting training!
[2024-02-04 22:03:45,877][__main__][INFO] - Starting testing!
[2024-02-05 19:33:44,231][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-05 19:33:44,242][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-05 19:33:44,366][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-05 19:33:44,373][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-05 19:33:44,427][__main__][INFO] - Instantiating callbacks...
[2024-02-05 19:33:44,427][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-05 19:33:44,432][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-05 19:33:44,434][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-05 19:33:44,435][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-05 19:33:44,437][__main__][INFO] - Instantiating loggers...
[2024-02-05 19:33:44,437][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-05 19:34:14,751][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-05 19:34:15,391][__main__][INFO] - Logging hyperparameters!
[2024-02-05 19:34:15,394][__main__][INFO] - Starting training!
[2024-02-05 19:36:39,080][__main__][INFO] - Starting testing!
[2024-02-05 19:36:45,574][__main__][INFO] - Best ckpt path: D:\data\CIFAR10_logs\logs\train\runs\2024-02-05_19-33-44\checkpoints\epoch_004.ckpt
[2024-02-05 19:36:45,576][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-02-05_19-33-44
[2024-02-05 19:36:45,576][src.utils.utils][INFO] - Closing wandb!
[2024-02-05 19:37:00,199][src.utils.utils][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-02-05 19:42:14,139][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-05 19:42:14,146][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-05 19:42:14,229][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-05 19:42:14,234][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-05 19:42:14,252][__main__][INFO] - Instantiating callbacks...
[2024-02-05 19:42:14,253][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-05 19:42:14,259][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-05 19:42:14,262][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-05 19:42:14,263][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-05 19:42:14,264][__main__][INFO] - Instantiating loggers...
[2024-02-05 19:42:14,265][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-05 19:42:46,443][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-05 19:42:47,113][__main__][INFO] - Logging hyperparameters!
[2024-02-05 19:42:47,117][__main__][INFO] - Starting training!
[2024-02-05 19:51:48,261][__main__][INFO] - Starting testing!
[2024-02-05 19:51:54,231][__main__][INFO] - Best ckpt path: D:\data\CIFAR10_logs\logs\train\runs\2024-02-05_19-42-14\checkpoints\epoch_008.ckpt
[2024-02-05 19:51:54,233][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-02-05_19-42-14
[2024-02-05 19:51:54,234][src.utils.utils][INFO] - Closing wandb!
[2024-02-05 19:52:17,631][src.utils.utils][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-02-05 19:52:52,800][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-05 19:52:52,807][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-05 19:52:52,888][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-05 19:52:52,893][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-05 19:52:52,912][__main__][INFO] - Instantiating callbacks...
[2024-02-05 19:52:52,912][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-05 19:52:52,915][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-05 19:52:52,917][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-05 19:52:52,917][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-05 19:52:52,919][__main__][INFO] - Instantiating loggers...
[2024-02-05 19:52:52,919][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-05 19:53:19,739][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-05 19:53:20,335][__main__][INFO] - Logging hyperparameters!
[2024-02-05 19:53:20,338][__main__][INFO] - Starting training!
[2024-02-05 19:55:24,161][__main__][INFO] - Starting testing!
[2024-02-05 19:55:41,428][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-05 19:55:41,436][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-05 19:55:41,529][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-05 19:55:41,534][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-05 19:55:41,554][__main__][INFO] - Instantiating callbacks...
[2024-02-05 19:55:41,556][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-05 19:55:41,559][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-05 19:55:41,561][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-05 19:55:41,562][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-05 19:55:41,563][__main__][INFO] - Instantiating loggers...
[2024-02-05 19:55:41,563][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-05 19:56:13,399][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-05 19:56:14,142][__main__][INFO] - Logging hyperparameters!
[2024-02-05 19:56:14,144][__main__][INFO] - Starting training!
[2024-02-05 19:57:49,259][__main__][INFO] - Starting testing!
[2024-02-05 19:58:15,726][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-05 19:58:15,734][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-05 19:58:15,818][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-05 19:58:15,823][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-05 19:58:15,843][__main__][INFO] - Instantiating callbacks...
[2024-02-05 19:58:15,844][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-05 19:58:15,847][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-05 19:58:15,849][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-05 19:58:15,850][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-05 19:58:15,851][__main__][INFO] - Instantiating loggers...
[2024-02-05 19:58:15,852][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-05 19:58:48,324][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-05 19:58:49,007][__main__][INFO] - Logging hyperparameters!
[2024-02-05 19:58:49,010][__main__][INFO] - Starting training!
[2024-02-05 20:09:35,302][__main__][INFO] - Starting testing!
[2024-02-05 20:09:40,826][__main__][INFO] - Best ckpt path: D:\data\CIFAR10_logs\logs\train\runs\2024-02-05_19-58-15\checkpoints\epoch_014.ckpt
[2024-02-05 20:09:40,828][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-02-05_19-58-15
[2024-02-05 20:09:40,828][src.utils.utils][INFO] - Closing wandb!
[2024-02-05 20:10:03,893][src.utils.utils][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-02-07 13:39:37,163][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-07 13:39:37,185][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-07 13:39:37,413][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-07 13:39:37,433][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-07 13:39:37,525][__main__][INFO] - Instantiating callbacks...
[2024-02-07 13:39:37,526][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-07 13:39:37,537][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-07 13:39:37,541][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-07 13:39:37,543][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-07 13:39:37,549][__main__][INFO] - Instantiating loggers...
[2024-02-07 13:39:37,550][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-07 13:40:28,342][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-07 13:40:31,781][__main__][INFO] - Logging hyperparameters!
[2024-02-07 13:40:31,791][__main__][INFO] - Starting training!
[2024-02-07 13:52:08,913][__main__][INFO] - Starting testing!
[2024-02-07 13:52:18,226][__main__][INFO] - Best ckpt path: D:\data\CIFAR10_logs\logs\train\runs\2024-02-07_13-39-36\checkpoints\epoch_009.ckpt
[2024-02-07 13:52:18,229][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-02-07_13-39-36
[2024-02-07 13:52:18,229][src.utils.utils][INFO] - Closing wandb!
[2024-02-07 13:53:14,395][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-07 13:53:14,405][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-07 13:53:14,524][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-07 13:53:14,533][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-07 13:53:14,561][__main__][INFO] - Instantiating callbacks...
[2024-02-07 13:53:14,562][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-07 13:53:14,568][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-07 13:53:14,571][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-07 13:53:14,573][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-07 13:53:14,576][__main__][INFO] - Instantiating loggers...
[2024-02-07 13:53:14,577][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-07 13:53:57,546][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-07 13:54:00,161][__main__][INFO] - Logging hyperparameters!
[2024-02-07 13:54:00,167][__main__][INFO] - Starting training!
[2024-02-07 14:02:33,224][__main__][INFO] - Starting testing!
[2024-02-07 14:02:39,538][__main__][INFO] - Best ckpt path: D:\data\CIFAR10_logs\logs\train\runs\2024-02-07_13-53-14\checkpoints\epoch_018.ckpt
[2024-02-07 14:02:39,541][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-02-07_13-53-14
[2024-02-07 14:02:39,541][src.utils.utils][INFO] - Closing wandb!
[2024-02-07 14:04:44,828][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-07 14:04:44,840][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-07 14:04:44,928][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-07 14:04:44,939][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-07 14:04:44,960][__main__][INFO] - Instantiating callbacks...
[2024-02-07 14:04:44,962][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-07 14:04:44,969][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-07 14:04:44,973][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-07 14:04:44,974][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-07 14:04:44,977][__main__][INFO] - Instantiating loggers...
[2024-02-07 14:04:44,978][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-07 14:05:27,931][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-07 14:05:31,654][__main__][INFO] - Logging hyperparameters!
[2024-02-07 14:05:31,664][__main__][INFO] - Starting training!
[2024-02-07 14:13:26,299][__main__][INFO] - Starting testing!
[2024-02-07 14:13:32,656][__main__][INFO] - Best ckpt path: D:\data\CIFAR10_logs\logs\train\runs\2024-02-07_14-04-44\checkpoints\epoch_019.ckpt
[2024-02-07 14:13:32,659][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-02-07_14-04-44
[2024-02-07 14:13:32,661][src.utils.utils][INFO] - Closing wandb!
[2024-02-07 14:16:10,098][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-07 14:16:10,115][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-07 14:16:10,296][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-07 14:16:10,310][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-07 14:16:10,360][__main__][INFO] - Instantiating callbacks...
[2024-02-07 14:16:10,361][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-07 14:16:10,372][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-07 14:16:10,375][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-07 14:16:10,377][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-07 14:16:10,380][__main__][INFO] - Instantiating loggers...
[2024-02-07 14:16:10,381][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-07 14:16:49,721][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-07 14:16:51,793][__main__][INFO] - Logging hyperparameters!
[2024-02-07 14:16:51,798][__main__][INFO] - Starting training!
[2024-02-07 14:26:56,651][__main__][INFO] - Starting testing!
[2024-02-07 14:27:02,150][__main__][INFO] - Best ckpt path: D:\data\CIFAR10_logs\logs\train\runs\2024-02-07_14-16-09\checkpoints\epoch_010.ckpt
[2024-02-07 14:27:02,152][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-02-07_14-16-09
[2024-02-07 14:27:02,153][src.utils.utils][INFO] - Closing wandb!
[2024-02-07 14:44:26,782][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-07 14:44:26,789][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-07 14:44:26,869][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-07 14:44:26,875][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-07 14:44:26,892][__main__][INFO] - Instantiating callbacks...
[2024-02-07 14:44:26,892][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-07 14:44:26,896][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-07 14:44:26,898][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-07 14:44:26,899][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-07 14:44:26,902][__main__][INFO] - Instantiating loggers...
[2024-02-07 14:44:26,902][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-07 14:44:57,104][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-07 14:44:58,172][__main__][INFO] - Logging hyperparameters!
[2024-02-07 14:44:58,175][__main__][INFO] - Starting training!
[2024-02-07 14:51:01,334][__main__][INFO] - Starting testing!
[2024-02-07 14:51:29,965][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-07 14:51:29,977][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-07 14:51:30,088][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-07 14:51:30,097][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-07 14:51:30,126][__main__][INFO] - Instantiating callbacks...
[2024-02-07 14:51:30,127][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-07 14:51:30,133][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-07 14:51:30,135][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-07 14:51:30,137][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-07 14:51:30,141][__main__][INFO] - Instantiating loggers...
[2024-02-07 14:51:30,141][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-07 14:52:14,463][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-07 14:52:17,113][__main__][INFO] - Logging hyperparameters!
[2024-02-07 14:52:17,123][__main__][INFO] - Starting training!
[2024-02-07 14:55:41,081][__main__][INFO] - Starting testing!
[2024-02-07 14:57:34,826][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-07 14:57:34,835][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-07 14:57:34,933][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-07 14:57:34,941][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-07 14:57:34,969][__main__][INFO] - Instantiating callbacks...
[2024-02-07 14:57:34,969][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-07 14:57:34,974][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-07 14:57:34,976][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-07 14:57:34,977][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-07 14:57:34,979][__main__][INFO] - Instantiating loggers...
[2024-02-07 14:57:34,980][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-07 14:58:06,103][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-07 14:58:07,279][__main__][INFO] - Logging hyperparameters!
[2024-02-07 14:58:07,282][__main__][INFO] - Starting training!
[2024-02-07 14:59:03,302][__main__][INFO] - Starting testing!
[2024-02-07 14:59:09,022][__main__][INFO] - Best ckpt path: D:\data\CIFAR10_logs\logs\train\runs\2024-02-07_14-57-34\checkpoints\epoch_001.ckpt
[2024-02-07 14:59:09,024][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-02-07_14-57-34
[2024-02-07 14:59:09,025][src.utils.utils][INFO] - Closing wandb!
[2024-02-18 13:44:02,961][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-18 13:44:02,976][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-18 13:44:03,142][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-18 13:44:03,157][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-18 13:44:03,272][__main__][INFO] - Instantiating callbacks...
[2024-02-18 13:44:03,273][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-18 13:44:03,281][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-18 13:44:03,285][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-18 13:44:03,286][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-18 13:44:03,291][__main__][INFO] - Instantiating loggers...
[2024-02-18 13:44:03,291][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-18 13:44:47,454][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-18 13:44:49,207][__main__][INFO] - Logging hyperparameters!
[2024-02-18 13:44:49,213][__main__][INFO] - Starting training!
[2024-02-18 13:47:10,157][__main__][INFO] - Starting testing!
[2024-02-18 13:51:59,681][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-18 13:51:59,689][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-18 13:51:59,770][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-18 13:51:59,776][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-18 13:51:59,911][__main__][INFO] - Instantiating callbacks...
[2024-02-18 13:51:59,912][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-18 13:51:59,918][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-18 13:51:59,920][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-18 13:51:59,921][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-18 13:51:59,924][__main__][INFO] - Instantiating loggers...
[2024-02-18 13:51:59,924][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-18 13:52:33,074][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-18 13:52:34,043][__main__][INFO] - Logging hyperparameters!
[2024-02-18 13:52:34,048][__main__][INFO] - Starting training!
[2024-02-18 13:52:41,519][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1014, in _run_stage
    self._run_sanity_check()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1043, in _run_sanity_check
    val_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\utilities.py", line 177, in _decorator
    return loop_run(self, *args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\evaluation_loop.py", line 115, in run
    self._evaluation_step(batch, batch_idx, dataloader_idx)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\evaluation_loop.py", line 375, in _evaluation_step
    output = call._call_strategy_hook(trainer, hook_name, *step_kwargs.values())
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 291, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 379, in validation_step
    return self.model.validation_step(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 147, in validation_step
    self.val_acc(preds, targets)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\torchmetrics\metric.py", line 290, in forward
    self._forward_cache = self._forward_reduce_state_update(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\torchmetrics\metric.py", line 357, in _forward_reduce_state_update
    self.update(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\torchmetrics\metric.py", line 456, in wrapped_func
    raise err
  File "D:\Anaconda3\lib\site-packages\torchmetrics\metric.py", line 446, in wrapped_func
    update(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\torchmetrics\classification\stat_scores.py", line 314, in update
    _multiclass_stat_scores_tensor_validation(
  File "D:\Anaconda3\lib\site-packages\torchmetrics\functional\classification\stat_scores.py", line 313, in _multiclass_stat_scores_tensor_validation
    raise RuntimeError(
RuntimeError: Detected more unique values in `preds` than `num_classes`. Expected only 10 but found 11 in `preds`.
[2024-02-18 13:52:41,529][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-02-18_13-51-59
[2024-02-18 13:52:41,530][src.utils.utils][INFO] - Closing wandb!
[2024-02-18 13:53:09,588][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-18 13:53:09,597][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-18 13:53:09,699][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-18 13:53:09,708][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-18 13:53:09,849][__main__][INFO] - Instantiating callbacks...
[2024-02-18 13:53:09,849][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-18 13:53:09,858][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-18 13:53:09,861][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-18 13:53:09,863][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-18 13:53:09,866][__main__][INFO] - Instantiating loggers...
[2024-02-18 13:53:09,866][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-18 13:53:51,017][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-18 13:53:53,260][__main__][INFO] - Logging hyperparameters!
[2024-02-18 13:53:53,267][__main__][INFO] - Starting training!
[2024-02-18 14:00:37,185][__main__][INFO] - Starting testing!
[2024-02-18 14:01:06,894][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-18 14:01:06,903][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-18 14:01:07,014][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-18 14:01:07,020][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-18 14:01:07,169][__main__][INFO] - Instantiating callbacks...
[2024-02-18 14:01:07,169][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-18 14:01:07,175][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-18 14:01:07,178][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-18 14:01:07,180][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-18 14:01:07,183][__main__][INFO] - Instantiating loggers...
[2024-02-18 14:01:07,183][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-18 14:01:38,646][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-18 14:01:39,787][__main__][INFO] - Logging hyperparameters!
[2024-02-18 14:01:39,792][__main__][INFO] - Starting training!
[2024-02-18 14:04:28,506][__main__][INFO] - Starting testing!
[2024-02-18 14:04:54,892][__main__][INFO] - Best ckpt path: D:\data\CIFAR10_logs\logs\train\runs\2024-02-18_14-01-06\checkpoints\epoch_002.ckpt
[2024-02-18 14:04:54,897][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-02-18_14-01-06
[2024-02-18 14:04:54,899][src.utils.utils][INFO] - Closing wandb!
[2024-02-18 14:07:25,660][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-18 14:07:25,668][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-18 14:07:25,755][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-18 14:07:25,761][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-18 14:07:25,872][__main__][INFO] - Instantiating callbacks...
[2024-02-18 14:07:25,873][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-18 14:07:25,880][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-18 14:07:25,883][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-18 14:07:25,887][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-18 14:07:25,892][__main__][INFO] - Instantiating loggers...
[2024-02-18 14:07:25,892][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-18 14:08:08,218][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-18 14:08:10,607][__main__][INFO] - Logging hyperparameters!
[2024-02-18 14:08:10,613][__main__][INFO] - Starting training!
[2024-02-18 14:13:40,347][__main__][INFO] - Starting testing!
[2024-02-18 14:13:52,000][__main__][INFO] - Best ckpt path: D:\data\CIFAR10_logs\logs\train\runs\2024-02-18_14-07-25\checkpoints\epoch_002.ckpt
[2024-02-18 14:13:52,003][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-02-18_14-07-25
[2024-02-18 14:13:52,004][src.utils.utils][INFO] - Closing wandb!
[2024-02-18 14:15:46,588][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-18 14:15:46,596][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-18 14:15:46,694][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-18 14:15:46,701][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-18 14:15:46,816][__main__][INFO] - Instantiating callbacks...
[2024-02-18 14:15:46,817][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-18 14:15:46,821][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-18 14:15:46,824][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-18 14:15:46,825][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-18 14:15:46,829][__main__][INFO] - Instantiating loggers...
[2024-02-18 14:15:46,829][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-18 14:16:42,336][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-18 14:16:45,844][__main__][INFO] - Logging hyperparameters!
[2024-02-18 14:16:45,858][__main__][INFO] - Starting training!
[2024-02-18 14:20:14,497][__main__][INFO] - Starting testing!
[2024-02-21 18:00:09,718][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-21 18:00:09,727][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-21 18:00:09,828][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-21 18:00:09,837][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-21 18:00:10,025][__main__][INFO] - Instantiating callbacks...
[2024-02-21 18:00:10,026][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-21 18:00:10,031][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-21 18:00:10,032][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-21 18:00:10,034][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-21 18:00:10,037][__main__][INFO] - Instantiating loggers...
[2024-02-21 18:00:10,037][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-21 18:00:48,364][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-21 18:00:49,607][__main__][INFO] - Logging hyperparameters!
[2024-02-21 18:00:49,612][__main__][INFO] - Starting training!
[2024-02-21 18:01:00,360][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1016, in _run_stage
    self.fit_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 201, in run
    self.advance()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 354, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 133, in run
    self.advance(data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 218, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 185, in run
    self._optimizer_step(kwargs.get("batch_idx", 0), closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 260, in _optimizer_step
    call._call_lightning_module_hook(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 144, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\module.py", line 1256, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\optimizer.py", line 155, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 225, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\plugins\precision\precision_plugin.py", line 114, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\optim\optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\optim\Optimizer.py", line 138, in step
    lr = group['lr_base'] + (group['lr_max'] - group['lr_min']) * max(0, 1 - x) * group['gamma'] ** state['step']
KeyError: 'lr_base'
[2024-02-21 18:01:00,369][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-02-21_18-00-09
[2024-02-21 18:01:00,371][src.utils.utils][INFO] - Closing wandb!
[2024-02-21 18:02:05,725][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-21 18:02:05,738][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-21 18:02:05,863][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-21 18:02:05,870][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-21 18:02:06,012][__main__][INFO] - Instantiating callbacks...
[2024-02-21 18:02:06,012][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-21 18:02:06,018][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-21 18:02:06,020][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-21 18:02:06,022][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-21 18:02:06,028][__main__][INFO] - Instantiating loggers...
[2024-02-21 18:02:06,028][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-21 18:02:30,335][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-02-21_18-02-05
[2024-02-21 18:02:51,053][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-21 18:02:51,061][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-21 18:02:51,148][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-21 18:02:51,154][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-21 18:02:51,277][__main__][INFO] - Instantiating callbacks...
[2024-02-21 18:02:51,277][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-21 18:02:51,283][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-21 18:02:51,285][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-21 18:02:51,286][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-21 18:02:51,289][__main__][INFO] - Instantiating loggers...
[2024-02-21 18:02:51,289][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-21 18:03:37,718][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-21 18:03:38,801][__main__][INFO] - Logging hyperparameters!
[2024-02-21 18:03:38,810][__main__][INFO] - Starting training!
[2024-02-21 18:05:41,282][__main__][INFO] - Starting testing!
[2024-02-27 16:35:16,305][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-27 16:35:16,314][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-27 16:35:16,396][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-27 16:35:16,404][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-27 16:35:16,559][__main__][INFO] - Instantiating callbacks...
[2024-02-27 16:35:16,559][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-27 16:35:16,566][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-27 16:35:16,567][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-27 16:35:16,568][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-27 16:35:16,570][__main__][INFO] - Instantiating loggers...
[2024-02-27 16:35:16,570][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-27 16:35:46,959][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-27 16:35:48,209][__main__][INFO] - Logging hyperparameters!
[2024-02-27 16:35:48,214][__main__][INFO] - Starting training!
[2024-02-27 16:35:58,536][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1016, in _run_stage
    self.fit_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 201, in run
    self.advance()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 354, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 133, in run
    self.advance(data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 218, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 185, in run
    self._optimizer_step(kwargs.get("batch_idx", 0), closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 260, in _optimizer_step
    call._call_lightning_module_hook(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 144, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\module.py", line 1256, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\optimizer.py", line 155, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 225, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\plugins\precision\precision_plugin.py", line 114, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\optim\optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\optim\ZW.py", line 14, in step
    loss = closure()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\plugins\precision\precision_plugin.py", line 101, in _wrap_closure
    closure_result = closure()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 140, in __call__
    self._result = self.closure(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 126, in closure
    step_output = self._step_fn()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 307, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 291, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 367, in training_step
    return self.model.training_step(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 130, in training_step
    optimizer = AWD(self.model.parameters())
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1614, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'CIFAR10LitModule' object has no attribute 'model'
[2024-02-27 16:35:58,551][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-02-27_16-35-16
[2024-02-27 16:35:58,552][src.utils.utils][INFO] - Closing wandb!
[2024-02-27 16:43:47,861][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-27 16:43:47,867][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-27 16:43:47,940][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-27 16:43:47,946][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-27 16:43:48,043][__main__][INFO] - Instantiating callbacks...
[2024-02-27 16:43:48,043][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-27 16:43:48,047][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-27 16:43:48,049][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-27 16:43:48,049][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-27 16:43:48,051][__main__][INFO] - Instantiating loggers...
[2024-02-27 16:43:48,051][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-27 16:44:19,457][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-27 16:44:20,773][__main__][INFO] - Logging hyperparameters!
[2024-02-27 16:44:20,779][__main__][INFO] - Starting training!
[2024-02-27 16:44:29,943][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1016, in _run_stage
    self.fit_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 201, in run
    self.advance()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 354, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 133, in run
    self.advance(data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 218, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 185, in run
    self._optimizer_step(kwargs.get("batch_idx", 0), closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 260, in _optimizer_step
    call._call_lightning_module_hook(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 144, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\module.py", line 1256, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\optimizer.py", line 155, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 225, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\plugins\precision\precision_plugin.py", line 114, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\optim\optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\optim\ZW.py", line 14, in step
    loss = closure()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\plugins\precision\precision_plugin.py", line 101, in _wrap_closure
    closure_result = closure()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 140, in __call__
    self._result = self.closure(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 126, in closure
    step_output = self._step_fn()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 307, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 291, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 367, in training_step
    return self.model.training_step(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 132, in training_step
    current_lr = self.current_lr
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1614, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'CIFAR10LitModule' object has no attribute 'current_lr'
[2024-02-27 16:44:29,970][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-02-27_16-43-47
[2024-02-27 16:44:29,970][src.utils.utils][INFO] - Closing wandb!
[2024-02-27 17:11:33,601][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-27 17:11:33,608][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-27 17:11:33,693][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-27 17:11:33,701][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-27 17:11:33,823][__main__][INFO] - Instantiating callbacks...
[2024-02-27 17:11:33,823][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-27 17:11:33,828][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-27 17:11:33,829][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-27 17:11:33,830][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-27 17:11:33,833][__main__][INFO] - Instantiating loggers...
[2024-02-27 17:11:33,833][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-27 17:12:06,342][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-27 17:12:07,513][__main__][INFO] - Logging hyperparameters!
[2024-02-27 17:12:07,518][__main__][INFO] - Starting training!
[2024-02-27 17:12:14,085][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1016, in _run_stage
    self.fit_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 201, in run
    self.advance()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 354, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 133, in run
    self.advance(data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 218, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 185, in run
    self._optimizer_step(kwargs.get("batch_idx", 0), closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 260, in _optimizer_step
    call._call_lightning_module_hook(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 144, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\module.py", line 1256, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\optimizer.py", line 155, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 225, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\plugins\precision\precision_plugin.py", line 114, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\optim\optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\optim\Optimizer.py", line 92, in step
    loss = closure()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\plugins\precision\precision_plugin.py", line 101, in _wrap_closure
    closure_result = closure()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 140, in __call__
    self._result = self.closure(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 126, in closure
    step_output = self._step_fn()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 307, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 291, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 367, in training_step
    return self.model.training_step(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 131, in training_step
    current_lr = self.optimizer.get_current_lr()  # AWD
AttributeError: 'functools.partial' object has no attribute 'get_current_lr'
[2024-02-27 17:12:14,099][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-02-27_17-11-33
[2024-02-27 17:12:14,099][src.utils.utils][INFO] - Closing wandb!
[2024-02-28 19:26:05,510][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-28 19:26:05,518][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-28 19:26:05,597][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-28 19:26:05,607][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-28 19:26:05,690][__main__][INFO] - Instantiating callbacks...
[2024-02-28 19:26:05,691][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-28 19:26:05,695][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-28 19:26:05,696][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-28 19:26:05,697][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-28 19:26:05,700][__main__][INFO] - Instantiating loggers...
[2024-02-28 19:26:05,700][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-28 19:26:38,069][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-28 19:26:39,219][__main__][INFO] - Logging hyperparameters!
[2024-02-28 19:26:39,223][__main__][INFO] - Starting training!
[2024-02-28 19:27:57,771][__main__][INFO] - Starting testing!
[2024-02-28 19:31:19,566][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-28 19:31:19,573][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-28 19:31:19,661][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-28 19:31:19,669][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-28 19:31:19,694][__main__][INFO] - Instantiating callbacks...
[2024-02-28 19:31:19,694][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-28 19:31:19,700][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-28 19:31:19,702][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-28 19:31:19,703][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-28 19:31:19,706][__main__][INFO] - Instantiating loggers...
[2024-02-28 19:31:19,706][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-28 19:31:54,169][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-28 19:31:55,296][__main__][INFO] - Logging hyperparameters!
[2024-02-28 19:31:55,299][__main__][INFO] - Starting training!
[2024-02-28 19:32:02,325][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1016, in _run_stage
    self.fit_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 201, in run
    self.advance()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 354, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 133, in run
    self.advance(data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 218, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 185, in run
    self._optimizer_step(kwargs.get("batch_idx", 0), closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 260, in _optimizer_step
    call._call_lightning_module_hook(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 144, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\module.py", line 1256, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\optimizer.py", line 155, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 225, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\plugins\precision\precision_plugin.py", line 114, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\optim\optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
TypeError: step() got an unexpected keyword argument 'closure'
[2024-02-28 19:32:02,336][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-02-28_19-31-19
[2024-02-28 19:32:02,337][src.utils.utils][INFO] - Closing wandb!
[2024-02-28 20:25:09,375][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-28 20:25:09,384][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-28 20:25:09,482][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-28 20:25:09,493][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-28 20:25:09,521][__main__][INFO] - Instantiating callbacks...
[2024-02-28 20:25:09,522][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-28 20:25:09,526][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-28 20:25:09,528][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-28 20:25:09,528][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-28 20:25:09,532][__main__][INFO] - Instantiating loggers...
[2024-02-28 20:25:09,532][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-28 20:25:16,218][wandb.sdk.lib.retry][INFO] - Retry attempt failed:
Traceback (most recent call last):
  File "D:\Anaconda3\lib\site-packages\urllib3\connection.py", line 174, in _new_conn
    conn = connection.create_connection(
  File "D:\Anaconda3\lib\site-packages\urllib3\util\connection.py", line 72, in create_connection
    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):
  File "D:\Anaconda3\lib\socket.py", line 954, in getaddrinfo
    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
socket.gaierror: [Errno 11001] getaddrinfo failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen
    httplib_response = self._make_request(
  File "D:\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 386, in _make_request
    self._validate_conn(conn)
  File "D:\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 1042, in _validate_conn
    conn.connect()
  File "D:\Anaconda3\lib\site-packages\urllib3\connection.py", line 358, in connect
    self.sock = conn = self._new_conn()
  File "D:\Anaconda3\lib\site-packages\urllib3\connection.py", line 186, in _new_conn
    raise NewConnectionError(
urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x00000191F07E6400>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Anaconda3\lib\site-packages\requests\adapters.py", line 489, in send
    resp = conn.urlopen(
  File "D:\Anaconda3\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen
    retries = retries.increment(
  File "D:\Anaconda3\lib\site-packages\urllib3\util\retry.py", line 592, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='api.wandb.ai', port=443): Max retries exceeded with url: /graphql (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000191F07E6400>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Anaconda3\lib\site-packages\wandb\sdk\lib\retry.py", line 131, in __call__
    result = self._call_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\wandb\sdk\internal\internal_api.py", line 322, in execute
    return self.client.execute(*args, **kwargs)  # type: ignore
  File "D:\Anaconda3\lib\site-packages\wandb\vendor\gql-0.2.0\wandb_gql\client.py", line 52, in execute
    result = self._get_result(document, *args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\wandb\vendor\gql-0.2.0\wandb_gql\client.py", line 60, in _get_result
    return self.transport.execute(document, *args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\wandb\sdk\lib\gql_request.py", line 55, in execute
    request = self.session.post(self.url, **post_args)
  File "D:\Anaconda3\lib\site-packages\requests\sessions.py", line 635, in post
    return self.request("POST", url, data=data, json=json, **kwargs)
  File "D:\Anaconda3\lib\site-packages\requests\sessions.py", line 587, in request
    resp = self.send(prep, **send_kwargs)
  File "D:\Anaconda3\lib\site-packages\requests\sessions.py", line 701, in send
    r = adapter.send(request, **kwargs)
  File "D:\Anaconda3\lib\site-packages\requests\adapters.py", line 565, in send
    raise ConnectionError(e, request=request)
requests.exceptions.ConnectionError: HTTPSConnectionPool(host='api.wandb.ai', port=443): Max retries exceeded with url: /graphql (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000191F07E6400>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))
[2024-02-28 20:25:57,449][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-28 20:25:58,793][__main__][INFO] - Logging hyperparameters!
[2024-02-28 20:25:58,796][__main__][INFO] - Starting training!
[2024-02-28 20:26:06,399][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1016, in _run_stage
    self.fit_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 201, in run
    self.advance()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 354, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 133, in run
    self.advance(data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 218, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 185, in run
    self._optimizer_step(kwargs.get("batch_idx", 0), closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 260, in _optimizer_step
    call._call_lightning_module_hook(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 144, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\module.py", line 1256, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\optimizer.py", line 155, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 225, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\plugins\precision\precision_plugin.py", line 114, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\optim\optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
TypeError: step() got an unexpected keyword argument 'closure'
[2024-02-28 20:26:06,407][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-02-28_20-25-09
[2024-02-28 20:26:06,408][src.utils.utils][INFO] - Closing wandb!
[2024-02-28 20:29:28,657][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-28 20:29:28,664][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-28 20:29:28,753][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-28 20:29:28,761][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-28 20:29:28,785][__main__][INFO] - Instantiating callbacks...
[2024-02-28 20:29:28,786][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-28 20:29:28,790][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-28 20:29:28,793][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-28 20:29:28,793][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-28 20:29:28,797][__main__][INFO] - Instantiating loggers...
[2024-02-28 20:29:28,798][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-28 20:38:10,939][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-28 20:38:10,946][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-28 20:38:11,032][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-28 20:38:11,038][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-28 20:38:11,059][__main__][INFO] - Instantiating callbacks...
[2024-02-28 20:38:11,060][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-28 20:38:11,064][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-28 20:38:11,066][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-28 20:38:11,066][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-28 20:38:11,069][__main__][INFO] - Instantiating loggers...
[2024-02-28 20:38:11,070][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-28 20:38:43,966][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-28 20:38:45,012][__main__][INFO] - Logging hyperparameters!
[2024-02-28 20:38:45,015][__main__][INFO] - Starting training!
[2024-02-28 20:40:13,502][__main__][INFO] - Starting testing!
[2024-02-28 20:43:34,340][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-28 20:43:34,352][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-28 20:43:34,497][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-28 20:43:34,510][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-28 20:43:34,550][__main__][INFO] - Instantiating callbacks...
[2024-02-28 20:43:34,551][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-28 20:43:34,560][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-28 20:43:34,562][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-28 20:43:34,564][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-28 20:43:34,568][__main__][INFO] - Instantiating loggers...
[2024-02-28 20:43:34,570][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-28 20:44:14,658][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-28 20:44:15,904][__main__][INFO] - Logging hyperparameters!
[2024-02-28 20:44:15,910][__main__][INFO] - Starting training!
[2024-02-28 20:46:27,748][__main__][INFO] - Starting testing!
[2024-02-28 20:46:33,416][__main__][INFO] - Best ckpt path: D:\data\CIFAR10_logs\logs\train\runs\2024-02-28_20-43-34\checkpoints\epoch_000.ckpt
[2024-02-28 20:46:33,417][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-02-28_20-43-34
[2024-02-28 20:46:33,418][src.utils.utils][INFO] - Closing wandb!
[2024-02-28 20:46:47,556][src.utils.utils][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-02-28 21:04:42,443][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-28 21:04:42,450][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-28 21:04:42,533][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-28 21:04:42,540][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-28 21:04:42,560][__main__][INFO] - Instantiating callbacks...
[2024-02-28 21:04:42,561][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-28 21:04:42,564][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-28 21:04:42,567][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-28 21:04:42,567][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-28 21:04:42,570][__main__][INFO] - Instantiating loggers...
[2024-02-28 21:04:42,570][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-28 21:05:14,281][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-28 21:05:15,374][__main__][INFO] - Logging hyperparameters!
[2024-02-28 21:05:15,378][__main__][INFO] - Starting training!
[2024-02-28 21:07:26,070][__main__][INFO] - Starting testing!
[2024-02-28 21:07:32,850][__main__][INFO] - Best ckpt path: D:\data\CIFAR10_logs\logs\train\runs\2024-02-28_21-04-42\checkpoints\epoch_003.ckpt
[2024-02-28 21:07:32,852][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-02-28_21-04-42
[2024-02-28 21:07:32,852][src.utils.utils][INFO] - Closing wandb!
[2024-02-28 21:08:08,898][src.utils.utils][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-02-28 21:10:04,501][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-28 21:10:04,510][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-28 21:10:04,595][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-28 21:10:04,606][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-28 21:10:04,632][__main__][INFO] - Instantiating callbacks...
[2024-02-28 21:10:04,632][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-28 21:10:04,636][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-28 21:10:04,637][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-28 21:10:04,638][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-28 21:10:04,640][__main__][INFO] - Instantiating loggers...
[2024-02-28 21:10:04,641][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-28 21:10:36,246][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-28 21:10:37,335][__main__][INFO] - Logging hyperparameters!
[2024-02-28 21:10:37,338][__main__][INFO] - Starting training!
[2024-02-28 21:12:47,675][__main__][INFO] - Starting testing!
[2024-02-28 21:12:53,058][__main__][INFO] - Best ckpt path: D:\data\CIFAR10_logs\logs\train\runs\2024-02-28_21-10-04\checkpoints\epoch_004.ckpt
[2024-02-28 21:12:53,060][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-02-28_21-10-04
[2024-02-28 21:12:53,060][src.utils.utils][INFO] - Closing wandb!
[2024-02-28 21:13:03,440][src.utils.utils][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-02-28 21:15:55,345][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-28 21:15:55,352][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-28 21:15:55,452][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-28 21:15:55,457][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-28 21:15:55,479][__main__][INFO] - Instantiating callbacks...
[2024-02-28 21:15:55,479][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-28 21:15:55,485][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-28 21:15:55,487][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-28 21:15:55,488][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-28 21:15:55,490][__main__][INFO] - Instantiating loggers...
[2024-02-28 21:15:55,490][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-28 21:16:27,819][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-28 21:16:28,824][__main__][INFO] - Logging hyperparameters!
[2024-02-28 21:16:28,826][__main__][INFO] - Starting training!
[2024-02-28 21:17:16,246][__main__][INFO] - Starting testing!
[2024-02-28 21:19:02,603][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-28 21:19:02,614][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-28 21:19:02,747][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-28 21:19:02,759][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-28 21:19:02,794][__main__][INFO] - Instantiating callbacks...
[2024-02-28 21:19:02,794][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-28 21:19:02,799][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-28 21:19:02,800][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-28 21:19:02,801][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-28 21:19:02,803][__main__][INFO] - Instantiating loggers...
[2024-02-28 21:19:02,804][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-28 21:19:38,915][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-28 21:19:40,051][__main__][INFO] - Logging hyperparameters!
[2024-02-28 21:19:40,054][__main__][INFO] - Starting training!
[2024-02-28 21:19:46,406][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1016, in _run_stage
    self.fit_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 201, in run
    self.advance()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 354, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 133, in run
    self.advance(data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 218, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 185, in run
    self._optimizer_step(kwargs.get("batch_idx", 0), closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 260, in _optimizer_step
    call._call_lightning_module_hook(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 144, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\module.py", line 1256, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\optimizer.py", line 155, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 225, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\plugins\precision\precision_plugin.py", line 114, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\optim\optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\optim\Adam.py", line 115, in step
    p.data.addcdiv_(-step_size, grad, denom)
TypeError: addcdiv_() takes 2 positional arguments but 3 were given
[2024-02-28 21:19:46,413][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-02-28_21-19-02
[2024-02-28 21:19:46,414][src.utils.utils][INFO] - Closing wandb!
[2024-02-28 21:24:37,001][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-28 21:24:37,010][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-28 21:24:37,127][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-28 21:24:37,135][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-28 21:24:37,170][__main__][INFO] - Instantiating callbacks...
[2024-02-28 21:24:37,170][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-28 21:24:37,175][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-28 21:24:37,177][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-28 21:24:37,178][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-28 21:24:37,182][__main__][INFO] - Instantiating loggers...
[2024-02-28 21:24:37,183][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-28 21:25:14,753][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-28 21:25:15,840][__main__][INFO] - Logging hyperparameters!
[2024-02-28 21:25:15,843][__main__][INFO] - Starting training!
[2024-02-28 21:27:30,111][__main__][INFO] - Starting testing!
[2024-02-28 21:27:35,753][__main__][INFO] - Best ckpt path: D:\data\CIFAR10_logs\logs\train\runs\2024-02-28_21-24-36\checkpoints\epoch_004.ckpt
[2024-02-28 21:27:35,754][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-02-28_21-24-36
[2024-02-28 21:27:35,755][src.utils.utils][INFO] - Closing wandb!
[2024-02-28 21:27:49,228][src.utils.utils][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-02-28 21:28:05,764][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-28 21:28:05,772][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-28 21:28:05,854][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-28 21:28:05,860][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-28 21:28:05,881][__main__][INFO] - Instantiating callbacks...
[2024-02-28 21:28:05,882][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-28 21:28:05,887][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-28 21:28:05,888][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-28 21:28:05,889][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-28 21:28:05,892][__main__][INFO] - Instantiating loggers...
[2024-02-28 21:28:05,892][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-28 21:28:36,827][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-28 21:28:37,763][__main__][INFO] - Logging hyperparameters!
[2024-02-28 21:28:37,766][__main__][INFO] - Starting training!
[2024-02-28 21:30:47,522][__main__][INFO] - Starting testing!
[2024-02-28 21:30:54,068][__main__][INFO] - Best ckpt path: D:\data\CIFAR10_logs\logs\train\runs\2024-02-28_21-28-05\checkpoints\epoch_004.ckpt
[2024-02-28 21:30:54,071][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-02-28_21-28-05
[2024-02-28 21:30:54,072][src.utils.utils][INFO] - Closing wandb!
[2024-02-28 21:31:04,281][src.utils.utils][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-02-28 21:32:30,757][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-02-28 21:32:30,765][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-02-28 21:32:30,860][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-02-28 21:32:30,866][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-02-28 21:32:30,889][__main__][INFO] - Instantiating callbacks...
[2024-02-28 21:32:30,890][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-02-28 21:32:30,894][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-02-28 21:32:30,896][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-02-28 21:32:30,896][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-02-28 21:32:30,899][__main__][INFO] - Instantiating loggers...
[2024-02-28 21:32:30,899][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-02-28 21:33:02,891][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-02-28 21:33:03,779][__main__][INFO] - Logging hyperparameters!
[2024-02-28 21:33:03,782][__main__][INFO] - Starting training!
[2024-02-28 21:35:13,530][__main__][INFO] - Starting testing!
[2024-02-28 21:35:20,784][__main__][INFO] - Best ckpt path: D:\data\CIFAR10_logs\logs\train\runs\2024-02-28_21-32-30\checkpoints\epoch_004.ckpt
[2024-02-28 21:35:20,785][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-02-28_21-32-30
[2024-02-28 21:35:20,786][src.utils.utils][INFO] - Closing wandb!
[2024-02-28 21:35:32,600][src.utils.utils][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-03-01 20:08:45,795][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-03-01 20:08:45,817][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-03-01 20:08:45,939][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-03-01 20:08:45,951][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-03-01 20:08:46,039][__main__][INFO] - Instantiating callbacks...
[2024-03-01 20:08:46,040][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-03-01 20:08:46,048][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-03-01 20:08:46,050][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-03-01 20:08:46,051][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-03-01 20:08:46,055][__main__][INFO] - Instantiating loggers...
[2024-03-01 20:08:46,055][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-03-01 20:09:18,309][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-03-01 20:09:19,641][__main__][INFO] - Logging hyperparameters!
[2024-03-01 20:09:19,646][__main__][INFO] - Starting training!
[2024-03-01 20:09:52,265][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-03-01 20:09:52,273][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-03-01 20:09:52,345][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-03-01 20:09:52,351][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-03-01 20:09:52,526][__main__][INFO] - Instantiating callbacks...
[2024-03-01 20:09:52,527][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-03-01 20:09:52,535][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-03-01 20:09:52,538][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-03-01 20:09:52,539][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-03-01 20:09:52,542][__main__][INFO] - Instantiating loggers...
[2024-03-01 20:09:52,543][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-03-01 20:10:22,339][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-03-01 20:10:23,634][__main__][INFO] - Logging hyperparameters!
[2024-03-01 20:10:23,640][__main__][INFO] - Starting training!
[2024-03-01 20:29:22,332][__main__][INFO] - Starting testing!
[2024-03-01 20:29:29,734][__main__][INFO] - Best ckpt path: D:\data\CIFAR10_logs\logs\train\runs\2024-03-01_20-09-52\checkpoints\epoch_007.ckpt
[2024-03-01 20:29:29,736][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-03-01_20-09-52
[2024-03-01 20:29:29,736][src.utils.utils][INFO] - Closing wandb!
[2024-03-01 20:29:41,905][src.utils.utils][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-03-01 20:29:56,807][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-03-01 20:29:56,815][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-03-01 20:29:56,948][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-03-01 20:29:56,968][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-03-01 20:29:57,221][__main__][INFO] - Instantiating callbacks...
[2024-03-01 20:29:57,223][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-03-01 20:29:57,229][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-03-01 20:29:57,231][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-03-01 20:29:57,232][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-03-01 20:29:57,235][__main__][INFO] - Instantiating loggers...
[2024-03-01 20:29:57,235][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-03-01 20:30:30,513][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-03-01 20:30:31,746][__main__][INFO] - Logging hyperparameters!
[2024-03-01 20:30:31,750][__main__][INFO] - Starting training!
[2024-03-01 20:46:27,475][__main__][INFO] - Starting testing!
[2024-03-01 20:50:44,114][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-03-01 20:50:44,122][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-03-01 20:50:44,215][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-03-01 20:50:44,220][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-03-01 20:50:44,235][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 92, in _call_target
    return _target_(*args, **kwargs)
TypeError: __init__() missing 2 required positional arguments: 'ch_in' and 'ch_out'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 53, in train
    model: LightningModule = hydra.utils.instantiate(cfg.model)
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 226, in instantiate
    return instantiate_node(
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 342, in instantiate_node
    value = instantiate_node(
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 347, in instantiate_node
    return _call_target(_target_, partial, args, kwargs, full_key)
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 97, in _call_target
    raise InstantiationException(msg) from e
hydra.errors.InstantiationException: Error in call to target 'srcC.models.components.resnet.ResBlk':
TypeError("__init__() missing 2 required positional arguments: 'ch_in' and 'ch_out'")
full_key: model.net
[2024-03-01 20:50:44,239][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-03-01_20-50-44
[2024-03-01 20:51:16,523][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-03-01 20:51:16,528][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-03-01 20:51:16,605][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-03-01 20:51:16,611][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-03-01 20:51:16,618][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 92, in _call_target
    return _target_(*args, **kwargs)
TypeError: __init__() missing 1 required positional argument: 'num_class'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 53, in train
    model: LightningModule = hydra.utils.instantiate(cfg.model)
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 226, in instantiate
    return instantiate_node(
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 342, in instantiate_node
    value = instantiate_node(
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 347, in instantiate_node
    return _call_target(_target_, partial, args, kwargs, full_key)
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 97, in _call_target
    raise InstantiationException(msg) from e
hydra.errors.InstantiationException: Error in call to target 'srcC.models.components.resnet.ResNet18':
TypeError("__init__() missing 1 required positional argument: 'num_class'")
full_key: model.net
[2024-03-01 20:51:16,620][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-03-01_20-51-16
[2024-03-01 20:52:05,724][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-03-01 20:52:05,731][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-03-01 20:52:05,804][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-03-01 20:52:05,811][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-03-01 20:52:05,854][__main__][INFO] - Instantiating callbacks...
[2024-03-01 20:52:05,855][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-03-01 20:52:05,860][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-03-01 20:52:05,863][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-03-01 20:52:05,864][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-03-01 20:52:05,866][__main__][INFO] - Instantiating loggers...
[2024-03-01 20:52:05,867][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-03-01 20:52:39,254][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-03-01 20:52:40,317][__main__][INFO] - Logging hyperparameters!
[2024-03-01 20:52:40,320][__main__][INFO] - Starting training!
[2024-03-01 20:52:47,191][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1014, in _run_stage
    self._run_sanity_check()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1043, in _run_sanity_check
    val_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\utilities.py", line 177, in _decorator
    return loop_run(self, *args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\evaluation_loop.py", line 115, in run
    self._evaluation_step(batch, batch_idx, dataloader_idx)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\evaluation_loop.py", line 375, in _evaluation_step
    output = call._call_strategy_hook(trainer, hook_name, *step_kwargs.values())
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 291, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 379, in validation_step
    return self.model.validation_step(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 144, in validation_step
    loss, preds, targets = self.model_step(batch)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 108, in model_step
    logits = self.forward(x)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 85, in forward
    return self.net(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\resnet.py", line 86, in forward
    x = self.outlayer(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: mat1 and mat2 shapes cannot be multiplied (128x256 and 2304x10)
[2024-03-01 20:52:47,201][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-03-01_20-52-05
[2024-03-01 20:52:47,201][src.utils.utils][INFO] - Closing wandb!
[2024-03-01 20:54:53,740][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-03-01 20:54:53,751][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-03-01 20:54:53,841][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-03-01 20:54:53,848][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-03-01 20:54:53,886][__main__][INFO] - Instantiating callbacks...
[2024-03-01 20:54:53,887][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-03-01 20:54:53,891][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-03-01 20:54:53,893][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-03-01 20:54:53,894][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-03-01 20:54:53,897][__main__][INFO] - Instantiating loggers...
[2024-03-01 20:54:53,897][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-03-01 20:55:29,967][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-03-01 20:55:30,924][__main__][INFO] - Logging hyperparameters!
[2024-03-01 20:55:30,927][__main__][INFO] - Starting training!
[2024-03-01 20:55:37,562][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1014, in _run_stage
    self._run_sanity_check()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1043, in _run_sanity_check
    val_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\utilities.py", line 177, in _decorator
    return loop_run(self, *args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\evaluation_loop.py", line 115, in run
    self._evaluation_step(batch, batch_idx, dataloader_idx)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\evaluation_loop.py", line 375, in _evaluation_step
    output = call._call_strategy_hook(trainer, hook_name, *step_kwargs.values())
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 291, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 379, in validation_step
    return self.model.validation_step(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 144, in validation_step
    loss, preds, targets = self.model_step(batch)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 108, in model_step
    logits = self.forward(x)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 85, in forward
    return self.net(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\resnet.py", line 86, in forward
    x = self.outlayer(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: mat1 and mat2 shapes cannot be multiplied (128x256 and 2304x10)
[2024-03-01 20:55:37,569][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-03-01_20-54-53
[2024-03-01 20:55:37,569][src.utils.utils][INFO] - Closing wandb!
[2024-03-01 20:58:54,014][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-03-01 20:58:54,022][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-03-01 20:58:54,096][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-03-01 20:58:54,101][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-03-01 20:58:54,137][__main__][INFO] - Instantiating callbacks...
[2024-03-01 20:58:54,137][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-03-01 20:58:54,141][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-03-01 20:58:54,144][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-03-01 20:58:54,144][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-03-01 20:58:54,147][__main__][INFO] - Instantiating loggers...
[2024-03-01 20:58:54,147][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-03-01 20:59:26,922][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-03-01 20:59:28,106][__main__][INFO] - Logging hyperparameters!
[2024-03-01 20:59:28,110][__main__][INFO] - Starting training!
[2024-03-01 21:07:38,418][__main__][INFO] - Starting testing!
[2024-03-01 21:08:10,471][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-03-01 21:08:10,480][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-03-01 21:08:10,567][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-03-01 21:08:10,574][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-03-01 21:08:10,618][__main__][INFO] - Instantiating callbacks...
[2024-03-01 21:08:10,618][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-03-01 21:08:10,622][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-03-01 21:08:10,624][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-03-01 21:08:10,624][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-03-01 21:08:10,626][__main__][INFO] - Instantiating loggers...
[2024-03-01 21:08:10,627][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-03-01 21:08:45,061][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-03-01 21:08:46,759][__main__][INFO] - Logging hyperparameters!
[2024-03-01 21:08:46,764][__main__][INFO] - Starting training!
[2024-03-01 21:13:25,867][__main__][INFO] - Starting testing!
[2024-03-01 21:13:51,276][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-03-01 21:13:51,283][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-03-01 21:13:51,376][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-03-01 21:13:51,382][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-03-01 21:13:51,419][__main__][INFO] - Instantiating callbacks...
[2024-03-01 21:13:51,419][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-03-01 21:13:51,423][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-03-01 21:13:51,425][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-03-01 21:13:51,426][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-03-01 21:13:51,428][__main__][INFO] - Instantiating loggers...
[2024-03-01 21:13:51,428][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-03-01 21:14:27,919][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-03-01 21:14:29,248][__main__][INFO] - Logging hyperparameters!
[2024-03-01 21:14:29,252][__main__][INFO] - Starting training!
[2024-03-01 21:17:27,351][__main__][INFO] - Starting testing!
[2024-03-02 13:44:50,250][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-03-02 13:44:50,257][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-03-02 13:44:50,333][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-03-02 13:44:50,340][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-03-02 13:44:50,479][__main__][INFO] - Instantiating callbacks...
[2024-03-02 13:44:50,479][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-03-02 13:44:50,487][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-03-02 13:44:50,489][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-03-02 13:44:50,491][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-03-02 13:44:50,494][__main__][INFO] - Instantiating loggers...
[2024-03-02 13:44:50,494][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-03-02 13:45:20,675][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-03-02 13:45:22,073][__main__][INFO] - Logging hyperparameters!
[2024-03-02 13:45:22,078][__main__][INFO] - Starting training!
[2024-03-02 13:57:19,892][__main__][INFO] - Starting testing!
[2024-03-04 19:39:34,654][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-03-04 19:39:34,662][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-03-04 19:39:34,742][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-03-04 19:39:34,751][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-03-04 19:39:34,899][__main__][INFO] - Instantiating callbacks...
[2024-03-04 19:39:34,900][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-03-04 19:39:34,905][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-03-04 19:39:34,907][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-03-04 19:39:34,908][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-03-04 19:39:34,913][__main__][INFO] - Instantiating loggers...
[2024-03-04 19:39:34,920][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-03-04 19:40:06,733][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-03-04 19:40:07,847][__main__][INFO] - Logging hyperparameters!
[2024-03-04 19:40:07,852][__main__][INFO] - Starting training!
[2024-03-04 19:40:14,687][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 949, in _run
    self.strategy.setup(self)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\single_device.py", line 75, in setup
    super().setup(trainer)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 149, in setup
    self.setup_optimizers(trainer)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 139, in setup_optimizers
    self.optimizers, self.lr_scheduler_configs = _init_optimizers_and_lr_schedulers(self.lightning_module)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\optimizer.py", line 168, in _init_optimizers_and_lr_schedulers
    optim_conf = call._call_lightning_module_hook(model.trainer, "configure_optimizers", pl_module=model)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 144, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 189, in configure_optimizers
    optimizer = self.hparams.optimizer(params=self.parameters())
TypeError: __init__() got an unexpected keyword argument 'weight_decay'
[2024-03-04 19:40:14,696][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-03-04_19-39-34
[2024-03-04 19:40:14,697][src.utils.utils][INFO] - Closing wandb!
[2024-03-04 19:43:31,812][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-03-04 19:43:31,820][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-03-04 19:43:31,908][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-03-04 19:43:31,916][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-03-04 19:43:32,026][__main__][INFO] - Instantiating callbacks...
[2024-03-04 19:43:32,026][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-03-04 19:43:32,032][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-03-04 19:43:32,034][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-03-04 19:43:32,035][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-03-04 19:43:32,038][__main__][INFO] - Instantiating loggers...
[2024-03-04 19:43:32,038][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-03-04 19:44:06,479][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-03-04 19:44:07,423][__main__][INFO] - Logging hyperparameters!
[2024-03-04 19:44:07,428][__main__][INFO] - Starting training!
[2024-03-04 19:44:16,201][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1016, in _run_stage
    self.fit_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 201, in run
    self.advance()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 354, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 133, in run
    self.advance(data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 218, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 185, in run
    self._optimizer_step(kwargs.get("batch_idx", 0), closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 260, in _optimizer_step
    call._call_lightning_module_hook(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 144, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\module.py", line 1256, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\optimizer.py", line 155, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 225, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\plugins\precision\precision_plugin.py", line 114, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\optim\optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\optim\Adam.py", line 111, in step
    step_size = group['lr'] * torch.sqrt(bias_correction2) / bias_correction1
TypeError: sqrt(): argument 'input' (position 1) must be Tensor, not float
[2024-03-04 19:44:16,210][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-03-04_19-43-31
[2024-03-04 19:44:16,210][src.utils.utils][INFO] - Closing wandb!
[2024-03-04 19:51:46,838][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-03-04 19:51:46,845][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-03-04 19:51:46,935][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-03-04 19:51:46,940][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-03-04 19:51:47,043][__main__][INFO] - Instantiating callbacks...
[2024-03-04 19:51:47,044][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-03-04 19:51:47,050][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-03-04 19:51:47,053][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-03-04 19:51:47,054][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-03-04 19:51:47,056][__main__][INFO] - Instantiating loggers...
[2024-03-04 19:51:47,056][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-03-04 19:52:22,593][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-03-04 19:52:23,684][__main__][INFO] - Logging hyperparameters!
[2024-03-04 19:52:23,689][__main__][INFO] - Starting training!
[2024-03-04 19:52:29,547][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 949, in _run
    self.strategy.setup(self)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\single_device.py", line 75, in setup
    super().setup(trainer)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 149, in setup
    self.setup_optimizers(trainer)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 139, in setup_optimizers
    self.optimizers, self.lr_scheduler_configs = _init_optimizers_and_lr_schedulers(self.lightning_module)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\optimizer.py", line 168, in _init_optimizers_and_lr_schedulers
    optim_conf = call._call_lightning_module_hook(model.trainer, "configure_optimizers", pl_module=model)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 144, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 189, in configure_optimizers
    optimizer = self.hparams.optimizer(params=self.parameters())
TypeError: __init__() got an unexpected keyword argument 'weight_decay'
[2024-03-04 19:52:29,557][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-03-04_19-51-46
[2024-03-04 19:52:29,558][src.utils.utils][INFO] - Closing wandb!
[2024-03-04 19:53:40,056][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-03-04 19:53:40,063][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-03-04 19:53:40,156][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-03-04 19:53:40,163][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-03-04 19:53:40,288][__main__][INFO] - Instantiating callbacks...
[2024-03-04 19:53:40,288][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-03-04 19:53:40,292][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-03-04 19:53:40,294][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-03-04 19:53:40,294][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-03-04 19:53:40,297][__main__][INFO] - Instantiating loggers...
[2024-03-04 19:53:40,297][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-03-04 19:54:12,094][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-03-04 19:54:13,338][__main__][INFO] - Logging hyperparameters!
[2024-03-04 19:54:13,343][__main__][INFO] - Starting training!
[2024-03-04 19:58:53,376][__main__][INFO] - Starting testing!
[2024-03-04 19:59:00,493][__main__][INFO] - Best ckpt path: D:\data\CIFAR10_logs\logs\train\runs\2024-03-04_19-53-39\checkpoints\epoch_002.ckpt
[2024-03-04 19:59:00,495][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-03-04_19-53-39
[2024-03-04 19:59:00,495][src.utils.utils][INFO] - Closing wandb!
[2024-03-04 20:09:42,926][src.utils.utils][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-03-04 20:09:57,364][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-03-04 20:09:57,372][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-03-04 20:09:57,470][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-03-04 20:09:57,477][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-03-04 20:09:57,594][__main__][INFO] - Instantiating callbacks...
[2024-03-04 20:09:57,594][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-03-04 20:09:57,599][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-03-04 20:09:57,601][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-03-04 20:09:57,603][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-03-04 20:09:57,608][__main__][INFO] - Instantiating loggers...
[2024-03-04 20:09:57,608][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-03-04 20:10:29,609][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-03-04 20:10:30,761][__main__][INFO] - Logging hyperparameters!
[2024-03-04 20:10:30,770][__main__][INFO] - Starting training!
[2024-03-04 20:10:58,161][__main__][INFO] - Starting testing!
[2024-03-04 20:10:58,163][__main__][WARNING] - Best ckpt not found! Using current weights for testing...
[2024-03-04 21:05:34,701][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-03-04 21:05:34,708][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-03-04 21:05:34,784][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-03-04 21:05:34,790][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-03-04 21:05:34,887][__main__][INFO] - Instantiating callbacks...
[2024-03-04 21:05:34,888][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-03-04 21:05:34,892][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-03-04 21:05:34,893][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-03-04 21:05:34,894][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-03-04 21:05:34,896][__main__][INFO] - Instantiating loggers...
[2024-03-04 21:05:34,896][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-03-04 21:06:10,612][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-03-04 21:06:11,611][__main__][INFO] - Logging hyperparameters!
[2024-03-04 21:06:11,615][__main__][INFO] - Starting training!
[2024-03-04 21:08:06,748][__main__][INFO] - Starting testing!
[2024-03-08 17:19:01,850][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-03-08 17:19:01,859][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-03-08 17:19:01,945][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-03-08 17:19:01,952][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-03-08 17:19:02,107][__main__][INFO] - Instantiating callbacks...
[2024-03-08 17:19:02,108][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-03-08 17:19:02,114][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-03-08 17:19:02,117][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-03-08 17:19:02,118][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-03-08 17:19:02,121][__main__][INFO] - Instantiating loggers...
[2024-03-08 17:19:02,122][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-03-08 17:19:04,037][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\Anaconda3\lib\site-packages\wandb\sdk\wandb_init.py", line 1143, in init
    wi.setup(kwargs)
  File "D:\Anaconda3\lib\site-packages\wandb\sdk\wandb_init.py", line 172, in setup
    self._wl = wandb_setup.setup(settings=setup_settings)
  File "D:\Anaconda3\lib\site-packages\wandb\sdk\wandb_setup.py", line 327, in setup
    ret = _setup(settings=settings)
  File "D:\Anaconda3\lib\site-packages\wandb\sdk\wandb_setup.py", line 320, in _setup
    wl = _WandbSetup(settings=settings)
  File "D:\Anaconda3\lib\site-packages\wandb\sdk\wandb_setup.py", line 303, in __init__
    _WandbSetup._instance = _WandbSetup__WandbSetup(settings=settings, pid=pid)
  File "D:\Anaconda3\lib\site-packages\wandb\sdk\wandb_setup.py", line 114, in __init__
    self._setup()
  File "D:\Anaconda3\lib\site-packages\wandb\sdk\wandb_setup.py", line 250, in _setup
    self._setup_manager()
  File "D:\Anaconda3\lib\site-packages\wandb\sdk\wandb_setup.py", line 277, in _setup_manager
    self._manager = wandb_manager._Manager(settings=self._settings)
  File "D:\Anaconda3\lib\site-packages\wandb\sdk\wandb_manager.py", line 144, in __init__
    self._service.start()
  File "D:\Anaconda3\lib\site-packages\wandb\sdk\service\service.py", line 218, in start
    self._launch_server()
  File "D:\Anaconda3\lib\site-packages\wandb\sdk\service\service.py", line 210, in _launch_server
    self._wait_for_ports(fname, proc=internal_proc)
  File "D:\Anaconda3\lib\site-packages\wandb\sdk\service\service.py", line 125, in _wait_for_ports
    time.sleep(0.2)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 92, in _call_target
    return _target_(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loggers\wandb.py", line 358, in __init__
    _ = self.experiment
  File "D:\Anaconda3\lib\site-packages\lightning\fabric\loggers\logger.py", line 114, in experiment
    return fn(self)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loggers\wandb.py", line 406, in experiment
    self._experiment = wandb.init(**self._wandb_init)
  File "D:\Anaconda3\lib\site-packages\wandb\sdk\wandb_init.py", line 1168, in init
    assert logger
AssertionError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 59, in train
    logger: List[Logger] = utils.instantiate_loggers(cfg.get("logger"))
  File "D:\pycharmproject\template\src\utils\instantiators.py", line 48, in instantiate_loggers
    logger.append(hydra.utils.instantiate(lg_conf))
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 226, in instantiate
    return instantiate_node(
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 347, in instantiate_node
    return _call_target(_target_, partial, args, kwargs, full_key)
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 97, in _call_target
    raise InstantiationException(msg) from e
hydra.errors.InstantiationException: Error in call to target 'lightning.pytorch.loggers.wandb.WandbLogger':
AssertionError()
full_key: logger.wandb
[2024-03-08 17:19:04,048][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-03-08_17-19-01
[2024-03-08 17:19:25,735][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-03-08 17:19:25,743][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-03-08 17:19:25,835][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-03-08 17:19:25,841][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-03-08 17:19:26,077][__main__][INFO] - Instantiating callbacks...
[2024-03-08 17:19:26,078][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-03-08 17:19:26,091][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-03-08 17:19:26,094][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-03-08 17:19:26,095][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-03-08 17:19:26,102][__main__][INFO] - Instantiating loggers...
[2024-03-08 17:19:26,105][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-03-08 17:20:04,605][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-03-08 17:20:05,649][__main__][INFO] - Logging hyperparameters!
[2024-03-08 17:20:05,653][__main__][INFO] - Starting training!
[2024-03-08 17:22:17,248][__main__][INFO] - Starting testing!
[2024-03-09 15:01:26,228][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-03-09 15:01:26,240][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-03-09 15:01:26,377][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-03-09 15:01:26,391][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-03-09 15:01:27,176][__main__][INFO] - Instantiating callbacks...
[2024-03-09 15:01:27,176][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-03-09 15:01:27,182][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-03-09 15:01:27,184][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-03-09 15:01:27,185][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-03-09 15:01:27,187][__main__][INFO] - Instantiating loggers...
[2024-03-09 15:01:27,189][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-03-09 15:02:06,063][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-03-09 15:02:07,769][__main__][INFO] - Logging hyperparameters!
[2024-03-09 15:02:07,776][__main__][INFO] - Starting training!
[2024-03-09 15:07:36,479][__main__][INFO] - Starting testing!
[2024-03-09 15:07:53,515][__main__][INFO] - Best ckpt path: D:\data\CIFAR10_logs\logs\train\runs\2024-03-09_15-01-26\checkpoints\epoch_000.ckpt
[2024-03-09 15:07:53,516][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-03-09_15-01-26
[2024-03-09 15:07:53,517][src.utils.utils][INFO] - Closing wandb!
[2024-03-11 17:21:27,475][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-03-11 17:21:27,483][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-03-11 17:21:27,573][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-03-11 17:21:27,580][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-03-11 17:21:27,680][__main__][INFO] - Instantiating callbacks...
[2024-03-11 17:21:27,681][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-03-11 17:21:27,684][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-03-11 17:21:27,685][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-03-11 17:21:27,686][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-03-11 17:21:27,688][__main__][INFO] - Instantiating loggers...
[2024-03-11 17:21:27,688][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-03-11 17:21:55,363][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-03-11 17:21:56,624][__main__][INFO] - Logging hyperparameters!
[2024-03-11 17:21:56,628][__main__][INFO] - Starting training!
[2024-03-11 17:23:12,198][__main__][INFO] - Starting testing!
[2024-03-11 17:23:18,495][__main__][INFO] - Best ckpt path: D:\data\CIFAR10_logs\logs\train\runs\2024-03-11_17-21-27\checkpoints\epoch_000.ckpt
[2024-03-11 17:23:18,496][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-03-11_17-21-27
[2024-03-11 17:23:18,497][src.utils.utils][INFO] - Closing wandb!
[2024-03-11 17:23:48,919][src.utils.utils][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-03-11 17:32:30,242][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-03-11 17:32:30,248][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-03-11 17:32:30,322][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-03-11 17:32:30,327][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-03-11 17:32:30,398][__main__][INFO] - Instantiating callbacks...
[2024-03-11 17:32:30,399][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-03-11 17:32:30,402][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-03-11 17:32:30,404][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-03-11 17:32:30,404][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-03-11 17:32:30,406][__main__][INFO] - Instantiating loggers...
[2024-03-11 17:32:30,406][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-03-11 17:33:00,424][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-03-11 17:33:01,619][__main__][INFO] - Logging hyperparameters!
[2024-03-11 17:33:01,623][__main__][INFO] - Starting training!
[2024-03-11 17:34:07,472][__main__][INFO] - Starting testing!
[2024-03-11 17:34:14,339][__main__][INFO] - Best ckpt path: D:\data\CIFAR10_logs\logs\train\runs\2024-03-11_17-32-30\checkpoints\epoch_000.ckpt
[2024-03-11 17:34:14,341][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-03-11_17-32-30
[2024-03-11 17:34:14,341][src.utils.utils][INFO] - Closing wandb!
[2024-03-11 17:34:29,964][src.utils.utils][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-03-11 17:38:11,480][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-03-11 17:38:11,487][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-03-11 17:38:11,561][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-03-11 17:38:11,567][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-03-11 17:38:11,766][__main__][INFO] - Instantiating callbacks...
[2024-03-11 17:38:11,767][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-03-11 17:38:11,771][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-03-11 17:38:11,773][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-03-11 17:38:11,773][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-03-11 17:38:11,776][__main__][INFO] - Instantiating loggers...
[2024-03-11 17:38:11,777][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-03-11 17:38:41,896][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-03-11 17:38:43,038][__main__][INFO] - Logging hyperparameters!
[2024-03-11 17:38:43,042][__main__][INFO] - Starting training!
[2024-03-11 17:41:09,378][__main__][INFO] - Starting testing!
[2024-03-11 17:41:17,868][__main__][INFO] - Best ckpt path: D:\data\CIFAR10_logs\logs\train\runs\2024-03-11_17-38-11\checkpoints\epoch_001.ckpt
[2024-03-11 17:41:17,870][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-03-11_17-38-11
[2024-03-11 17:41:17,871][src.utils.utils][INFO] - Closing wandb!
[2024-03-11 17:41:32,795][src.utils.utils][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-03-11 19:33:52,842][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-03-11 19:33:52,850][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-03-11 19:33:52,921][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-03-11 19:33:52,927][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-03-11 19:33:53,124][__main__][INFO] - Instantiating callbacks...
[2024-03-11 19:33:53,125][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-03-11 19:33:53,128][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-03-11 19:33:53,130][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-03-11 19:33:53,132][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-03-11 19:33:53,135][__main__][INFO] - Instantiating loggers...
[2024-03-11 19:33:53,136][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-03-11 19:34:29,096][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-03-11 19:34:30,373][__main__][INFO] - Logging hyperparameters!
[2024-03-11 19:34:30,378][__main__][INFO] - Starting training!
[2024-03-11 19:37:18,812][__main__][INFO] - Starting testing!
[2024-03-11 19:37:37,653][__main__][INFO] - Best ckpt path: D:\data\CIFAR10_logs\logs\train\runs\2024-03-11_19-33-52\checkpoints\epoch_001.ckpt
[2024-03-11 19:37:37,657][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-03-11_19-33-52
[2024-03-11 19:37:37,659][src.utils.utils][INFO] - Closing wandb!
[2024-03-11 19:37:53,402][src.utils.utils][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-03-11 19:45:31,686][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-03-11 19:45:31,695][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-03-11 19:45:31,790][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-03-11 19:45:31,798][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-03-11 19:45:32,009][__main__][INFO] - Instantiating callbacks...
[2024-03-11 19:45:32,009][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-03-11 19:45:32,015][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-03-11 19:45:32,017][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-03-11 19:45:32,018][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-03-11 19:45:32,021][__main__][INFO] - Instantiating loggers...
[2024-03-11 19:45:32,022][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-03-11 19:46:09,960][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-03-11 19:46:11,464][__main__][INFO] - Logging hyperparameters!
[2024-03-11 19:46:11,469][__main__][INFO] - Starting training!
[2024-03-11 19:48:59,901][__main__][INFO] - Starting testing!
[2024-03-11 19:49:08,610][__main__][INFO] - Best ckpt path: D:\data\CIFAR10_logs\logs\train\runs\2024-03-11_19-45-31\checkpoints\epoch_001.ckpt
[2024-03-11 19:49:08,612][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-03-11_19-45-31
[2024-03-11 19:49:08,612][src.utils.utils][INFO] - Closing wandb!
[2024-03-12 19:14:19,645][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-03-12 19:14:19,653][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-03-12 19:14:19,738][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-03-12 19:14:19,747][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-03-12 19:14:19,965][__main__][INFO] - Instantiating callbacks...
[2024-03-12 19:14:19,966][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-03-12 19:14:19,971][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-03-12 19:14:19,973][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-03-12 19:14:19,974][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-03-12 19:14:19,977][__main__][INFO] - Instantiating loggers...
[2024-03-12 19:14:19,977][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-03-12 19:14:56,393][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-03-12 19:14:57,739][__main__][INFO] - Logging hyperparameters!
[2024-03-12 19:14:57,745][__main__][INFO] - Starting training!
[2024-03-12 19:16:45,817][__main__][INFO] - Starting testing!
[2024-03-12 19:17:53,183][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-03-12 19:17:53,191][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-03-12 19:17:53,259][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-03-12 19:17:53,265][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-03-12 19:17:53,446][__main__][INFO] - Instantiating callbacks...
[2024-03-12 19:17:53,447][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-03-12 19:17:53,453][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-03-12 19:17:53,455][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-03-12 19:17:53,456][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-03-12 19:17:53,459][__main__][INFO] - Instantiating loggers...
[2024-03-12 19:17:53,459][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-03-12 19:18:21,486][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-03-12 19:18:22,523][__main__][INFO] - Logging hyperparameters!
[2024-03-12 19:18:22,532][__main__][INFO] - Starting training!
[2024-03-12 19:18:30,296][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1016, in _run_stage
    self.fit_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 201, in run
    self.advance()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 354, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 133, in run
    self.advance(data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 218, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 185, in run
    self._optimizer_step(kwargs.get("batch_idx", 0), closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 260, in _optimizer_step
    call._call_lightning_module_hook(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 144, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\module.py", line 1256, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\optimizer.py", line 155, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 225, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\plugins\precision\precision_plugin.py", line 114, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\optim\optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\optim\optimizer.py", line 33, in _use_grad
    ret = func(self, *args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\optim\adam.py", line 121, in step
    loss = closure()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\plugins\precision\precision_plugin.py", line 101, in _wrap_closure
    closure_result = closure()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 140, in __call__
    self._result = self.closure(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 126, in closure
    step_output = self._step_fn()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 307, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 291, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 367, in training_step
    return self.model.training_step(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 127, in training_step
    self.train_acc(preds, targets)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\torchmetrics\metric.py", line 290, in forward
    self._forward_cache = self._forward_reduce_state_update(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\torchmetrics\metric.py", line 357, in _forward_reduce_state_update
    self.update(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\torchmetrics\metric.py", line 456, in wrapped_func
    raise err
  File "D:\Anaconda3\lib\site-packages\torchmetrics\metric.py", line 446, in wrapped_func
    update(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\torchmetrics\classification\stat_scores.py", line 314, in update
    _multiclass_stat_scores_tensor_validation(
  File "D:\Anaconda3\lib\site-packages\torchmetrics\functional\classification\stat_scores.py", line 313, in _multiclass_stat_scores_tensor_validation
    raise RuntimeError(
RuntimeError: Detected more unique values in `preds` than `num_classes`. Expected only 10 but found 56 in `preds`.
[2024-03-12 19:18:30,328][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-03-12_19-17-53
[2024-03-12 19:18:30,330][src.utils.utils][INFO] - Closing wandb!
[2024-03-12 19:20:36,684][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-03-12 19:20:36,691][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-03-12 19:20:36,787][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-03-12 19:20:36,794][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-03-12 19:20:36,972][__main__][INFO] - Instantiating callbacks...
[2024-03-12 19:20:36,973][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-03-12 19:20:36,976][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-03-12 19:20:36,979][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-03-12 19:20:36,980][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-03-12 19:20:36,982][__main__][INFO] - Instantiating loggers...
[2024-03-12 19:20:36,982][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-03-12 19:21:04,578][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-03-12 19:21:05,688][__main__][INFO] - Logging hyperparameters!
[2024-03-12 19:21:05,695][__main__][INFO] - Starting training!
[2024-03-12 19:25:19,341][__main__][INFO] - Starting testing!
[2024-03-12 19:25:34,076][__main__][INFO] - Best ckpt path: D:\data\CIFAR10_logs\logs\train\runs\2024-03-12_19-20-36\checkpoints\epoch_001.ckpt
[2024-03-12 19:25:34,078][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-03-12_19-20-36
[2024-03-12 19:25:34,078][src.utils.utils][INFO] - Closing wandb!
[2024-03-12 20:21:59,417][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-03-12 20:21:59,427][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-03-12 20:21:59,536][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-03-12 20:21:59,544][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-03-12 20:21:59,603][__main__][INFO] - Instantiating callbacks...
[2024-03-12 20:21:59,604][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-03-12 20:21:59,611][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-03-12 20:21:59,613][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-03-12 20:21:59,615][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-03-12 20:21:59,618][__main__][INFO] - Instantiating loggers...
[2024-03-12 20:21:59,620][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-03-12 20:22:31,136][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-03-12 20:22:32,516][__main__][INFO] - Logging hyperparameters!
[2024-03-12 20:22:32,520][__main__][INFO] - Starting training!
[2024-03-12 20:23:31,122][__main__][INFO] - Starting testing!
[2024-03-12 20:23:37,311][__main__][INFO] - Best ckpt path: D:\data\CIFAR10_logs\logs\train\runs\2024-03-12_20-21-59\checkpoints\epoch_001.ckpt
[2024-03-12 20:23:37,313][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-03-12_20-21-59
[2024-03-12 20:23:37,313][src.utils.utils][INFO] - Closing wandb!
[2024-05-28 20:46:10,426][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-05-28 20:46:10,438][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-05-28 20:46:10,545][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-05-28 20:46:10,554][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-05-28 20:46:10,582][__main__][INFO] - Instantiating callbacks...
[2024-05-28 20:46:10,583][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-28 20:46:10,587][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-05-28 20:46:10,589][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-28 20:46:10,590][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-28 20:46:10,593][__main__][INFO] - Instantiating loggers...
[2024-05-28 20:46:10,593][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-05-28 20:46:45,305][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-05-28 20:46:46,750][__main__][INFO] - Logging hyperparameters!
[2024-05-28 20:46:46,755][__main__][INFO] - Starting training!
[2024-05-28 20:49:48,139][__main__][INFO] - Starting testing!
[2024-05-28 20:49:55,501][__main__][INFO] - Best ckpt path: D:\data\CIFAR10_logs\logs\train\runs\2024-05-28_20-46-10\checkpoints\epoch_004.ckpt
[2024-05-28 20:49:55,502][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-05-28_20-46-10
[2024-05-28 20:49:55,504][src.utils.utils][INFO] - Closing wandb!
[2024-05-28 20:50:12,188][src.utils.utils][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-28 20:52:14,404][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-05-28 20:52:14,417][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-05-28 20:52:14,528][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-05-28 20:52:14,534][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-05-28 20:52:14,552][__main__][INFO] - Instantiating callbacks...
[2024-05-28 20:52:14,554][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-28 20:52:14,558][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-05-28 20:52:14,560][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-28 20:52:14,561][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-28 20:52:14,563][__main__][INFO] - Instantiating loggers...
[2024-05-28 20:52:14,563][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-05-28 20:52:50,427][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-05-28 20:52:51,697][__main__][INFO] - Logging hyperparameters!
[2024-05-28 20:52:51,704][__main__][INFO] - Starting training!
[2024-05-28 20:55:54,694][__main__][INFO] - Starting testing!
[2024-05-28 20:56:03,780][__main__][INFO] - Best ckpt path: D:\data\CIFAR10_logs\logs\train\runs\2024-05-28_20-52-14\checkpoints\epoch_004.ckpt
[2024-05-28 20:56:03,783][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-05-28_20-52-14
[2024-05-28 20:56:03,784][src.utils.utils][INFO] - Closing wandb!
[2024-05-28 20:56:15,880][src.utils.utils][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-28 20:56:43,755][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-05-28 20:56:43,764][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-05-28 20:56:43,931][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-05-28 20:56:43,940][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-05-28 20:56:43,962][__main__][INFO] - Instantiating callbacks...
[2024-05-28 20:56:43,962][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-28 20:56:43,967][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-05-28 20:56:43,971][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-28 20:56:43,973][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-28 20:56:43,978][__main__][INFO] - Instantiating loggers...
[2024-05-28 20:56:43,979][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-05-28 20:57:24,074][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-05-28 20:57:26,051][__main__][INFO] - Logging hyperparameters!
[2024-05-28 20:57:26,059][__main__][INFO] - Starting training!
[2024-05-28 21:01:17,079][__main__][INFO] - Starting testing!
[2024-05-28 21:01:28,714][__main__][INFO] - Best ckpt path: D:\data\CIFAR10_logs\logs\train\runs\2024-05-28_20-56-43\checkpoints\epoch_004.ckpt
[2024-05-28 21:01:28,718][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-05-28_20-56-43
[2024-05-28 21:01:28,719][src.utils.utils][INFO] - Closing wandb!
[2024-05-28 21:01:39,444][src.utils.utils][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-28 21:03:13,988][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-05-28 21:03:13,996][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-05-28 21:03:14,094][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-05-28 21:03:14,102][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-05-28 21:03:14,107][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\utils.py", line 644, in _locate
    obj = getattr(obj, part)
AttributeError: module 'srcC.optim' has no attribute 'Adam'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\utils.py", line 650, in _locate
    obj = import_module(mod)
  File "D:\Anaconda3\lib\importlib\__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 984, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'srcC.optim.Adam'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 134, in _resolve_target
    target = _locate(target)
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\utils.py", line 653, in _locate
    raise ImportError(
ImportError: Error loading 'srcC.optim.Adam.Adam':
ModuleNotFoundError("No module named 'srcC.optim.Adam'")
Are you sure that 'Adam' is importable from module 'srcC.optim'?

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 53, in train
    model: LightningModule = hydra.utils.instantiate(cfg.model)
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 226, in instantiate
    return instantiate_node(
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 342, in instantiate_node
    value = instantiate_node(
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 333, in instantiate_node
    _target_ = _resolve_target(node.get(_Keys.TARGET), full_key)
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 139, in _resolve_target
    raise InstantiationException(msg) from e
hydra.errors.InstantiationException: Error locating target 'srcC.optim.Adam.Adam', set env var HYDRA_FULL_ERROR=1 to see chained exception.
full_key: model.optimizer
[2024-05-28 21:03:14,112][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-05-28_21-03-13
[2024-05-28 21:04:32,584][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-05-28 21:04:32,592][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-05-28 21:04:32,686][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-05-28 21:04:32,693][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-05-28 21:04:32,712][__main__][INFO] - Instantiating callbacks...
[2024-05-28 21:04:32,713][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-28 21:04:32,717][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-05-28 21:04:32,719][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-28 21:04:32,720][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-28 21:04:32,723][__main__][INFO] - Instantiating loggers...
[2024-05-28 21:04:32,723][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-05-28 21:05:09,481][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-05-28 21:05:11,047][__main__][INFO] - Logging hyperparameters!
[2024-05-28 21:05:11,052][__main__][INFO] - Starting training!
[2024-05-28 21:08:52,797][__main__][INFO] - Starting testing!
[2024-05-28 21:09:01,386][__main__][INFO] - Best ckpt path: D:\data\CIFAR10_logs\logs\train\runs\2024-05-28_21-04-32\checkpoints\epoch_004.ckpt
[2024-05-28 21:09:01,388][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-05-28_21-04-32
[2024-05-28 21:09:01,388][src.utils.utils][INFO] - Closing wandb!
[2024-05-28 21:09:12,586][src.utils.utils][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-28 21:09:36,936][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-05-28 21:09:36,944][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-05-28 21:09:37,028][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-05-28 21:09:37,035][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-05-28 21:09:37,058][__main__][INFO] - Instantiating callbacks...
[2024-05-28 21:09:37,059][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-28 21:09:37,062][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-05-28 21:09:37,064][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-28 21:09:37,066][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-28 21:09:37,068][__main__][INFO] - Instantiating loggers...
[2024-05-28 21:09:37,068][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-05-28 21:10:12,486][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-05-28 21:10:13,864][__main__][INFO] - Logging hyperparameters!
[2024-05-28 21:10:13,867][__main__][INFO] - Starting training!
[2024-05-28 21:13:39,100][__main__][INFO] - Starting testing!
[2024-05-28 21:13:47,168][__main__][INFO] - Best ckpt path: D:\data\CIFAR10_logs\logs\train\runs\2024-05-28_21-09-36\checkpoints\epoch_003.ckpt
[2024-05-28 21:13:47,171][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-05-28_21-09-36
[2024-05-28 21:13:47,172][src.utils.utils][INFO] - Closing wandb!
[2024-05-28 21:13:56,787][src.utils.utils][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-28 21:14:29,111][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-05-28 21:14:29,122][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-05-28 21:14:29,243][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-05-28 21:14:29,251][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-05-28 21:14:29,282][__main__][INFO] - Instantiating callbacks...
[2024-05-28 21:14:29,283][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-28 21:14:29,289][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-05-28 21:14:29,290][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-28 21:14:29,291][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-28 21:14:29,296][__main__][INFO] - Instantiating loggers...
[2024-05-28 21:14:29,297][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-05-28 21:15:08,158][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-05-28 21:15:10,153][__main__][INFO] - Logging hyperparameters!
[2024-05-28 21:15:10,157][__main__][INFO] - Starting training!
[2024-05-28 21:18:40,349][__main__][INFO] - Starting testing!
[2024-05-28 21:18:47,497][__main__][INFO] - Best ckpt path: D:\data\CIFAR10_logs\logs\train\runs\2024-05-28_21-14-28\checkpoints\epoch_004.ckpt
[2024-05-28 21:18:47,498][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-05-28_21-14-28
[2024-05-28 21:18:47,499][src.utils.utils][INFO] - Closing wandb!
[2024-05-28 21:18:56,998][src.utils.utils][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-02 15:45:31,081][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-06-02 15:45:31,090][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-06-02 15:45:31,188][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-06-02 15:45:31,198][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-06-02 15:45:31,221][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\utils.py", line 650, in _locate
    obj = import_module(mod)
  File "D:\Anaconda3\lib\importlib\__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 981, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'srcC.models.components.shufflenetv2.Shufflenetv2'; 'srcC.models.components.shufflenetv2' is not a package

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 134, in _resolve_target
    target = _locate(target)
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\utils.py", line 653, in _locate
    raise ImportError(
ImportError: Error loading 'srcC.models.components.shufflenetv2.Shufflenetv2':
ModuleNotFoundError("No module named 'srcC.models.components.shufflenetv2.Shufflenetv2'; 'srcC.models.components.shufflenetv2' is not a package")
Are you sure that 'Shufflenetv2' is importable from module 'srcC.models.components.shufflenetv2'?

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 53, in train
    model: LightningModule = hydra.utils.instantiate(cfg.model)
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 226, in instantiate
    return instantiate_node(
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 342, in instantiate_node
    value = instantiate_node(
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 333, in instantiate_node
    _target_ = _resolve_target(node.get(_Keys.TARGET), full_key)
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 139, in _resolve_target
    raise InstantiationException(msg) from e
hydra.errors.InstantiationException: Error locating target 'srcC.models.components.shufflenetv2.Shufflenetv2', set env var HYDRA_FULL_ERROR=1 to see chained exception.
full_key: model.net
[2024-06-02 15:45:31,231][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-06-02_15-45-30
[2024-06-02 15:46:23,619][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-06-02 15:46:23,630][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-06-02 15:46:23,783][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-06-02 15:46:23,796][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-06-02 15:46:24,006][__main__][INFO] - Instantiating callbacks...
[2024-06-02 15:46:24,007][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-02 15:46:24,012][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-06-02 15:46:24,014][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-02 15:46:24,017][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-02 15:46:24,019][__main__][INFO] - Instantiating loggers...
[2024-06-02 15:46:24,019][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-06-02 15:47:04,772][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-06-02 15:47:06,009][__main__][INFO] - Logging hyperparameters!
[2024-06-02 15:47:06,017][__main__][INFO] - Starting training!
[2024-06-02 15:47:17,664][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1016, in _run_stage
    self.fit_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 201, in run
    self.advance()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 354, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 133, in run
    self.advance(data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 218, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 185, in run
    self._optimizer_step(kwargs.get("batch_idx", 0), closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 260, in _optimizer_step
    call._call_lightning_module_hook(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 144, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\module.py", line 1256, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\optimizer.py", line 155, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 225, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\plugins\precision\precision_plugin.py", line 114, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\optim\optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\optim\Adam.py", line 21, in step
    loss = closure()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\plugins\precision\precision_plugin.py", line 101, in _wrap_closure
    closure_result = closure()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 140, in __call__
    self._result = self.closure(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 126, in closure
    step_output = self._step_fn()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 307, in _training_step
    training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 291, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 367, in training_step
    return self.model.training_step(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 127, in training_step
    self.train_acc(preds, targets)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\torchmetrics\metric.py", line 290, in forward
    self._forward_cache = self._forward_reduce_state_update(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\torchmetrics\metric.py", line 357, in _forward_reduce_state_update
    self.update(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\torchmetrics\metric.py", line 456, in wrapped_func
    raise err
  File "D:\Anaconda3\lib\site-packages\torchmetrics\metric.py", line 446, in wrapped_func
    update(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\torchmetrics\classification\stat_scores.py", line 314, in update
    _multiclass_stat_scores_tensor_validation(
  File "D:\Anaconda3\lib\site-packages\torchmetrics\functional\classification\stat_scores.py", line 313, in _multiclass_stat_scores_tensor_validation
    raise RuntimeError(
RuntimeError: Detected more unique values in `preds` than `num_classes`. Expected only 10 but found 28 in `preds`.
[2024-06-02 15:47:17,694][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-06-02_15-46-23
[2024-06-02 15:47:17,695][src.utils.utils][INFO] - Closing wandb!
[2024-06-02 15:48:08,658][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-06-02 15:48:08,666][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-06-02 15:48:08,750][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-06-02 15:48:08,756][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-06-02 15:48:08,835][__main__][INFO] - Instantiating callbacks...
[2024-06-02 15:48:08,835][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-02 15:48:08,840][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-06-02 15:48:08,841][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-02 15:48:08,842][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-02 15:48:08,844][__main__][INFO] - Instantiating loggers...
[2024-06-02 15:48:08,845][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-06-02 15:48:50,835][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-06-02 15:48:52,191][__main__][INFO] - Logging hyperparameters!
[2024-06-02 15:48:52,196][__main__][INFO] - Starting training!
[2024-06-02 15:53:10,116][__main__][INFO] - Starting testing!
[2024-06-02 15:53:28,156][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-06-02 15:53:28,163][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-06-02 15:53:28,247][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-06-02 15:53:28,252][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-06-02 15:53:28,262][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\utils.py", line 650, in _locate
    obj = import_module(mod)
  File "D:\Anaconda3\lib\importlib\__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 981, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'srcC.models.components.shufflenetv1.ShuffleNetV1'; 'srcC.models.components.shufflenetv1' is not a package

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 134, in _resolve_target
    target = _locate(target)
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\utils.py", line 653, in _locate
    raise ImportError(
ImportError: Error loading 'srcC.models.components.shufflenetv1.ShuffleNetV1':
ModuleNotFoundError("No module named 'srcC.models.components.shufflenetv1.ShuffleNetV1'; 'srcC.models.components.shufflenetv1' is not a package")
Are you sure that 'ShuffleNetV1' is importable from module 'srcC.models.components.shufflenetv1'?

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 53, in train
    model: LightningModule = hydra.utils.instantiate(cfg.model)
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 226, in instantiate
    return instantiate_node(
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 342, in instantiate_node
    value = instantiate_node(
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 333, in instantiate_node
    _target_ = _resolve_target(node.get(_Keys.TARGET), full_key)
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 139, in _resolve_target
    raise InstantiationException(msg) from e
hydra.errors.InstantiationException: Error locating target 'srcC.models.components.shufflenetv1.ShuffleNetV1', set env var HYDRA_FULL_ERROR=1 to see chained exception.
full_key: model.net
[2024-06-02 15:53:28,264][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-06-02_15-53-28
[2024-06-02 15:54:06,728][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-06-02 15:54:06,737][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-06-02 15:54:06,836][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-06-02 15:54:06,843][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-06-02 15:54:06,851][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 92, in _call_target
    return _target_(*args, **kwargs)
TypeError: __init__() missing 2 required positional arguments: 'num_class' and 'group'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 53, in train
    model: LightningModule = hydra.utils.instantiate(cfg.model)
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 226, in instantiate
    return instantiate_node(
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 342, in instantiate_node
    value = instantiate_node(
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 347, in instantiate_node
    return _call_target(_target_, partial, args, kwargs, full_key)
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 97, in _call_target
    raise InstantiationException(msg) from e
hydra.errors.InstantiationException: Error in call to target 'srcC.models.components.shufflenetv1.shufflenet':
TypeError("__init__() missing 2 required positional arguments: 'num_class' and 'group'")
full_key: model.net
[2024-06-02 15:54:06,853][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-06-02_15-54-06
[2024-06-02 15:54:48,224][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-06-02 15:54:48,232][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-06-02 15:54:48,309][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-06-02 15:54:48,314][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-06-02 15:54:48,323][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 92, in _call_target
    return _target_(*args, **kwargs)
TypeError: __init__() missing 2 required positional arguments: 'num_class' and 'group'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 53, in train
    model: LightningModule = hydra.utils.instantiate(cfg.model)
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 226, in instantiate
    return instantiate_node(
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 342, in instantiate_node
    value = instantiate_node(
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 347, in instantiate_node
    return _call_target(_target_, partial, args, kwargs, full_key)
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 97, in _call_target
    raise InstantiationException(msg) from e
hydra.errors.InstantiationException: Error in call to target 'srcC.models.components.shufflenetv1.shufflenet':
TypeError("__init__() missing 2 required positional arguments: 'num_class' and 'group'")
full_key: model.net
[2024-06-02 15:54:48,325][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-06-02_15-54-48
[2024-06-02 15:55:53,326][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-06-02 15:55:53,334][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-06-02 15:55:53,413][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-06-02 15:55:53,419][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-06-02 15:55:53,491][__main__][INFO] - Instantiating callbacks...
[2024-06-02 15:55:53,492][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-02 15:55:53,495][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-06-02 15:55:53,496][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-02 15:55:53,497][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-02 15:55:53,499][__main__][INFO] - Instantiating loggers...
[2024-06-02 15:55:53,499][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-06-02 15:56:30,056][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-06-02 15:56:31,438][__main__][INFO] - Logging hyperparameters!
[2024-06-02 15:56:31,445][__main__][INFO] - Starting training!
[2024-06-02 15:56:39,133][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1014, in _run_stage
    self._run_sanity_check()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1043, in _run_sanity_check
    val_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\utilities.py", line 177, in _decorator
    return loop_run(self, *args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\evaluation_loop.py", line 115, in run
    self._evaluation_step(batch, batch_idx, dataloader_idx)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\evaluation_loop.py", line 375, in _evaluation_step
    output = call._call_strategy_hook(trainer, hook_name, *step_kwargs.values())
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 291, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 379, in validation_step
    return self.model.validation_step(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 144, in validation_step
    loss, preds, targets = self.model_step(batch)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 108, in model_step
    logits = self.forward(x)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 85, in forward
    return self.net(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\shufflenetv1.py", line 100, in forward
    x=self.layer1(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\container.py", line 217, in forward
    input = module(input)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\shufflenetv1.py", line 43, in forward
    return F.relu(torch.cat([x,residual],1))
AttributeError: module 'torch.functional' has no attribute 'relu'
[2024-06-02 15:56:39,142][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-06-02_15-55-53
[2024-06-02 15:56:39,143][src.utils.utils][INFO] - Closing wandb!
[2024-06-02 15:58:08,772][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-06-02 15:58:08,784][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-06-02 15:58:08,895][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-06-02 15:58:08,902][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-06-02 15:58:08,977][__main__][INFO] - Instantiating callbacks...
[2024-06-02 15:58:08,977][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-02 15:58:08,981][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-06-02 15:58:08,983][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-02 15:58:08,984][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-02 15:58:08,987][__main__][INFO] - Instantiating loggers...
[2024-06-02 15:58:08,987][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-06-02 15:58:47,348][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-06-02 15:58:48,793][__main__][INFO] - Logging hyperparameters!
[2024-06-02 15:58:48,800][__main__][INFO] - Starting training!
[2024-06-02 16:04:14,832][__main__][INFO] - Starting testing!
[2024-06-02 16:04:35,384][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-06-02 16:04:35,413][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-06-02 16:04:35,539][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-06-02 16:04:35,548][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-06-02 16:04:35,703][__main__][INFO] - Instantiating callbacks...
[2024-06-02 16:04:35,704][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-02 16:04:35,712][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-06-02 16:04:35,717][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-02 16:04:35,718][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-02 16:04:35,721][__main__][INFO] - Instantiating loggers...
[2024-06-02 16:04:35,721][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-06-02 16:05:15,062][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-06-02 16:05:16,379][__main__][INFO] - Logging hyperparameters!
[2024-06-02 16:05:16,385][__main__][INFO] - Starting training!
[2024-06-02 16:14:08,100][__main__][INFO] - Starting testing!
[2024-06-02 16:14:21,909][__main__][INFO] - Best ckpt path: D:\data\CIFAR10_logs\logs\train\runs\2024-06-02_16-04-35\checkpoints\epoch_004.ckpt
[2024-06-02 16:14:21,912][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-06-02_16-04-35
[2024-06-02 16:14:21,912][src.utils.utils][INFO] - Closing wandb!
[2024-06-02 16:14:42,532][src.utils.utils][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-02 16:15:27,344][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-06-02 16:15:27,353][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-06-02 16:15:27,471][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-06-02 16:15:27,478][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-06-02 16:15:27,589][__main__][INFO] - Instantiating callbacks...
[2024-06-02 16:15:27,589][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-02 16:15:27,595][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-06-02 16:15:27,598][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-02 16:15:27,599][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-02 16:15:27,603][__main__][INFO] - Instantiating loggers...
[2024-06-02 16:15:27,603][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-06-02 16:16:05,790][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-06-02 16:16:07,329][__main__][INFO] - Logging hyperparameters!
[2024-06-02 16:16:07,335][__main__][INFO] - Starting training!
[2024-06-02 16:23:41,994][__main__][INFO] - Starting testing!
[2024-06-02 16:23:54,788][__main__][INFO] - Best ckpt path: D:\data\CIFAR10_logs\logs\train\runs\2024-06-02_16-15-27\checkpoints\epoch_004.ckpt
[2024-06-02 16:23:54,791][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-06-02_16-15-27
[2024-06-02 16:23:54,791][src.utils.utils][INFO] - Closing wandb!
[2024-06-02 16:24:08,056][src.utils.utils][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-09-09 17:21:28,199][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-09-09 17:21:28,207][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-09-09 17:21:28,289][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-09-09 17:21:28,296][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-09-09 17:21:28,397][__main__][INFO] - Instantiating callbacks...
[2024-09-09 17:21:28,398][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-09-09 17:21:28,402][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-09-09 17:21:28,403][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-09-09 17:21:28,404][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-09-09 17:21:28,407][__main__][INFO] - Instantiating loggers...
[2024-09-09 17:21:28,407][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-09-09 17:21:52,538][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-09-09 17:21:54,301][__main__][INFO] - Logging hyperparameters!
[2024-09-09 17:21:54,312][__main__][INFO] - Starting training!
[2024-09-09 17:22:57,783][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-09-09 17:22:57,790][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-09-09 17:22:57,885][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-09-09 17:22:57,892][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-09-09 17:22:58,010][__main__][INFO] - Instantiating callbacks...
[2024-09-09 17:22:58,010][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-09-09 17:22:58,015][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-09-09 17:22:58,017][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-09-09 17:22:58,017][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-09-09 17:22:58,019][__main__][INFO] - Instantiating loggers...
[2024-09-09 17:22:58,020][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-09-09 17:23:20,504][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-09-09 17:23:22,208][__main__][INFO] - Logging hyperparameters!
[2024-09-09 17:23:22,215][__main__][INFO] - Starting training!
[2024-09-09 17:28:03,964][__main__][INFO] - Starting testing!
[2024-09-19 15:08:20,393][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-09-19 15:08:20,411][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-09-19 15:08:20,577][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-09-19 15:08:20,592][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-09-19 15:08:20,937][__main__][INFO] - Instantiating callbacks...
[2024-09-19 15:08:20,938][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-09-19 15:08:20,949][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-09-19 15:08:20,954][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-09-19 15:08:20,956][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-09-19 15:08:20,962][__main__][INFO] - Instantiating loggers...
[2024-09-19 15:08:20,962][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-09-19 15:08:46,592][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-09-19 15:08:48,915][__main__][INFO] - Logging hyperparameters!
[2024-09-19 15:08:48,920][__main__][INFO] - Starting training!
[2024-09-19 15:13:46,194][__main__][INFO] - Starting testing!
[2024-09-19 15:13:57,355][__main__][INFO] - Best ckpt path: D:\data\CIFAR10_logs\logs\train\runs\2024-09-19_15-08-20/checkpoints\epoch_000.ckpt
[2024-09-19 15:13:57,356][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-09-19_15-08-20
[2024-09-19 15:13:57,356][src.utils.utils][INFO] - Closing wandb!
[2024-09-19 15:16:02,301][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-09-19 15:16:02,330][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-09-19 15:16:02,472][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-09-19 15:16:02,485][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-09-19 15:16:02,599][__main__][INFO] - Instantiating callbacks...
[2024-09-19 15:16:02,600][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-09-19 15:16:02,605][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-09-19 15:16:02,606][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-09-19 15:16:02,607][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-09-19 15:16:02,609][__main__][INFO] - Instantiating loggers...
[2024-09-19 15:16:02,609][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-09-19 15:16:27,121][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-09-19 15:16:28,408][__main__][INFO] - Logging hyperparameters!
[2024-09-19 15:16:28,412][__main__][INFO] - Starting training!
[2024-09-19 15:20:54,471][__main__][INFO] - Starting testing!
[2024-09-19 15:21:14,823][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-09-19 15:21:14,831][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-09-19 15:21:14,944][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-09-19 15:21:14,950][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-09-19 15:21:15,083][__main__][INFO] - Instantiating callbacks...
[2024-09-19 15:21:15,084][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-09-19 15:21:15,091][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-09-19 15:21:15,093][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-09-19 15:21:15,094][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-09-19 15:21:15,096][__main__][INFO] - Instantiating loggers...
[2024-09-19 15:21:15,097][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-09-19 15:21:39,464][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-09-19 15:21:41,020][__main__][INFO] - Logging hyperparameters!
[2024-09-19 15:21:41,027][__main__][INFO] - Starting training!
[2024-09-19 15:30:12,612][__main__][INFO] - Starting testing!
[2024-09-19 15:45:11,560][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-09-19 15:45:11,567][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-09-19 15:45:11,676][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-09-19 15:45:11,683][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-09-19 15:45:11,802][__main__][INFO] - Instantiating callbacks...
[2024-09-19 15:45:11,803][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-09-19 15:45:11,809][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-09-19 15:45:11,811][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-09-19 15:45:11,812][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-09-19 15:45:11,814][__main__][INFO] - Instantiating loggers...
[2024-09-19 15:45:11,815][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-09-19 15:45:35,680][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-09-19 15:45:37,618][__main__][INFO] - Logging hyperparameters!
[2024-09-19 15:45:37,629][__main__][INFO] - Starting training!
[2024-09-19 15:45:43,434][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 949, in _run
    self.strategy.setup(self)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\single_device.py", line 75, in setup
    super().setup(trainer)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 149, in setup
    self.setup_optimizers(trainer)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 139, in setup_optimizers
    self.optimizers, self.lr_scheduler_configs = _init_optimizers_and_lr_schedulers(self.lightning_module)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\optimizer.py", line 168, in _init_optimizers_and_lr_schedulers
    optim_conf = call._call_lightning_module_hook(model.trainer, "configure_optimizers", pl_module=model)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 144, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 189, in configure_optimizers
    optimizer = self.hparams.optimizer(params=self.parameters())
  File "D:\pycharmproject\template\srcC\optim\Adam.py", line 13, in __init__
    self.distortion_matrix = torch.eye(params[0].numel()).to(params[0].device)  # 
TypeError: 'generator' object is not subscriptable
[2024-09-19 15:45:43,444][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-09-19_15-45-11
[2024-09-19 15:45:43,444][src.utils.utils][INFO] - Closing wandb!
[2024-09-19 15:47:00,202][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-09-19 15:47:00,208][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-09-19 15:47:00,284][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-09-19 15:47:00,289][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-09-19 15:47:00,384][__main__][INFO] - Instantiating callbacks...
[2024-09-19 15:47:00,385][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-09-19 15:47:00,388][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-09-19 15:47:00,390][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-09-19 15:47:00,390][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-09-19 15:47:00,392][__main__][INFO] - Instantiating loggers...
[2024-09-19 15:47:00,393][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-09-19 15:47:23,854][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-09-19 15:47:25,695][__main__][INFO] - Logging hyperparameters!
[2024-09-19 15:47:25,702][__main__][INFO] - Starting training!
[2024-09-19 15:48:34,378][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1016, in _run_stage
    self.fit_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 201, in run
    self.advance()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 354, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 133, in run
    self.advance(data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 218, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 185, in run
    self._optimizer_step(kwargs.get("batch_idx", 0), closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 260, in _optimizer_step
    call._call_lightning_module_hook(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 144, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\module.py", line 1256, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\optimizer.py", line 155, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 225, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\plugins\precision\precision_plugin.py", line 114, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\optim\optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\optim\Adam.py", line 36, in step
    self.update_distortion_matrix(grad)
  File "D:\pycharmproject\template\srcC\optim\Adam.py", line 23, in update_distortion_matrix
    self.distortion_matrix = torch.eye(grad.numel()).to(grad.device) + self.beta3 * (grad_mean / (grad_std + 1e-8))
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 5.06 GiB (GPU 0; 2.00 GiB total capacity; 5.10 GiB already allocated; 0 bytes free; 5.14 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[2024-09-19 15:48:34,430][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-09-19_15-47-00
[2024-09-19 15:48:34,434][src.utils.utils][INFO] - Closing wandb!
[2024-09-19 15:58:04,550][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-09-19 15:58:04,559][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-09-19 15:58:04,650][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-09-19 15:58:04,656][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-09-19 15:58:04,781][__main__][INFO] - Instantiating callbacks...
[2024-09-19 15:58:04,782][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-09-19 15:58:04,786][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-09-19 15:58:04,789][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-09-19 15:58:04,791][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-09-19 15:58:04,793][__main__][INFO] - Instantiating loggers...
[2024-09-19 15:58:04,793][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-09-19 15:58:28,236][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-09-19 15:58:29,525][__main__][INFO] - Logging hyperparameters!
[2024-09-19 15:58:29,529][__main__][INFO] - Starting training!
[2024-09-19 16:02:43,187][__main__][INFO] - Starting testing!
[2024-09-19 16:09:23,511][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-09-19 16:09:23,518][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-09-19 16:09:23,595][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-09-19 16:09:23,600][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-09-19 16:09:23,697][__main__][INFO] - Instantiating callbacks...
[2024-09-19 16:09:23,698][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-09-19 16:09:23,702][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-09-19 16:09:23,703][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-09-19 16:09:23,704][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-09-19 16:09:23,706][__main__][INFO] - Instantiating loggers...
[2024-09-19 16:09:23,707][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-09-19 16:09:47,518][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-09-19 16:09:48,931][__main__][INFO] - Logging hyperparameters!
[2024-09-19 16:09:48,937][__main__][INFO] - Starting training!
[2024-09-19 16:09:59,491][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1016, in _run_stage
    self.fit_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 201, in run
    self.advance()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 354, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 133, in run
    self.advance(data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 218, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 185, in run
    self._optimizer_step(kwargs.get("batch_idx", 0), closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 260, in _optimizer_step
    call._call_lightning_module_hook(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 144, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\module.py", line 1256, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\optimizer.py", line 155, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 225, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\plugins\precision\precision_plugin.py", line 114, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\optim\optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\optim\Adam.py", line 66, in step
    p.data.addcdiv_(-step_size, combined_momentum, denom).mm(hessian_inv)
RuntimeError: self must be a matrix
[2024-09-19 16:09:59,498][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-09-19_16-09-23
[2024-09-19 16:09:59,500][src.utils.utils][INFO] - Closing wandb!
[2024-09-19 16:11:41,233][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-09-19 16:11:41,240][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-09-19 16:11:41,321][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-09-19 16:11:41,326][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-09-19 16:11:41,424][__main__][INFO] - Instantiating callbacks...
[2024-09-19 16:11:41,424][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-09-19 16:11:41,428][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-09-19 16:11:41,429][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-09-19 16:11:41,430][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-09-19 16:11:41,432][__main__][INFO] - Instantiating loggers...
[2024-09-19 16:11:41,433][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-09-19 16:12:05,353][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-09-19 16:12:08,320][__main__][INFO] - Logging hyperparameters!
[2024-09-19 16:12:08,330][__main__][INFO] - Starting training!
[2024-09-19 16:12:20,844][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1016, in _run_stage
    self.fit_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 201, in run
    self.advance()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 354, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 133, in run
    self.advance(data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 218, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 185, in run
    self._optimizer_step(kwargs.get("batch_idx", 0), closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 260, in _optimizer_step
    call._call_lightning_module_hook(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 144, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\module.py", line 1256, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\optimizer.py", line 155, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 225, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\plugins\precision\precision_plugin.py", line 114, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\optim\optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\optim\Adam.py", line 70, in step
    p.data.addcmul_(-step_size, combined_momentum, hessian_inv)  # Hessian
RuntimeError: The size of tensor a (3) must match the size of tensor b (864) at non-singleton dimension 3
[2024-09-19 16:12:20,854][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-09-19_16-11-41
[2024-09-19 16:12:20,855][src.utils.utils][INFO] - Closing wandb!
[2024-09-19 16:16:00,418][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-09-19 16:16:00,425][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-09-19 16:16:00,513][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-09-19 16:16:00,518][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-09-19 16:16:00,619][__main__][INFO] - Instantiating callbacks...
[2024-09-19 16:16:00,621][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-09-19 16:16:00,625][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-09-19 16:16:00,626][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-09-19 16:16:00,627][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-09-19 16:16:00,629][__main__][INFO] - Instantiating loggers...
[2024-09-19 16:16:00,629][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-09-19 16:16:24,337][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-09-19 16:16:25,648][__main__][INFO] - Logging hyperparameters!
[2024-09-19 16:16:25,653][__main__][INFO] - Starting training!
[2024-09-19 16:20:57,111][__main__][INFO] - Starting testing!
[2024-11-25 17:57:15,122][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-11-25 17:57:15,131][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-11-25 17:57:15,234][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-11-25 17:57:15,242][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-11-25 17:57:15,603][__main__][INFO] - Instantiating callbacks...
[2024-11-25 17:57:15,603][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-11-25 17:57:15,608][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-11-25 17:57:15,609][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-11-25 17:57:15,610][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-11-25 17:57:15,614][__main__][INFO] - Instantiating loggers...
[2024-11-25 17:57:15,614][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-11-25 17:57:46,228][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-11-25 17:57:47,579][__main__][INFO] - Logging hyperparameters!
[2024-11-25 17:57:47,586][__main__][INFO] - Starting training!
[2024-11-25 18:04:08,927][__main__][INFO] - Starting testing!
[2024-12-01 14:46:58,885][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-12-01 14:46:58,893][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-12-01 14:46:58,967][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-12-01 14:46:58,974][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-12-01 14:46:59,253][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 92, in _call_target
    return _target_(*args, **kwargs)
TypeError: __init__() missing 7 required keyword-only arguments: 'image_size', 'patch_size', 'num_classes', 'dim', 'depth', 'heads', and 'mlp_dim'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 53, in train
    model: LightningModule = hydra.utils.instantiate(cfg.model)
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 226, in instantiate
    return instantiate_node(
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 342, in instantiate_node
    value = instantiate_node(
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 347, in instantiate_node
    return _call_target(_target_, partial, args, kwargs, full_key)
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 97, in _call_target
    raise InstantiationException(msg) from e
hydra.errors.InstantiationException: Error in call to target 'srcC.models.components.vit.ViT':
TypeError("__init__() missing 7 required keyword-only arguments: 'image_size', 'patch_size', 'num_classes', 'dim', 'depth', 'heads', and 'mlp_dim'")
full_key: model.net
[2024-12-01 14:46:59,258][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-12-01_14-46-58
[2024-12-01 14:48:50,735][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-12-01 14:48:50,744][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-12-01 14:48:50,819][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-12-01 14:48:50,825][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-12-01 14:48:51,069][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 92, in _call_target
    return _target_(*args, **kwargs)
TypeError: __init__() missing 7 required keyword-only arguments: 'image_size', 'patch_size', 'num_classes', 'dim', 'depth', 'heads', and 'mlp_dim'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 53, in train
    model: LightningModule = hydra.utils.instantiate(cfg.model)
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 226, in instantiate
    return instantiate_node(
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 342, in instantiate_node
    value = instantiate_node(
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 347, in instantiate_node
    return _call_target(_target_, partial, args, kwargs, full_key)
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 97, in _call_target
    raise InstantiationException(msg) from e
hydra.errors.InstantiationException: Error in call to target 'srcC.models.components.vit.ViT':
TypeError("__init__() missing 7 required keyword-only arguments: 'image_size', 'patch_size', 'num_classes', 'dim', 'depth', 'heads', and 'mlp_dim'")
full_key: model.net
[2024-12-01 14:48:51,072][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-12-01_14-48-50
[2024-12-01 15:01:52,768][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-12-01 15:01:52,775][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-12-01 15:01:52,857][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-12-01 15:01:52,864][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 92, in _call_target
    return _target_(*args, **kwargs)
TypeError: __init__() got an unexpected keyword argument 'patch'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 50, in train
    datamodule: LightningDataModule = hydra.utils.instantiate(cfg.data)
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 226, in instantiate
    return instantiate_node(
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 347, in instantiate_node
    return _call_target(_target_, partial, args, kwargs, full_key)
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 97, in _call_target
    raise InstantiationException(msg) from e
hydra.errors.InstantiationException: Error in call to target 'srcC.data.CIFAR10_datamodule.CIFAR10DataModule':
TypeError("__init__() got an unexpected keyword argument 'patch'")
full_key: data
[2024-12-01 15:01:52,867][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-12-01_15-01-52
[2024-12-01 15:03:24,095][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-12-01 15:03:24,105][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-12-01 15:03:24,191][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-12-01 15:03:24,199][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-12-01 15:03:24,446][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 92, in _call_target
    return _target_(*args, **kwargs)
TypeError: __init__() missing 7 required keyword-only arguments: 'image_size', 'patch_size', 'num_classes', 'dim', 'depth', 'heads', and 'mlp_dim'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 53, in train
    model: LightningModule = hydra.utils.instantiate(cfg.model)
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 226, in instantiate
    return instantiate_node(
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 342, in instantiate_node
    value = instantiate_node(
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 347, in instantiate_node
    return _call_target(_target_, partial, args, kwargs, full_key)
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 97, in _call_target
    raise InstantiationException(msg) from e
hydra.errors.InstantiationException: Error in call to target 'srcC.models.components.vit.ViT':
TypeError("__init__() missing 7 required keyword-only arguments: 'image_size', 'patch_size', 'num_classes', 'dim', 'depth', 'heads', and 'mlp_dim'")
full_key: model.net
[2024-12-01 15:03:24,448][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-12-01_15-03-23
[2024-12-01 15:03:58,373][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-12-01 15:03:58,380][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-12-01 15:03:58,453][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-12-01 15:03:58,460][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-12-01 15:03:58,684][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 92, in _call_target
    return _target_(*args, **kwargs)
TypeError: __init__() missing 7 required keyword-only arguments: 'image_size', 'patch_size', 'num_classes', 'dim', 'depth', 'heads', and 'mlp_dim'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 53, in train
    model: LightningModule = hydra.utils.instantiate(cfg.model)
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 226, in instantiate
    return instantiate_node(
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 342, in instantiate_node
    value = instantiate_node(
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 347, in instantiate_node
    return _call_target(_target_, partial, args, kwargs, full_key)
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 97, in _call_target
    raise InstantiationException(msg) from e
hydra.errors.InstantiationException: Error in call to target 'srcC.models.components.vit.ViT':
TypeError("__init__() missing 7 required keyword-only arguments: 'image_size', 'patch_size', 'num_classes', 'dim', 'depth', 'heads', and 'mlp_dim'")
full_key: model.net
[2024-12-01 15:03:58,688][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-12-01_15-03-58
[2024-12-01 15:06:35,930][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-12-01 15:06:35,937][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-12-01 15:06:36,011][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-12-01 15:06:36,019][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-12-01 15:06:36,255][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 92, in _call_target
    return _target_(*args, **kwargs)
TypeError: __init__() missing 6 required keyword-only arguments: 'patch_size', 'num_classes', 'dim', 'depth', 'heads', and 'mlp_dim'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 53, in train
    model: LightningModule = hydra.utils.instantiate(cfg.model)
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 226, in instantiate
    return instantiate_node(
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 342, in instantiate_node
    value = instantiate_node(
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 347, in instantiate_node
    return _call_target(_target_, partial, args, kwargs, full_key)
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 97, in _call_target
    raise InstantiationException(msg) from e
hydra.errors.InstantiationException: Error in call to target 'srcC.models.components.vit.ViT':
TypeError("__init__() missing 6 required keyword-only arguments: 'patch_size', 'num_classes', 'dim', 'depth', 'heads', and 'mlp_dim'")
full_key: model.net
[2024-12-01 15:06:36,258][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-12-01_15-06-35
[2024-12-01 15:11:28,602][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-12-01 15:11:28,610][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-12-01 15:11:28,701][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-12-01 15:11:28,707][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-12-01 15:11:29,090][__main__][INFO] - Instantiating callbacks...
[2024-12-01 15:11:29,091][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-12-01 15:11:29,096][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-12-01 15:11:29,097][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-12-01 15:11:29,098][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-12-01 15:11:29,101][__main__][INFO] - Instantiating loggers...
[2024-12-01 15:11:29,101][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-12-01 15:11:53,938][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-12-01 15:11:55,199][__main__][INFO] - Logging hyperparameters!
[2024-12-01 15:11:55,205][__main__][INFO] - Starting training!
[2024-12-01 15:15:43,708][__main__][INFO] - Starting testing!
[2024-12-01 15:15:43,710][__main__][WARNING] - Best ckpt not found! Using current weights for testing...
[2024-12-01 15:15:58,346][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-12-01 15:15:58,353][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-12-01 15:15:58,461][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-12-01 15:15:58,467][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-12-01 15:15:58,864][__main__][INFO] - Instantiating callbacks...
[2024-12-01 15:15:58,864][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-12-01 15:15:58,869][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-12-01 15:15:58,871][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-12-01 15:15:58,872][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-12-01 15:15:58,875][__main__][INFO] - Instantiating loggers...
[2024-12-01 15:15:58,875][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-12-01 15:16:22,818][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-12-01 15:16:24,234][__main__][INFO] - Logging hyperparameters!
[2024-12-01 15:16:24,239][__main__][INFO] - Starting training!
[2024-12-01 15:16:33,823][__main__][INFO] - Starting testing!
[2024-12-01 15:16:33,823][__main__][WARNING] - Best ckpt not found! Using current weights for testing...
[2024-12-04 14:00:59,263][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-12-04 14:00:59,272][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-12-04 14:00:59,362][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-12-04 14:00:59,368][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-12-04 14:00:59,662][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 92, in _call_target
    return _target_(*args, **kwargs)
TypeError: __init__() got an unexpected keyword argument 'depth'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 53, in train
    model: LightningModule = hydra.utils.instantiate(cfg.model)
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 226, in instantiate
    return instantiate_node(
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 342, in instantiate_node
    value = instantiate_node(
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 347, in instantiate_node
    return _call_target(_target_, partial, args, kwargs, full_key)
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 97, in _call_target
    raise InstantiationException(msg) from e
hydra.errors.InstantiationException: Error in call to target 'srcC.models.components.v.ViT':
TypeError("__init__() got an unexpected keyword argument 'depth'")
full_key: model.net
[2024-12-04 14:00:59,667][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-12-04_14-00-59
[2024-12-04 14:02:28,949][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-12-04 14:02:28,956][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-12-04 14:02:29,037][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-12-04 14:02:29,042][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-12-04 14:02:29,287][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 92, in _call_target
    return _target_(*args, **kwargs)
TypeError: __init__() got an unexpected keyword argument 'windows_size'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 53, in train
    model: LightningModule = hydra.utils.instantiate(cfg.model)
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 226, in instantiate
    return instantiate_node(
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 342, in instantiate_node
    value = instantiate_node(
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 347, in instantiate_node
    return _call_target(_target_, partial, args, kwargs, full_key)
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 97, in _call_target
    raise InstantiationException(msg) from e
hydra.errors.InstantiationException: Error in call to target 'srcC.models.components.v.ViT':
TypeError("__init__() got an unexpected keyword argument 'windows_size'")
full_key: model.net
[2024-12-04 14:02:29,289][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-12-04_14-02-28
[2024-12-04 14:03:43,446][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-12-04 14:03:43,454][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-12-04 14:03:43,546][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-12-04 14:03:43,552][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-12-04 14:03:43,783][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 92, in _call_target
    return _target_(*args, **kwargs)
TypeError: __init__() got an unexpected keyword argument 'windows_size'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 53, in train
    model: LightningModule = hydra.utils.instantiate(cfg.model)
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 226, in instantiate
    return instantiate_node(
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 342, in instantiate_node
    value = instantiate_node(
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 347, in instantiate_node
    return _call_target(_target_, partial, args, kwargs, full_key)
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 97, in _call_target
    raise InstantiationException(msg) from e
hydra.errors.InstantiationException: Error in call to target 'srcC.models.components.v.ViT':
TypeError("__init__() got an unexpected keyword argument 'windows_size'")
full_key: model.net
[2024-12-04 14:03:43,785][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-12-04_14-03-43
[2024-12-04 14:04:17,079][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-12-04 14:04:17,086][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-12-04 14:04:17,176][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-12-04 14:04:17,181][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-12-04 14:04:17,414][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 92, in _call_target
    return _target_(*args, **kwargs)
TypeError: __init__() missing 1 required keyword-only argument: 'dims'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 53, in train
    model: LightningModule = hydra.utils.instantiate(cfg.model)
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 226, in instantiate
    return instantiate_node(
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 342, in instantiate_node
    value = instantiate_node(
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 347, in instantiate_node
    return _call_target(_target_, partial, args, kwargs, full_key)
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 97, in _call_target
    raise InstantiationException(msg) from e
hydra.errors.InstantiationException: Error in call to target 'srcC.models.components.v.ViT':
TypeError("__init__() missing 1 required keyword-only argument: 'dims'")
full_key: model.net
[2024-12-04 14:04:17,417][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-12-04_14-04-16
[2024-12-04 14:04:43,706][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-12-04 14:04:43,713][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-12-04 14:04:43,812][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-12-04 14:04:43,817][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-12-04 14:04:44,216][__main__][INFO] - Instantiating callbacks...
[2024-12-04 14:04:44,216][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-12-04 14:04:44,220][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-12-04 14:04:44,221][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-12-04 14:04:44,222][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-12-04 14:04:44,224][__main__][INFO] - Instantiating loggers...
[2024-12-04 14:04:44,224][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-12-04 14:05:08,473][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-12-04 14:05:09,717][__main__][INFO] - Logging hyperparameters!
[2024-12-04 14:05:09,723][__main__][INFO] - Starting training!
[2024-12-04 14:05:19,668][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1014, in _run_stage
    self._run_sanity_check()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1043, in _run_sanity_check
    val_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\utilities.py", line 177, in _decorator
    return loop_run(self, *args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\evaluation_loop.py", line 115, in run
    self._evaluation_step(batch, batch_idx, dataloader_idx)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\evaluation_loop.py", line 375, in _evaluation_step
    output = call._call_strategy_hook(trainer, hook_name, *step_kwargs.values())
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 291, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 379, in validation_step
    return self.model.validation_step(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 144, in validation_step
    loss, preds, targets = self.model_step(batch)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 108, in model_step
    logits = self.forward(x)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 85, in forward
    return self.net(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 177, in forward
    x = self.transformer(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 132, in forward
    x = block(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 103, in forward
    x = self.attn(x) + x
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 18, in forward
    return self.fn(self.norm(x), **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\normalization.py", line 190, in forward
    return F.layer_norm(
  File "D:\Anaconda3\lib\site-packages\torch\nn\functional.py", line 2515, in layer_norm
    return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)
RuntimeError: Given normalized_shape=[64], expected input with shape [*, 64], but got input of size[32, 65, 512]
[2024-12-04 14:05:19,677][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-12-04_14-04-43
[2024-12-04 14:05:19,687][src.utils.utils][INFO] - Closing wandb!
[2024-12-04 14:06:44,538][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-12-04 14:06:44,549][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-12-04 14:06:44,664][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-12-04 14:06:44,669][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-12-04 14:06:44,678][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\utils.py", line 644, in _locate
    obj = getattr(obj, part)
AttributeError: module 'srcC.models.components' has no attribute 'v'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\utils.py", line 650, in _locate
    obj = import_module(mod)
  File "D:\Anaconda3\lib\importlib\__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1030, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1007, in _find_and_load
  File "<frozen importlib._bootstrap>", line 986, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 680, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 846, in exec_module
  File "<frozen importlib._bootstrap_external>", line 983, in get_code
  File "<frozen importlib._bootstrap_external>", line 913, in source_to_code
  File "<frozen importlib._bootstrap>", line 228, in _call_with_frames_removed
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 63
    windows = rearrange(x, 'b (h s1) (w s2) d -> b (h w) s1 s2 d', s1=self.window_size, s2=self.window_size)
    ^
SyntaxError: invalid syntax

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 134, in _resolve_target
    target = _locate(target)
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\utils.py", line 658, in _locate
    raise ImportError(
ImportError: Error loading 'srcC.models.components.v.ViT':
SyntaxError('invalid syntax', ('D:\\pycharmproject\\template\\srcC\\models\\components\\v.py', 63, 9, "        windows = rearrange(x, 'b (h s1) (w s2) d -> b (h w) s1 s2 d', s1=self.window_size, s2=self.window_size)\n"))

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 53, in train
    model: LightningModule = hydra.utils.instantiate(cfg.model)
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 226, in instantiate
    return instantiate_node(
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 342, in instantiate_node
    value = instantiate_node(
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 333, in instantiate_node
    _target_ = _resolve_target(node.get(_Keys.TARGET), full_key)
  File "D:\Anaconda3\lib\site-packages\hydra\_internal\instantiate\_instantiate2.py", line 139, in _resolve_target
    raise InstantiationException(msg) from e
hydra.errors.InstantiationException: Error locating target 'srcC.models.components.v.ViT', set env var HYDRA_FULL_ERROR=1 to see chained exception.
full_key: model.net
[2024-12-04 14:06:44,682][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-12-04_14-06-44
[2024-12-04 14:07:25,615][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-12-04 14:07:25,623][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-12-04 14:07:25,720][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-12-04 14:07:25,726][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-12-04 14:07:26,100][__main__][INFO] - Instantiating callbacks...
[2024-12-04 14:07:26,100][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-12-04 14:07:26,104][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-12-04 14:07:26,106][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-12-04 14:07:26,106][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-12-04 14:07:26,109][__main__][INFO] - Instantiating loggers...
[2024-12-04 14:07:26,109][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-12-04 14:07:50,108][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-12-04 14:07:51,319][__main__][INFO] - Logging hyperparameters!
[2024-12-04 14:07:51,326][__main__][INFO] - Starting training!
[2024-12-04 14:07:59,827][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1014, in _run_stage
    self._run_sanity_check()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1043, in _run_sanity_check
    val_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\utilities.py", line 177, in _decorator
    return loop_run(self, *args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\evaluation_loop.py", line 115, in run
    self._evaluation_step(batch, batch_idx, dataloader_idx)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\evaluation_loop.py", line 375, in _evaluation_step
    output = call._call_strategy_hook(trainer, hook_name, *step_kwargs.values())
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 291, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 379, in validation_step
    return self.model.validation_step(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 144, in validation_step
    loss, preds, targets = self.model_step(batch)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 108, in model_step
    logits = self.forward(x)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 85, in forward
    return self.net(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 177, in forward
    x = self.transformer(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 132, in forward
    x = block(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 103, in forward
    x = self.attn(x) + x
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 18, in forward
    return self.fn(self.norm(x), **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\normalization.py", line 190, in forward
    return F.layer_norm(
  File "D:\Anaconda3\lib\site-packages\torch\nn\functional.py", line 2515, in layer_norm
    return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)
RuntimeError: Given normalized_shape=[64], expected input with shape [*, 64], but got input of size[32, 65, 512]
[2024-12-04 14:07:59,836][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-12-04_14-07-25
[2024-12-04 14:07:59,847][src.utils.utils][INFO] - Closing wandb!
[2024-12-04 14:11:17,639][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-12-04 14:11:17,645][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-12-04 14:11:17,720][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-12-04 14:11:17,725][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-12-04 14:11:18,086][__main__][INFO] - Instantiating callbacks...
[2024-12-04 14:11:18,087][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-12-04 14:11:18,091][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-12-04 14:11:18,093][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-12-04 14:11:18,093][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-12-04 14:11:18,095][__main__][INFO] - Instantiating loggers...
[2024-12-04 14:11:18,096][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-12-04 14:11:42,276][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-12-04 14:11:43,506][__main__][INFO] - Logging hyperparameters!
[2024-12-04 14:11:43,511][__main__][INFO] - Starting training!
[2024-12-04 14:11:50,918][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1014, in _run_stage
    self._run_sanity_check()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1043, in _run_sanity_check
    val_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\utilities.py", line 177, in _decorator
    return loop_run(self, *args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\evaluation_loop.py", line 115, in run
    self._evaluation_step(batch, batch_idx, dataloader_idx)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\evaluation_loop.py", line 375, in _evaluation_step
    output = call._call_strategy_hook(trainer, hook_name, *step_kwargs.values())
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 291, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 379, in validation_step
    return self.model.validation_step(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 144, in validation_step
    loss, preds, targets = self.model_step(batch)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 108, in model_step
    logits = self.forward(x)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 85, in forward
    return self.net(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 177, in forward
    x = self.transformer(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 132, in forward
    x = block(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 103, in forward
    x = self.attn(x) + x
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 18, in forward
    return self.fn(self.norm(x), **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\normalization.py", line 190, in forward
    return F.layer_norm(
  File "D:\Anaconda3\lib\site-packages\torch\nn\functional.py", line 2515, in layer_norm
    return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)
RuntimeError: Given normalized_shape=[32], expected input with shape [*, 32], but got input of size[32, 65, 512]
[2024-12-04 14:11:50,926][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-12-04_14-11-17
[2024-12-04 14:11:50,939][src.utils.utils][INFO] - Closing wandb!
[2024-12-04 14:12:29,076][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-12-04 14:12:29,083][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-12-04 14:12:29,161][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-12-04 14:12:29,166][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-12-04 14:12:29,515][__main__][INFO] - Instantiating callbacks...
[2024-12-04 14:12:29,515][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-12-04 14:12:29,519][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-12-04 14:12:29,520][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-12-04 14:12:29,521][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-12-04 14:12:29,522][__main__][INFO] - Instantiating loggers...
[2024-12-04 14:12:29,522][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-12-04 14:12:54,112][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-12-04 14:12:55,368][__main__][INFO] - Logging hyperparameters!
[2024-12-04 14:12:55,375][__main__][INFO] - Starting training!
[2024-12-04 14:13:02,345][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 523, in reduce
    return _apply_recipe(
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 234, in _apply_recipe
    init_shapes, axes_reordering, reduced_axes, added_axes, final_shapes, n_axes_w_added = _reconstruct_from_shape(
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 187, in _reconstruct_from_shape_uncached
    raise EinopsError(f"Shape mismatch, can't divide axis of length {length} in chunks of {known_product}")
einops.EinopsError: Shape mismatch, can't divide axis of length 65 in chunks of 8

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1014, in _run_stage
    self._run_sanity_check()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1043, in _run_sanity_check
    val_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\utilities.py", line 177, in _decorator
    return loop_run(self, *args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\evaluation_loop.py", line 115, in run
    self._evaluation_step(batch, batch_idx, dataloader_idx)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\evaluation_loop.py", line 375, in _evaluation_step
    output = call._call_strategy_hook(trainer, hook_name, *step_kwargs.values())
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 291, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 379, in validation_step
    return self.model.validation_step(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 144, in validation_step
    loss, preds, targets = self.model_step(batch)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 108, in model_step
    logits = self.forward(x)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 85, in forward
    return self.net(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 177, in forward
    x = self.transformer(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 132, in forward
    x = block(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 103, in forward
    x = self.attn(x) + x
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 18, in forward
    return self.fn(self.norm(x), **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 62, in forward
    x = rearrange(x, 'b (h w) d -> b h w d', h=int(n ** 0.5))
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 591, in rearrange
    return reduce(tensor, pattern, reduction="rearrange", **axes_lengths)
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 533, in reduce
    raise EinopsError(message + "\n {}".format(e))
einops.EinopsError:  Error while processing rearrange-reduction pattern "b (h w) d -> b h w d".
 Input tensor shape: torch.Size([32, 65, 32]). Additional info: {'h': 8}.
 Shape mismatch, can't divide axis of length 65 in chunks of 8
[2024-12-04 14:13:02,359][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-12-04_14-12-28
[2024-12-04 14:13:02,359][src.utils.utils][INFO] - Closing wandb!
[2024-12-04 14:14:23,257][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-12-04 14:14:23,263][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-12-04 14:14:23,355][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-12-04 14:14:23,360][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-12-04 14:14:23,712][__main__][INFO] - Instantiating callbacks...
[2024-12-04 14:14:23,713][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-12-04 14:14:23,716][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-12-04 14:14:23,718][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-12-04 14:14:23,718][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-12-04 14:14:23,720][__main__][INFO] - Instantiating loggers...
[2024-12-04 14:14:23,721][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-12-04 14:14:49,285][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-12-04 14:14:50,690][__main__][INFO] - Logging hyperparameters!
[2024-12-04 14:14:50,696][__main__][INFO] - Starting training!
[2024-12-04 14:14:57,994][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 523, in reduce
    return _apply_recipe(
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 234, in _apply_recipe
    init_shapes, axes_reordering, reduced_axes, added_axes, final_shapes, n_axes_w_added = _reconstruct_from_shape(
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 187, in _reconstruct_from_shape_uncached
    raise EinopsError(f"Shape mismatch, can't divide axis of length {length} in chunks of {known_product}")
einops.EinopsError: Shape mismatch, can't divide axis of length 65 in chunks of 2

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1014, in _run_stage
    self._run_sanity_check()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1043, in _run_sanity_check
    val_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\utilities.py", line 177, in _decorator
    return loop_run(self, *args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\evaluation_loop.py", line 115, in run
    self._evaluation_step(batch, batch_idx, dataloader_idx)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\evaluation_loop.py", line 375, in _evaluation_step
    output = call._call_strategy_hook(trainer, hook_name, *step_kwargs.values())
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 291, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 379, in validation_step
    return self.model.validation_step(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 144, in validation_step
    loss, preds, targets = self.model_step(batch)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 108, in model_step
    logits = self.forward(x)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 85, in forward
    return self.net(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 177, in forward
    x = self.transformer(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 132, in forward
    x = block(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 103, in forward
    x = self.attn(x) + x
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 18, in forward
    return self.fn(self.norm(x), **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 63, in forward
    windows = rearrange(x, 'b (h s1) (w s2) d -> b (h w) s1 s2 d', s1=self.window_size, s2=self.window_size)
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 591, in rearrange
    return reduce(tensor, pattern, reduction="rearrange", **axes_lengths)
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 533, in reduce
    raise EinopsError(message + "\n {}".format(e))
einops.EinopsError:  Error while processing rearrange-reduction pattern "b (h s1) (w s2) d -> b (h w) s1 s2 d".
 Input tensor shape: torch.Size([32, 65, 1, 32]). Additional info: {'s1': 2, 's2': 2}.
 Shape mismatch, can't divide axis of length 65 in chunks of 2
[2024-12-04 14:14:58,017][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-12-04_14-14-23
[2024-12-04 14:14:58,018][src.utils.utils][INFO] - Closing wandb!
[2024-12-04 14:15:57,758][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-12-04 14:15:57,767][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-12-04 14:15:57,880][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-12-04 14:15:57,890][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-12-04 14:15:58,364][__main__][INFO] - Instantiating callbacks...
[2024-12-04 14:15:58,365][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-12-04 14:15:58,369][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-12-04 14:15:58,370][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-12-04 14:15:58,371][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-12-04 14:15:58,373][__main__][INFO] - Instantiating loggers...
[2024-12-04 14:15:58,373][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-12-04 14:16:22,886][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-12-04 14:16:24,069][__main__][INFO] - Logging hyperparameters!
[2024-12-04 14:16:24,074][__main__][INFO] - Starting training!
[2024-12-04 14:16:30,474][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 523, in reduce
    return _apply_recipe(
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 234, in _apply_recipe
    init_shapes, axes_reordering, reduced_axes, added_axes, final_shapes, n_axes_w_added = _reconstruct_from_shape(
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 187, in _reconstruct_from_shape_uncached
    raise EinopsError(f"Shape mismatch, can't divide axis of length {length} in chunks of {known_product}")
einops.EinopsError: Shape mismatch, can't divide axis of length 65 in chunks of 64

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1014, in _run_stage
    self._run_sanity_check()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1043, in _run_sanity_check
    val_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\utilities.py", line 177, in _decorator
    return loop_run(self, *args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\evaluation_loop.py", line 115, in run
    self._evaluation_step(batch, batch_idx, dataloader_idx)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\evaluation_loop.py", line 375, in _evaluation_step
    output = call._call_strategy_hook(trainer, hook_name, *step_kwargs.values())
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 291, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 379, in validation_step
    return self.model.validation_step(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 144, in validation_step
    loss, preds, targets = self.model_step(batch)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 108, in model_step
    logits = self.forward(x)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 85, in forward
    return self.net(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 177, in forward
    x = self.transformer(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 132, in forward
    x = block(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 103, in forward
    x = self.attn(x) + x
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 18, in forward
    return self.fn(self.norm(x), **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 62, in forward
    x = rearrange(x, 'b (h w) d -> b h w d', h=64)
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 591, in rearrange
    return reduce(tensor, pattern, reduction="rearrange", **axes_lengths)
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 533, in reduce
    raise EinopsError(message + "\n {}".format(e))
einops.EinopsError:  Error while processing rearrange-reduction pattern "b (h w) d -> b h w d".
 Input tensor shape: torch.Size([32, 65, 32]). Additional info: {'h': 64}.
 Shape mismatch, can't divide axis of length 65 in chunks of 64
[2024-12-04 14:16:30,491][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-12-04_14-15-57
[2024-12-04 14:16:30,492][src.utils.utils][INFO] - Closing wandb!
[2024-12-04 14:16:57,032][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-12-04 14:16:57,038][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-12-04 14:16:57,140][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-12-04 14:16:57,145][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-12-04 14:16:57,487][__main__][INFO] - Instantiating callbacks...
[2024-12-04 14:16:57,487][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-12-04 14:16:57,491][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-12-04 14:16:57,493][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-12-04 14:16:57,493][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-12-04 14:16:57,495][__main__][INFO] - Instantiating loggers...
[2024-12-04 14:16:57,496][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-12-04 14:17:21,405][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-12-04 14:17:22,606][__main__][INFO] - Logging hyperparameters!
[2024-12-04 14:17:22,613][__main__][INFO] - Starting training!
[2024-12-04 14:17:29,944][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 523, in reduce
    return _apply_recipe(
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 234, in _apply_recipe
    init_shapes, axes_reordering, reduced_axes, added_axes, final_shapes, n_axes_w_added = _reconstruct_from_shape(
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 187, in _reconstruct_from_shape_uncached
    raise EinopsError(f"Shape mismatch, can't divide axis of length {length} in chunks of {known_product}")
einops.EinopsError: Shape mismatch, can't divide axis of length 65 in chunks of 3

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1014, in _run_stage
    self._run_sanity_check()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1043, in _run_sanity_check
    val_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\utilities.py", line 177, in _decorator
    return loop_run(self, *args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\evaluation_loop.py", line 115, in run
    self._evaluation_step(batch, batch_idx, dataloader_idx)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\evaluation_loop.py", line 375, in _evaluation_step
    output = call._call_strategy_hook(trainer, hook_name, *step_kwargs.values())
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 291, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 379, in validation_step
    return self.model.validation_step(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 144, in validation_step
    loss, preds, targets = self.model_step(batch)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 108, in model_step
    logits = self.forward(x)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 85, in forward
    return self.net(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 177, in forward
    x = self.transformer(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 132, in forward
    x = block(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 103, in forward
    x = self.attn(x) + x
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 18, in forward
    return self.fn(self.norm(x), **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 63, in forward
    windows = rearrange(x, 'b (h s1) (w s2) d -> b (h w) s1 s2 d', s1=self.window_size, s2=self.window_size)
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 591, in rearrange
    return reduce(tensor, pattern, reduction="rearrange", **axes_lengths)
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 533, in reduce
    raise EinopsError(message + "\n {}".format(e))
einops.EinopsError:  Error while processing rearrange-reduction pattern "b (h s1) (w s2) d -> b (h w) s1 s2 d".
 Input tensor shape: torch.Size([32, 65, 1, 32]). Additional info: {'s1': 3, 's2': 3}.
 Shape mismatch, can't divide axis of length 65 in chunks of 3
[2024-12-04 14:17:29,953][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-12-04_14-16-56
[2024-12-04 14:17:29,954][src.utils.utils][INFO] - Closing wandb!
[2024-12-04 14:18:11,641][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-12-04 14:18:11,649][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-12-04 14:18:11,751][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-12-04 14:18:11,757][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-12-04 14:18:12,111][__main__][INFO] - Instantiating callbacks...
[2024-12-04 14:18:12,111][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-12-04 14:18:12,114][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-12-04 14:18:12,116][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-12-04 14:18:12,117][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-12-04 14:18:12,120][__main__][INFO] - Instantiating loggers...
[2024-12-04 14:18:12,120][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-12-04 14:18:36,045][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-12-04 14:18:37,252][__main__][INFO] - Logging hyperparameters!
[2024-12-04 14:18:37,260][__main__][INFO] - Starting training!
[2024-12-04 14:18:43,849][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 523, in reduce
    return _apply_recipe(
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 234, in _apply_recipe
    init_shapes, axes_reordering, reduced_axes, added_axes, final_shapes, n_axes_w_added = _reconstruct_from_shape(
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 187, in _reconstruct_from_shape_uncached
    raise EinopsError(f"Shape mismatch, can't divide axis of length {length} in chunks of {known_product}")
einops.EinopsError: Shape mismatch, can't divide axis of length 17 in chunks of 65

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1014, in _run_stage
    self._run_sanity_check()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1043, in _run_sanity_check
    val_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\utilities.py", line 177, in _decorator
    return loop_run(self, *args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\evaluation_loop.py", line 115, in run
    self._evaluation_step(batch, batch_idx, dataloader_idx)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\evaluation_loop.py", line 375, in _evaluation_step
    output = call._call_strategy_hook(trainer, hook_name, *step_kwargs.values())
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 291, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 379, in validation_step
    return self.model.validation_step(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 144, in validation_step
    loss, preds, targets = self.model_step(batch)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 108, in model_step
    logits = self.forward(x)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 85, in forward
    return self.net(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 177, in forward
    x = self.transformer(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 132, in forward
    x = block(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 103, in forward
    x = self.attn(x) + x
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 18, in forward
    return self.fn(self.norm(x), **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 62, in forward
    x = rearrange(x, 'b (h w) d -> b h w d', h=65)
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 591, in rearrange
    return reduce(tensor, pattern, reduction="rearrange", **axes_lengths)
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 533, in reduce
    raise EinopsError(message + "\n {}".format(e))
einops.EinopsError:  Error while processing rearrange-reduction pattern "b (h w) d -> b h w d".
 Input tensor shape: torch.Size([32, 17, 32]). Additional info: {'h': 65}.
 Shape mismatch, can't divide axis of length 17 in chunks of 65
[2024-12-04 14:18:43,859][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-12-04_14-18-11
[2024-12-04 14:18:43,860][src.utils.utils][INFO] - Closing wandb!
[2024-12-04 14:19:46,137][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-12-04 14:19:46,145][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-12-04 14:19:46,248][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-12-04 14:19:46,253][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-12-04 14:19:46,599][__main__][INFO] - Instantiating callbacks...
[2024-12-04 14:19:46,600][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-12-04 14:19:46,603][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-12-04 14:19:46,604][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-12-04 14:19:46,605][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-12-04 14:19:46,607][__main__][INFO] - Instantiating loggers...
[2024-12-04 14:19:46,607][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-12-04 14:20:10,514][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-12-04 14:20:11,825][__main__][INFO] - Logging hyperparameters!
[2024-12-04 14:20:11,837][__main__][INFO] - Starting training!
[2024-12-04 14:20:19,376][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1014, in _run_stage
    self._run_sanity_check()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1043, in _run_sanity_check
    val_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\utilities.py", line 177, in _decorator
    return loop_run(self, *args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\evaluation_loop.py", line 115, in run
    self._evaluation_step(batch, batch_idx, dataloader_idx)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\evaluation_loop.py", line 375, in _evaluation_step
    output = call._call_strategy_hook(trainer, hook_name, *step_kwargs.values())
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 291, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 379, in validation_step
    return self.model.validation_step(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 144, in validation_step
    loss, preds, targets = self.model_step(batch)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 108, in model_step
    logits = self.forward(x)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 85, in forward
    return self.net(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 177, in forward
    x = self.transformer(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 132, in forward
    x = block(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 103, in forward
    x = self.attn(x) + x
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 18, in forward
    return self.fn(self.norm(x), **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 62, in forward
    x = rearrange(x, 'b (h w) d -> b h w d', h=n**0.5)
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 591, in rearrange
    return reduce(tensor, pattern, reduction="rearrange", **axes_lengths)
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 523, in reduce
    return _apply_recipe(
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 242, in _apply_recipe
    tensor = backend.reshape(tensor, init_shapes)
  File "D:\Anaconda3\lib\site-packages\einops\_backends.py", line 92, in reshape
    return x.reshape(shape)
TypeError: reshape(): argument 'shape' must be tuple of ints, but found element of type float at pos 2
[2024-12-04 14:20:19,391][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-12-04_14-19-46
[2024-12-04 14:20:19,393][src.utils.utils][INFO] - Closing wandb!
[2024-12-04 14:20:47,122][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-12-04 14:20:47,130][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-12-04 14:20:47,223][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-12-04 14:20:47,228][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-12-04 14:20:47,582][__main__][INFO] - Instantiating callbacks...
[2024-12-04 14:20:47,583][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-12-04 14:20:47,586][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-12-04 14:20:47,587][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-12-04 14:20:47,588][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-12-04 14:20:47,590][__main__][INFO] - Instantiating loggers...
[2024-12-04 14:20:47,590][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-12-04 14:21:11,546][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-12-04 14:21:12,770][__main__][INFO] - Logging hyperparameters!
[2024-12-04 14:21:12,777][__main__][INFO] - Starting training!
[2024-12-04 14:21:19,202][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 523, in reduce
    return _apply_recipe(
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 234, in _apply_recipe
    init_shapes, axes_reordering, reduced_axes, added_axes, final_shapes, n_axes_w_added = _reconstruct_from_shape(
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 187, in _reconstruct_from_shape_uncached
    raise EinopsError(f"Shape mismatch, can't divide axis of length {length} in chunks of {known_product}")
einops.EinopsError: Shape mismatch, can't divide axis of length 17 in chunks of 4

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1014, in _run_stage
    self._run_sanity_check()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1043, in _run_sanity_check
    val_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\utilities.py", line 177, in _decorator
    return loop_run(self, *args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\evaluation_loop.py", line 115, in run
    self._evaluation_step(batch, batch_idx, dataloader_idx)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\evaluation_loop.py", line 375, in _evaluation_step
    output = call._call_strategy_hook(trainer, hook_name, *step_kwargs.values())
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 291, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 379, in validation_step
    return self.model.validation_step(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 144, in validation_step
    loss, preds, targets = self.model_step(batch)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 108, in model_step
    logits = self.forward(x)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 85, in forward
    return self.net(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 177, in forward
    x = self.transformer(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 132, in forward
    x = block(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 103, in forward
    x = self.attn(x) + x
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 18, in forward
    return self.fn(self.norm(x), **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 62, in forward
    x = rearrange(x, 'b (h w) d -> b h w d', h=int(n**0.5))
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 591, in rearrange
    return reduce(tensor, pattern, reduction="rearrange", **axes_lengths)
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 533, in reduce
    raise EinopsError(message + "\n {}".format(e))
einops.EinopsError:  Error while processing rearrange-reduction pattern "b (h w) d -> b h w d".
 Input tensor shape: torch.Size([32, 17, 32]). Additional info: {'h': 4}.
 Shape mismatch, can't divide axis of length 17 in chunks of 4
[2024-12-04 14:21:19,216][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-12-04_14-20-47
[2024-12-04 14:21:19,217][src.utils.utils][INFO] - Closing wandb!
[2024-12-04 14:22:06,585][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-12-04 14:22:06,592][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-12-04 14:22:06,690][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-12-04 14:22:06,695][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-12-04 14:22:07,048][__main__][INFO] - Instantiating callbacks...
[2024-12-04 14:22:07,050][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-12-04 14:22:07,053][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-12-04 14:22:07,054][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-12-04 14:22:07,055][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-12-04 14:22:07,057][__main__][INFO] - Instantiating loggers...
[2024-12-04 14:22:07,058][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-12-04 14:22:30,851][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-12-04 14:22:32,085][__main__][INFO] - Logging hyperparameters!
[2024-12-04 14:22:32,092][__main__][INFO] - Starting training!
[2024-12-04 14:22:39,038][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 523, in reduce
    return _apply_recipe(
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 234, in _apply_recipe
    init_shapes, axes_reordering, reduced_axes, added_axes, final_shapes, n_axes_w_added = _reconstruct_from_shape(
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 187, in _reconstruct_from_shape_uncached
    raise EinopsError(f"Shape mismatch, can't divide axis of length {length} in chunks of {known_product}")
einops.EinopsError: Shape mismatch, can't divide axis of length 5 in chunks of 2

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1014, in _run_stage
    self._run_sanity_check()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1043, in _run_sanity_check
    val_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\utilities.py", line 177, in _decorator
    return loop_run(self, *args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\evaluation_loop.py", line 115, in run
    self._evaluation_step(batch, batch_idx, dataloader_idx)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\evaluation_loop.py", line 375, in _evaluation_step
    output = call._call_strategy_hook(trainer, hook_name, *step_kwargs.values())
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 291, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 379, in validation_step
    return self.model.validation_step(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 144, in validation_step
    loss, preds, targets = self.model_step(batch)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 108, in model_step
    logits = self.forward(x)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 85, in forward
    return self.net(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 177, in forward
    x = self.transformer(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 132, in forward
    x = block(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 103, in forward
    x = self.attn(x) + x
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 18, in forward
    return self.fn(self.norm(x), **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 63, in forward
    windows = rearrange(x, 'b (h s1) (w s2) d -> b (h w) s1 s2 d', s1=self.window_size, s2=self.window_size)
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 591, in rearrange
    return reduce(tensor, pattern, reduction="rearrange", **axes_lengths)
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 533, in reduce
    raise EinopsError(message + "\n {}".format(e))
einops.EinopsError:  Error while processing rearrange-reduction pattern "b (h s1) (w s2) d -> b (h w) s1 s2 d".
 Input tensor shape: torch.Size([32, 5, 13, 32]). Additional info: {'s1': 2, 's2': 2}.
 Shape mismatch, can't divide axis of length 5 in chunks of 2
[2024-12-04 14:22:39,058][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-12-04_14-22-06
[2024-12-04 14:22:39,058][src.utils.utils][INFO] - Closing wandb!
[2024-12-04 14:23:05,762][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-12-04 14:23:05,768][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-12-04 14:23:05,875][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-12-04 14:23:05,882][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-12-04 14:23:06,236][__main__][INFO] - Instantiating callbacks...
[2024-12-04 14:23:06,237][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-12-04 14:23:06,239][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-12-04 14:23:06,241][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-12-04 14:23:06,241][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-12-04 14:23:06,243][__main__][INFO] - Instantiating loggers...
[2024-12-04 14:23:06,244][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-12-04 14:23:30,333][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-12-04 14:23:31,558][__main__][INFO] - Logging hyperparameters!
[2024-12-04 14:23:31,565][__main__][INFO] - Starting training!
[2024-12-04 14:23:38,782][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 523, in reduce
    return _apply_recipe(
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 234, in _apply_recipe
    init_shapes, axes_reordering, reduced_axes, added_axes, final_shapes, n_axes_w_added = _reconstruct_from_shape(
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 187, in _reconstruct_from_shape_uncached
    raise EinopsError(f"Shape mismatch, can't divide axis of length {length} in chunks of {known_product}")
einops.EinopsError: Shape mismatch, can't divide axis of length 5 in chunks of 2

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1014, in _run_stage
    self._run_sanity_check()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1043, in _run_sanity_check
    val_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\utilities.py", line 177, in _decorator
    return loop_run(self, *args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\evaluation_loop.py", line 115, in run
    self._evaluation_step(batch, batch_idx, dataloader_idx)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\evaluation_loop.py", line 375, in _evaluation_step
    output = call._call_strategy_hook(trainer, hook_name, *step_kwargs.values())
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 291, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 379, in validation_step
    return self.model.validation_step(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 144, in validation_step
    loss, preds, targets = self.model_step(batch)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 108, in model_step
    logits = self.forward(x)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 85, in forward
    return self.net(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 177, in forward
    x = self.transformer(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 132, in forward
    x = block(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 103, in forward
    x = self.attn(x) + x
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 18, in forward
    return self.fn(self.norm(x), **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 63, in forward
    windows = rearrange(x, 'b (h s1) (w s2) d -> b (h w) s1 s2 d', s1=self.window_size, s2=self.window_size)
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 591, in rearrange
    return reduce(tensor, pattern, reduction="rearrange", **axes_lengths)
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 533, in reduce
    raise EinopsError(message + "\n {}".format(e))
einops.EinopsError:  Error while processing rearrange-reduction pattern "b (h s1) (w s2) d -> b (h w) s1 s2 d".
 Input tensor shape: torch.Size([32, 5, 13, 32]). Additional info: {'s1': 2, 's2': 2}.
 Shape mismatch, can't divide axis of length 5 in chunks of 2
[2024-12-04 14:23:38,802][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-12-04_14-23-05
[2024-12-04 14:23:38,803][src.utils.utils][INFO] - Closing wandb!
[2024-12-04 14:27:17,091][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-12-04 14:27:17,100][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-12-04 14:27:17,234][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-12-04 14:27:17,239][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-12-04 14:27:17,689][__main__][INFO] - Instantiating callbacks...
[2024-12-04 14:27:17,689][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-12-04 14:27:17,693][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-12-04 14:27:17,694][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-12-04 14:27:17,695][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-12-04 14:27:17,697][__main__][INFO] - Instantiating loggers...
[2024-12-04 14:27:17,697][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-12-04 14:27:41,522][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-12-04 14:27:42,731][__main__][INFO] - Logging hyperparameters!
[2024-12-04 14:27:42,738][__main__][INFO] - Starting training!
[2024-12-04 14:27:49,294][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 523, in reduce
    return _apply_recipe(
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 234, in _apply_recipe
    init_shapes, axes_reordering, reduced_axes, added_axes, final_shapes, n_axes_w_added = _reconstruct_from_shape(
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 187, in _reconstruct_from_shape_uncached
    raise EinopsError(f"Shape mismatch, can't divide axis of length {length} in chunks of {known_product}")
einops.EinopsError: Shape mismatch, can't divide axis of length 65 in chunks of 8

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1014, in _run_stage
    self._run_sanity_check()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1043, in _run_sanity_check
    val_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\utilities.py", line 177, in _decorator
    return loop_run(self, *args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\evaluation_loop.py", line 115, in run
    self._evaluation_step(batch, batch_idx, dataloader_idx)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\evaluation_loop.py", line 375, in _evaluation_step
    output = call._call_strategy_hook(trainer, hook_name, *step_kwargs.values())
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 291, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 379, in validation_step
    return self.model.validation_step(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 144, in validation_step
    loss, preds, targets = self.model_step(batch)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 108, in model_step
    logits = self.forward(x)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 85, in forward
    return self.net(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 178, in forward
    x = self.transformer(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 133, in forward
    x = block(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 104, in forward
    x = self.attn(x) + x
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 18, in forward
    return self.fn(self.norm(x), **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 63, in forward
    x = rearrange(x, 'b (h w) d -> b h w d', h=h)
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 591, in rearrange
    return reduce(tensor, pattern, reduction="rearrange", **axes_lengths)
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 533, in reduce
    raise EinopsError(message + "\n {}".format(e))
einops.EinopsError:  Error while processing rearrange-reduction pattern "b (h w) d -> b h w d".
 Input tensor shape: torch.Size([32, 65, 32]). Additional info: {'h': 8}.
 Shape mismatch, can't divide axis of length 65 in chunks of 8
[2024-12-04 14:27:49,315][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-12-04_14-27-16
[2024-12-04 14:27:49,317][src.utils.utils][INFO] - Closing wandb!
[2024-12-04 14:33:28,295][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-12-04 14:33:28,303][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-12-04 14:33:28,421][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-12-04 14:33:28,434][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-12-04 14:33:28,886][__main__][INFO] - Instantiating callbacks...
[2024-12-04 14:33:28,887][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-12-04 14:33:28,890][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-12-04 14:33:28,892][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-12-04 14:33:28,893][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-12-04 14:33:28,895][__main__][INFO] - Instantiating loggers...
[2024-12-04 14:33:28,895][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-12-04 14:33:52,835][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-12-04 14:33:54,054][__main__][INFO] - Logging hyperparameters!
[2024-12-04 14:33:54,065][__main__][INFO] - Starting training!
[2024-12-04 14:34:00,322][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 523, in reduce
    return _apply_recipe(
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 234, in _apply_recipe
    init_shapes, axes_reordering, reduced_axes, added_axes, final_shapes, n_axes_w_added = _reconstruct_from_shape(
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 187, in _reconstruct_from_shape_uncached
    raise EinopsError(f"Shape mismatch, can't divide axis of length {length} in chunks of {known_product}")
einops.EinopsError: Shape mismatch, can't divide axis of length 65 in chunks of 8

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1014, in _run_stage
    self._run_sanity_check()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1043, in _run_sanity_check
    val_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\utilities.py", line 177, in _decorator
    return loop_run(self, *args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\evaluation_loop.py", line 115, in run
    self._evaluation_step(batch, batch_idx, dataloader_idx)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\evaluation_loop.py", line 375, in _evaluation_step
    output = call._call_strategy_hook(trainer, hook_name, *step_kwargs.values())
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 291, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 379, in validation_step
    return self.model.validation_step(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 144, in validation_step
    loss, preds, targets = self.model_step(batch)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 108, in model_step
    logits = self.forward(x)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 85, in forward
    return self.net(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 201, in forward
    x = self.transformer(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 133, in forward
    x = block(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 104, in forward
    x = self.attn(x) + x
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 18, in forward
    return self.fn(self.norm(x), **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 63, in forward
    x = rearrange(x, 'b (h w) d -> b h w d', h=h)
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 591, in rearrange
    return reduce(tensor, pattern, reduction="rearrange", **axes_lengths)
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 533, in reduce
    raise EinopsError(message + "\n {}".format(e))
einops.EinopsError:  Error while processing rearrange-reduction pattern "b (h w) d -> b h w d".
 Input tensor shape: torch.Size([32, 65, 32]). Additional info: {'h': 8}.
 Shape mismatch, can't divide axis of length 65 in chunks of 8
[2024-12-04 14:34:00,337][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-12-04_14-33-28
[2024-12-04 14:34:00,338][src.utils.utils][INFO] - Closing wandb!
[2024-12-04 14:39:38,715][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-12-04 14:39:38,724][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-12-04 14:39:38,846][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-12-04 14:39:38,854][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-12-04 14:39:39,273][__main__][INFO] - Instantiating callbacks...
[2024-12-04 14:39:39,274][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-12-04 14:39:39,276][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-12-04 14:39:39,279][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-12-04 14:39:39,279][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-12-04 14:39:39,282][__main__][INFO] - Instantiating loggers...
[2024-12-04 14:39:39,282][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-12-04 14:40:03,526][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-12-04 14:40:04,761][__main__][INFO] - Logging hyperparameters!
[2024-12-04 14:40:04,767][__main__][INFO] - Starting training!
[2024-12-04 14:40:11,871][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 523, in reduce
    return _apply_recipe(
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 234, in _apply_recipe
    init_shapes, axes_reordering, reduced_axes, added_axes, final_shapes, n_axes_w_added = _reconstruct_from_shape(
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 187, in _reconstruct_from_shape_uncached
    raise EinopsError(f"Shape mismatch, can't divide axis of length {length} in chunks of {known_product}")
einops.EinopsError: Shape mismatch, can't divide axis of length 65 in chunks of 8

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1014, in _run_stage
    self._run_sanity_check()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1043, in _run_sanity_check
    val_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\utilities.py", line 177, in _decorator
    return loop_run(self, *args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\evaluation_loop.py", line 115, in run
    self._evaluation_step(batch, batch_idx, dataloader_idx)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\evaluation_loop.py", line 375, in _evaluation_step
    output = call._call_strategy_hook(trainer, hook_name, *step_kwargs.values())
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 291, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 379, in validation_step
    return self.model.validation_step(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 144, in validation_step
    loss, preds, targets = self.model_step(batch)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 108, in model_step
    logits = self.forward(x)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 85, in forward
    return self.net(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 200, in forward
    x = self.transformer(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 133, in forward
    x = block(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 104, in forward
    x = self.attn(x) + x
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 19, in forward
    return self.fn(self.norm(x), **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 63, in forward
    x = rearrange(x, 'b (h w) d -> b h w d', h=int(n ** 0.5))
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 591, in rearrange
    return reduce(tensor, pattern, reduction="rearrange", **axes_lengths)
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 533, in reduce
    raise EinopsError(message + "\n {}".format(e))
einops.EinopsError:  Error while processing rearrange-reduction pattern "b (h w) d -> b h w d".
 Input tensor shape: torch.Size([32, 65, 32]). Additional info: {'h': 8}.
 Shape mismatch, can't divide axis of length 65 in chunks of 8
[2024-12-04 14:40:11,894][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-12-04_14-39-38
[2024-12-04 14:40:11,895][src.utils.utils][INFO] - Closing wandb!
[2024-12-04 14:42:07,955][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-12-04 14:42:07,961][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-12-04 14:42:08,061][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-12-04 14:42:08,066][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-12-04 14:42:08,410][__main__][INFO] - Instantiating callbacks...
[2024-12-04 14:42:08,411][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-12-04 14:42:08,414][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-12-04 14:42:08,415][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-12-04 14:42:08,416][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-12-04 14:42:08,418][__main__][INFO] - Instantiating loggers...
[2024-12-04 14:42:08,418][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-12-04 14:42:33,264][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-12-04 14:42:34,494][__main__][INFO] - Logging hyperparameters!
[2024-12-04 14:42:34,500][__main__][INFO] - Starting training!
[2024-12-04 14:42:42,014][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 523, in reduce
    return _apply_recipe(
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 234, in _apply_recipe
    init_shapes, axes_reordering, reduced_axes, added_axes, final_shapes, n_axes_w_added = _reconstruct_from_shape(
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 187, in _reconstruct_from_shape_uncached
    raise EinopsError(f"Shape mismatch, can't divide axis of length {length} in chunks of {known_product}")
einops.EinopsError: Shape mismatch, can't divide axis of length 65 in chunks of 8

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1014, in _run_stage
    self._run_sanity_check()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1043, in _run_sanity_check
    val_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\utilities.py", line 177, in _decorator
    return loop_run(self, *args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\evaluation_loop.py", line 115, in run
    self._evaluation_step(batch, batch_idx, dataloader_idx)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\evaluation_loop.py", line 375, in _evaluation_step
    output = call._call_strategy_hook(trainer, hook_name, *step_kwargs.values())
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 291, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 379, in validation_step
    return self.model.validation_step(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 144, in validation_step
    loss, preds, targets = self.model_step(batch)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 108, in model_step
    logits = self.forward(x)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 85, in forward
    return self.net(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 203, in forward
    x = self.transformer(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 133, in forward
    x = block(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 104, in forward
    x = self.attn(x) + x
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 19, in forward
    return self.fn(self.norm(x), **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 63, in forward
    x = rearrange(x, 'b (h w) d -> b h w d', h=int(n ** 0.5))
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 591, in rearrange
    return reduce(tensor, pattern, reduction="rearrange", **axes_lengths)
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 533, in reduce
    raise EinopsError(message + "\n {}".format(e))
einops.EinopsError:  Error while processing rearrange-reduction pattern "b (h w) d -> b h w d".
 Input tensor shape: torch.Size([32, 65, 32]). Additional info: {'h': 8}.
 Shape mismatch, can't divide axis of length 65 in chunks of 8
[2024-12-04 14:42:42,029][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-12-04_14-42-07
[2024-12-04 14:42:42,030][src.utils.utils][INFO] - Closing wandb!
[2024-12-04 14:46:58,305][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-12-04 14:46:58,312][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-12-04 14:46:58,414][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-12-04 14:46:58,420][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-12-04 14:46:58,781][__main__][INFO] - Instantiating callbacks...
[2024-12-04 14:46:58,782][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-12-04 14:46:58,784][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-12-04 14:46:58,785][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-12-04 14:46:58,787][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-12-04 14:46:58,789][__main__][INFO] - Instantiating loggers...
[2024-12-04 14:46:58,789][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-12-04 14:47:23,602][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-12-04 14:47:25,042][__main__][INFO] - Logging hyperparameters!
[2024-12-04 14:47:25,055][__main__][INFO] - Starting training!
[2024-12-04 14:47:33,376][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 523, in reduce
    return _apply_recipe(
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 234, in _apply_recipe
    init_shapes, axes_reordering, reduced_axes, added_axes, final_shapes, n_axes_w_added = _reconstruct_from_shape(
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 187, in _reconstruct_from_shape_uncached
    raise EinopsError(f"Shape mismatch, can't divide axis of length {length} in chunks of {known_product}")
einops.EinopsError: Shape mismatch, can't divide axis of length 65 in chunks of 8

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1014, in _run_stage
    self._run_sanity_check()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1043, in _run_sanity_check
    val_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\utilities.py", line 177, in _decorator
    return loop_run(self, *args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\evaluation_loop.py", line 115, in run
    self._evaluation_step(batch, batch_idx, dataloader_idx)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\evaluation_loop.py", line 375, in _evaluation_step
    output = call._call_strategy_hook(trainer, hook_name, *step_kwargs.values())
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 291, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 379, in validation_step
    return self.model.validation_step(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 144, in validation_step
    loss, preds, targets = self.model_step(batch)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 108, in model_step
    logits = self.forward(x)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 85, in forward
    return self.net(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 177, in forward
    x = self.transformer(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 132, in forward
    x = block(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 103, in forward
    x = self.attn(x) + x
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 18, in forward
    return self.fn(self.norm(x), **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 62, in forward
    x = rearrange(x, 'b (h w) d -> b h w d', h=int(n ** 0.5))
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 591, in rearrange
    return reduce(tensor, pattern, reduction="rearrange", **axes_lengths)
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 533, in reduce
    raise EinopsError(message + "\n {}".format(e))
einops.EinopsError:  Error while processing rearrange-reduction pattern "b (h w) d -> b h w d".
 Input tensor shape: torch.Size([32, 65, 32]). Additional info: {'h': 8}.
 Shape mismatch, can't divide axis of length 65 in chunks of 8
[2024-12-04 14:47:33,392][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-12-04_14-46-58
[2024-12-04 14:47:33,392][src.utils.utils][INFO] - Closing wandb!
[2024-12-04 14:51:26,210][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-12-04 14:51:26,218][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-12-04 14:51:26,312][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-12-04 14:51:26,318][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-12-04 14:51:26,666][__main__][INFO] - Instantiating callbacks...
[2024-12-04 14:51:26,667][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-12-04 14:51:26,670][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-12-04 14:51:26,671][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-12-04 14:51:26,672][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-12-04 14:51:26,674][__main__][INFO] - Instantiating loggers...
[2024-12-04 14:51:26,674][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-12-04 14:51:51,050][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-12-04 14:51:52,265][__main__][INFO] - Logging hyperparameters!
[2024-12-04 14:51:52,271][__main__][INFO] - Starting training!
[2024-12-04 14:51:58,381][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1014, in _run_stage
    self._run_sanity_check()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1043, in _run_sanity_check
    val_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\utilities.py", line 177, in _decorator
    return loop_run(self, *args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\evaluation_loop.py", line 115, in run
    self._evaluation_step(batch, batch_idx, dataloader_idx)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\evaluation_loop.py", line 375, in _evaluation_step
    output = call._call_strategy_hook(trainer, hook_name, *step_kwargs.values())
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 291, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 379, in validation_step
    return self.model.validation_step(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 144, in validation_step
    loss, preds, targets = self.model_step(batch)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 108, in model_step
    logits = self.forward(x)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 85, in forward
    return self.net(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 182, in forward
    x = self.transformer(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 137, in forward
    x = block(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 108, in forward
    x = self.attn(x) + x
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 18, in forward
    return self.fn(self.norm(x), **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 64, in forward
    x = F.pad(x, (0, 0, 0, padding_needed))  # 
NameError: name 'F' is not defined
[2024-12-04 14:51:58,392][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-12-04_14-51-26
[2024-12-04 14:51:58,392][src.utils.utils][INFO] - Closing wandb!
[2024-12-04 14:55:00,512][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-12-04 14:55:00,519][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-12-04 14:55:00,624][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-12-04 14:55:00,631][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-12-04 14:55:00,987][__main__][INFO] - Instantiating callbacks...
[2024-12-04 14:55:00,988][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-12-04 14:55:00,992][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-12-04 14:55:00,993][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-12-04 14:55:00,994][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-12-04 14:55:00,995][__main__][INFO] - Instantiating loggers...
[2024-12-04 14:55:00,996][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-12-04 14:55:25,062][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-12-04 14:55:26,306][__main__][INFO] - Logging hyperparameters!
[2024-12-04 14:55:26,313][__main__][INFO] - Starting training!
[2024-12-04 14:55:33,616][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1014, in _run_stage
    self._run_sanity_check()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1043, in _run_sanity_check
    val_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\utilities.py", line 177, in _decorator
    return loop_run(self, *args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\evaluation_loop.py", line 115, in run
    self._evaluation_step(batch, batch_idx, dataloader_idx)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\evaluation_loop.py", line 375, in _evaluation_step
    output = call._call_strategy_hook(trainer, hook_name, *step_kwargs.values())
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 291, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 379, in validation_step
    return self.model.validation_step(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 144, in validation_step
    loss, preds, targets = self.model_step(batch)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 108, in model_step
    logits = self.forward(x)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 85, in forward
    return self.net(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 186, in forward
    x = self.transformer(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 141, in forward
    x = block(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 112, in forward
    x = self.attn(x) + x
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 22, in forward
    return self.fn(self.norm(x), **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 68, in forward
    x = F.pad(x, (0, 0, 0, padding_needed))  # 
AttributeError: 'TypeVar' object has no attribute 'pad'
[2024-12-04 14:55:33,623][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-12-04_14-55-00
[2024-12-04 14:55:33,624][src.utils.utils][INFO] - Closing wandb!
[2024-12-04 14:56:15,523][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-12-04 14:56:15,530][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-12-04 14:56:15,633][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-12-04 14:56:15,640][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-12-04 14:56:15,991][__main__][INFO] - Instantiating callbacks...
[2024-12-04 14:56:15,992][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-12-04 14:56:15,996][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-12-04 14:56:15,996][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-12-04 14:56:15,998][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-12-04 14:56:16,000][__main__][INFO] - Instantiating loggers...
[2024-12-04 14:56:16,000][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-12-04 14:56:40,054][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-12-04 14:56:41,263][__main__][INFO] - Logging hyperparameters!
[2024-12-04 14:56:41,270][__main__][INFO] - Starting training!
[2024-12-04 14:56:47,328][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 523, in reduce
    return _apply_recipe(
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 234, in _apply_recipe
    init_shapes, axes_reordering, reduced_axes, added_axes, final_shapes, n_axes_w_added = _reconstruct_from_shape(
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 187, in _reconstruct_from_shape_uncached
    raise EinopsError(f"Shape mismatch, can't divide axis of length {length} in chunks of {known_product}")
einops.EinopsError: Shape mismatch, can't divide axis of length 9 in chunks of 2

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1014, in _run_stage
    self._run_sanity_check()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1043, in _run_sanity_check
    val_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\utilities.py", line 177, in _decorator
    return loop_run(self, *args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\evaluation_loop.py", line 115, in run
    self._evaluation_step(batch, batch_idx, dataloader_idx)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\evaluation_loop.py", line 375, in _evaluation_step
    output = call._call_strategy_hook(trainer, hook_name, *step_kwargs.values())
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 291, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 379, in validation_step
    return self.model.validation_step(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 144, in validation_step
    loss, preds, targets = self.model_step(batch)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 108, in model_step
    logits = self.forward(x)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 85, in forward
    return self.net(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 185, in forward
    x = self.transformer(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 140, in forward
    x = block(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 111, in forward
    x = self.attn(x) + x
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 21, in forward
    return self.fn(self.norm(x), **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 71, in forward
    windows = rearrange(x, 'b (h s1) (w s2) d -> b (h w) s1 s2 d', s1=self.window_size, s2=self.window_size)
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 591, in rearrange
    return reduce(tensor, pattern, reduction="rearrange", **axes_lengths)
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 533, in reduce
    raise EinopsError(message + "\n {}".format(e))
einops.EinopsError:  Error while processing rearrange-reduction pattern "b (h s1) (w s2) d -> b (h w) s1 s2 d".
 Input tensor shape: torch.Size([32, 8, 9, 32]). Additional info: {'s1': 2, 's2': 2}.
 Shape mismatch, can't divide axis of length 9 in chunks of 2
[2024-12-04 14:56:47,344][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-12-04_14-56-15
[2024-12-04 14:56:47,345][src.utils.utils][INFO] - Closing wandb!
[2024-12-04 14:58:07,969][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-12-04 14:58:07,976][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-12-04 14:58:08,066][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-12-04 14:58:08,071][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-12-04 14:58:08,424][__main__][INFO] - Instantiating callbacks...
[2024-12-04 14:58:08,424][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-12-04 14:58:08,428][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-12-04 14:58:08,429][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-12-04 14:58:08,430][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-12-04 14:58:08,432][__main__][INFO] - Instantiating loggers...
[2024-12-04 14:58:08,432][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-12-04 14:58:32,743][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-12-04 14:58:34,258][__main__][INFO] - Logging hyperparameters!
[2024-12-04 14:58:34,267][__main__][INFO] - Starting training!
[2024-12-04 14:58:43,444][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 523, in reduce
    return _apply_recipe(
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 234, in _apply_recipe
    init_shapes, axes_reordering, reduced_axes, added_axes, final_shapes, n_axes_w_added = _reconstruct_from_shape(
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 187, in _reconstruct_from_shape_uncached
    raise EinopsError(f"Shape mismatch, can't divide axis of length {length} in chunks of {known_product}")
einops.EinopsError: Shape mismatch, can't divide axis of length 66 in chunks of 8

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1014, in _run_stage
    self._run_sanity_check()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1043, in _run_sanity_check
    val_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\utilities.py", line 177, in _decorator
    return loop_run(self, *args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\evaluation_loop.py", line 115, in run
    self._evaluation_step(batch, batch_idx, dataloader_idx)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\evaluation_loop.py", line 375, in _evaluation_step
    output = call._call_strategy_hook(trainer, hook_name, *step_kwargs.values())
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 291, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 379, in validation_step
    return self.model.validation_step(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 144, in validation_step
    loss, preds, targets = self.model_step(batch)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 108, in model_step
    logits = self.forward(x)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 85, in forward
    return self.net(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 182, in forward
    x = self.transformer(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 137, in forward
    x = block(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 108, in forward
    x = self.attn(x) + x
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 18, in forward
    return self.fn(self.norm(x), **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 67, in forward
    x = rearrange(x, 'b (h w) d -> b h w d', h=int(n ** 0.5))
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 591, in rearrange
    return reduce(tensor, pattern, reduction="rearrange", **axes_lengths)
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 533, in reduce
    raise EinopsError(message + "\n {}".format(e))
einops.EinopsError:  Error while processing rearrange-reduction pattern "b (h w) d -> b h w d".
 Input tensor shape: torch.Size([32, 66, 32]). Additional info: {'h': 8}.
 Shape mismatch, can't divide axis of length 66 in chunks of 8
[2024-12-04 14:58:43,461][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-12-04_14-58-07
[2024-12-04 14:58:43,462][src.utils.utils][INFO] - Closing wandb!
[2024-12-04 15:16:05,223][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-12-04 15:16:05,234][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-12-04 15:16:05,352][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-12-04 15:16:05,360][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-12-04 15:16:05,740][__main__][INFO] - Instantiating callbacks...
[2024-12-04 15:16:05,741][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-12-04 15:16:05,744][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-12-04 15:16:05,745][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-12-04 15:16:05,745][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-12-04 15:16:05,747][__main__][INFO] - Instantiating loggers...
[2024-12-04 15:16:05,748][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-12-04 15:16:30,078][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-12-04 15:16:31,280][__main__][INFO] - Logging hyperparameters!
[2024-12-04 15:16:31,287][__main__][INFO] - Starting training!
[2024-12-04 15:16:38,181][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 523, in reduce
    return _apply_recipe(
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 234, in _apply_recipe
    init_shapes, axes_reordering, reduced_axes, added_axes, final_shapes, n_axes_w_added = _reconstruct_from_shape(
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 187, in _reconstruct_from_shape_uncached
    raise EinopsError(f"Shape mismatch, can't divide axis of length {length} in chunks of {known_product}")
einops.EinopsError: Shape mismatch, can't divide axis of length 9 in chunks of 2

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1014, in _run_stage
    self._run_sanity_check()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1043, in _run_sanity_check
    val_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\utilities.py", line 177, in _decorator
    return loop_run(self, *args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\evaluation_loop.py", line 115, in run
    self._evaluation_step(batch, batch_idx, dataloader_idx)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\evaluation_loop.py", line 375, in _evaluation_step
    output = call._call_strategy_hook(trainer, hook_name, *step_kwargs.values())
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 291, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 379, in validation_step
    return self.model.validation_step(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 144, in validation_step
    loss, preds, targets = self.model_step(batch)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 108, in model_step
    logits = self.forward(x)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 85, in forward
    return self.net(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 182, in forward
    x = self.transformer(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 137, in forward
    x = block(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 108, in forward
    x = self.attn(x) + x
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 18, in forward
    return self.fn(self.norm(x), **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 68, in forward
    windows = rearrange(x, 'b (h s1) (w s2) d -> b (h w) s1 s2 d', s1=self.window_size, s2=self.window_size)
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 591, in rearrange
    return reduce(tensor, pattern, reduction="rearrange", **axes_lengths)
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 533, in reduce
    raise EinopsError(message + "\n {}".format(e))
einops.EinopsError:  Error while processing rearrange-reduction pattern "b (h s1) (w s2) d -> b (h w) s1 s2 d".
 Input tensor shape: torch.Size([32, 8, 9, 32]). Additional info: {'s1': 2, 's2': 2}.
 Shape mismatch, can't divide axis of length 9 in chunks of 2
[2024-12-04 15:16:38,195][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-12-04_15-16-05
[2024-12-04 15:16:38,195][src.utils.utils][INFO] - Closing wandb!
[2024-12-04 15:17:28,142][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-12-04 15:17:28,150][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-12-04 15:17:28,244][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-12-04 15:17:28,250][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-12-04 15:17:28,605][__main__][INFO] - Instantiating callbacks...
[2024-12-04 15:17:28,605][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-12-04 15:17:28,608][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-12-04 15:17:28,610][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-12-04 15:17:28,612][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-12-04 15:17:28,614][__main__][INFO] - Instantiating loggers...
[2024-12-04 15:17:28,614][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-12-04 15:17:52,602][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-12-04 15:17:53,802][__main__][INFO] - Logging hyperparameters!
[2024-12-04 15:17:53,809][__main__][INFO] - Starting training!
[2024-12-04 15:18:00,470][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 523, in reduce
    return _apply_recipe(
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 234, in _apply_recipe
    init_shapes, axes_reordering, reduced_axes, added_axes, final_shapes, n_axes_w_added = _reconstruct_from_shape(
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 187, in _reconstruct_from_shape_uncached
    raise EinopsError(f"Shape mismatch, can't divide axis of length {length} in chunks of {known_product}")
einops.EinopsError: Shape mismatch, can't divide axis of length 66 in chunks of 8

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1014, in _run_stage
    self._run_sanity_check()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1043, in _run_sanity_check
    val_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\utilities.py", line 177, in _decorator
    return loop_run(self, *args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\evaluation_loop.py", line 115, in run
    self._evaluation_step(batch, batch_idx, dataloader_idx)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\evaluation_loop.py", line 375, in _evaluation_step
    output = call._call_strategy_hook(trainer, hook_name, *step_kwargs.values())
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 291, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 379, in validation_step
    return self.model.validation_step(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 144, in validation_step
    loss, preds, targets = self.model_step(batch)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 108, in model_step
    logits = self.forward(x)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 85, in forward
    return self.net(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 182, in forward
    x = self.transformer(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 137, in forward
    x = block(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 108, in forward
    x = self.attn(x) + x
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 18, in forward
    return self.fn(self.norm(x), **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\v.py", line 67, in forward
    x = rearrange(x, 'b (h w) d -> b h w d', h=int(n ** 0.5))
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 591, in rearrange
    return reduce(tensor, pattern, reduction="rearrange", **axes_lengths)
  File "D:\Anaconda3\lib\site-packages\einops\einops.py", line 533, in reduce
    raise EinopsError(message + "\n {}".format(e))
einops.EinopsError:  Error while processing rearrange-reduction pattern "b (h w) d -> b h w d".
 Input tensor shape: torch.Size([32, 66, 32]). Additional info: {'h': 8}.
 Shape mismatch, can't divide axis of length 66 in chunks of 8
[2024-12-04 15:18:00,490][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-12-04_15-17-28
[2024-12-04 15:18:00,491][src.utils.utils][INFO] - Closing wandb!
[2024-12-09 15:09:27,951][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-12-09 15:09:27,961][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-12-09 15:09:28,057][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-12-09 15:09:28,064][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-12-09 15:09:30,588][__main__][INFO] - Instantiating callbacks...
[2024-12-09 15:09:30,589][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-12-09 15:09:30,594][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-12-09 15:09:30,596][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-12-09 15:09:30,596][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-12-09 15:09:30,598][__main__][INFO] - Instantiating loggers...
[2024-12-09 15:09:30,599][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-12-09 15:09:53,508][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-12-09 15:09:55,170][__main__][INFO] - Logging hyperparameters!
[2024-12-09 15:09:55,179][__main__][INFO] - Starting training!
[2024-12-09 15:10:07,216][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1014, in _run_stage
    self._run_sanity_check()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1043, in _run_sanity_check
    val_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\utilities.py", line 177, in _decorator
    return loop_run(self, *args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\evaluation_loop.py", line 115, in run
    self._evaluation_step(batch, batch_idx, dataloader_idx)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\evaluation_loop.py", line 375, in _evaluation_step
    output = call._call_strategy_hook(trainer, hook_name, *step_kwargs.values())
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 291, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 379, in validation_step
    return self.model.validation_step(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 144, in validation_step
    loss, preds, targets = self.model_step(batch)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 108, in model_step
    logits = self.forward(x)
  File "D:\pycharmproject\template\srcC\models\CIFAR10_module.py", line 85, in forward
    return self.net(x)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\cnnKvit.py", line 276, in forward
    combined_features = self.cross_attention(x.unsqueeze(1), conv_features.unsqueeze(1))
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\cnnKvit.py", line 211, in forward
    attn_x = self.attn(x, y, y)  # x y 
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\models\components\cnnKvit.py", line 138, in forward
    k, v = self.to_kv(key).chunk(2, dim=-1)  #  k  v
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\nn\modules\linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: mat1 and mat2 shapes cannot be multiplied (32x512 and 256x1024)
[2024-12-09 15:10:07,226][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2024-12-09_15-09-27
[2024-12-09 15:10:07,226][src.utils.utils][INFO] - Closing wandb!
[2024-12-09 15:11:15,474][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2024-12-09 15:11:15,481][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2024-12-09 15:11:15,583][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2024-12-09 15:11:15,588][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2024-12-09 15:11:16,960][__main__][INFO] - Instantiating callbacks...
[2024-12-09 15:11:16,961][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-12-09 15:11:16,965][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2024-12-09 15:11:16,967][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-12-09 15:11:16,968][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-12-09 15:11:16,973][__main__][INFO] - Instantiating loggers...
[2024-12-09 15:11:16,973][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2024-12-09 15:11:39,639][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2024-12-09 15:11:41,212][__main__][INFO] - Logging hyperparameters!
[2024-12-09 15:11:41,217][__main__][INFO] - Starting training!
[2025-03-04 13:52:36,082][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2025-03-04 13:52:36,090][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2025-03-04 13:52:36,173][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2025-03-04 13:52:36,179][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2025-03-04 13:52:36,785][__main__][INFO] - Instantiating callbacks...
[2025-03-04 13:52:36,786][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2025-03-04 13:52:36,791][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2025-03-04 13:52:36,792][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2025-03-04 13:52:36,793][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2025-03-04 13:52:36,796][__main__][INFO] - Instantiating loggers...
[2025-03-04 13:52:36,796][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2025-03-04 13:53:00,590][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2025-03-04 13:53:02,460][__main__][INFO] - Logging hyperparameters!
[2025-03-04 13:53:02,470][__main__][INFO] - Starting training!
[2025-03-04 13:53:16,892][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1016, in _run_stage
    self.fit_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 201, in run
    self.advance()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 354, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 133, in run
    self.advance(data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 218, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 185, in run
    self._optimizer_step(kwargs.get("batch_idx", 0), closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 260, in _optimizer_step
    call._call_lightning_module_hook(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 144, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\module.py", line 1256, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\optimizer.py", line 155, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 225, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\plugins\precision\precision_plugin.py", line 114, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\optim\optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\optim\ys.py", line 98, in step
    p.data.addcdiv_(-step_size, exp_avg, denom)
TypeError: addcdiv_() takes 2 positional arguments but 3 were given
[2025-03-04 13:53:16,913][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2025-03-04_13-52-35
[2025-03-04 13:53:16,922][src.utils.utils][INFO] - Closing wandb!
[2025-03-04 13:57:27,211][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2025-03-04 13:57:27,221][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2025-03-04 13:57:27,320][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2025-03-04 13:57:27,326][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2025-03-04 13:57:27,723][__main__][INFO] - Instantiating callbacks...
[2025-03-04 13:57:27,723][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2025-03-04 13:57:27,727][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2025-03-04 13:57:27,728][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2025-03-04 13:57:27,729][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2025-03-04 13:57:27,732][__main__][INFO] - Instantiating loggers...
[2025-03-04 13:57:27,732][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2025-03-04 13:57:50,711][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2025-03-04 13:57:51,972][__main__][INFO] - Logging hyperparameters!
[2025-03-04 13:57:51,976][__main__][INFO] - Starting training!
[2025-03-04 13:57:59,919][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1016, in _run_stage
    self.fit_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 201, in run
    self.advance()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 354, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 133, in run
    self.advance(data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 218, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 185, in run
    self._optimizer_step(kwargs.get("batch_idx", 0), closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 260, in _optimizer_step
    call._call_lightning_module_hook(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 144, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\module.py", line 1256, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\optimizer.py", line 155, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 225, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\plugins\precision\precision_plugin.py", line 114, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\optim\optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\optim\ys.py", line 98, in step
    p.data.addcdiv_(exp_avg, denom, value=-step_size)
TypeError: addcdiv_(): argument 'value' must be Number, not Tensor
[2025-03-04 13:57:59,926][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2025-03-04_13-57-27
[2025-03-04 13:57:59,927][src.utils.utils][INFO] - Closing wandb!
[2025-03-04 14:02:10,138][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2025-03-04 14:02:10,146][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2025-03-04 14:02:10,228][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2025-03-04 14:02:10,233][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2025-03-04 14:02:10,716][__main__][INFO] - Instantiating callbacks...
[2025-03-04 14:02:10,717][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2025-03-04 14:02:10,721][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2025-03-04 14:02:10,722][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2025-03-04 14:02:10,723][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2025-03-04 14:02:10,725][__main__][INFO] - Instantiating loggers...
[2025-03-04 14:02:10,725][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2025-03-04 14:02:43,098][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2025-03-04 14:02:44,407][__main__][INFO] - Logging hyperparameters!
[2025-03-04 14:02:44,413][__main__][INFO] - Starting training!
[2025-03-04 14:02:53,196][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1016, in _run_stage
    self.fit_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 201, in run
    self.advance()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 354, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 133, in run
    self.advance(data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 218, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 185, in run
    self._optimizer_step(kwargs.get("batch_idx", 0), closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 260, in _optimizer_step
    call._call_lightning_module_hook(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 144, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\module.py", line 1256, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\optimizer.py", line 155, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 225, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\plugins\precision\precision_plugin.py", line 114, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\optim\optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\optim\ys.py", line 98, in step
    p.data.addcdiv_(exp_avg, denom, value=-step_size.item())  #  .item()
RuntimeError: a Tensor with 864 elements cannot be converted to Scalar
[2025-03-04 14:02:53,203][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2025-03-04_14-02-10
[2025-03-04 14:02:53,204][src.utils.utils][INFO] - Closing wandb!
[2025-03-04 14:03:36,026][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2025-03-04 14:03:36,034][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2025-03-04 14:03:36,125][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2025-03-04 14:03:36,132][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2025-03-04 14:03:36,505][__main__][INFO] - Instantiating callbacks...
[2025-03-04 14:03:36,505][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2025-03-04 14:03:36,510][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2025-03-04 14:03:36,512][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2025-03-04 14:03:36,513][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2025-03-04 14:03:36,515][__main__][INFO] - Instantiating loggers...
[2025-03-04 14:03:36,516][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2025-03-04 14:04:09,232][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2025-03-04 14:04:10,797][__main__][INFO] - Logging hyperparameters!
[2025-03-04 14:04:10,804][__main__][INFO] - Starting training!
[2025-03-04 14:04:20,635][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2025-03-04_14-03-35
[2025-03-04 14:04:20,636][src.utils.utils][INFO] - Closing wandb!
[2025-03-04 14:05:28,558][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2025-03-04 14:05:28,567][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2025-03-04 14:05:28,645][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2025-03-04 14:05:28,650][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2025-03-04 14:05:28,937][__main__][INFO] - Instantiating callbacks...
[2025-03-04 14:05:28,939][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2025-03-04 14:05:28,941][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2025-03-04 14:05:28,943][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2025-03-04 14:05:28,943][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2025-03-04 14:05:28,946][__main__][INFO] - Instantiating loggers...
[2025-03-04 14:05:28,946][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2025-03-04 14:06:01,253][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2025-03-04 14:06:02,496][__main__][INFO] - Logging hyperparameters!
[2025-03-04 14:06:02,503][__main__][INFO] - Starting training!
[2025-03-04 14:06:10,317][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1016, in _run_stage
    self.fit_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 201, in run
    self.advance()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 354, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 133, in run
    self.advance(data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 218, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 185, in run
    self._optimizer_step(kwargs.get("batch_idx", 0), closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 260, in _optimizer_step
    call._call_lightning_module_hook(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 144, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\module.py", line 1256, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\optimizer.py", line 155, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 225, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\plugins\precision\precision_plugin.py", line 114, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\optim\optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\optim\ys.py", line 98, in step
    p.data.addcdiv_(exp_avg, denom, step_size.mul(-1))
TypeError: addcdiv_() takes 2 positional arguments but 3 were given
[2025-03-04 14:06:10,328][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2025-03-04_14-05-28
[2025-03-04 14:06:10,330][src.utils.utils][INFO] - Closing wandb!
[2025-03-04 14:07:20,988][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2025-03-04 14:07:20,995][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2025-03-04 14:07:21,079][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2025-03-04 14:07:21,085][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2025-03-04 14:07:21,382][__main__][INFO] - Instantiating callbacks...
[2025-03-04 14:07:21,382][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2025-03-04 14:07:21,386][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2025-03-04 14:07:21,388][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2025-03-04 14:07:21,388][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2025-03-04 14:07:21,390][__main__][INFO] - Instantiating loggers...
[2025-03-04 14:07:21,391][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2025-03-04 14:07:53,720][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2025-03-04 14:07:54,964][__main__][INFO] - Logging hyperparameters!
[2025-03-04 14:07:54,968][__main__][INFO] - Starting training!
[2025-03-04 14:13:00,902][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2025-03-04 14:13:00,910][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2025-03-04 14:13:00,993][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2025-03-04 14:13:00,999][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2025-03-04 14:13:01,294][__main__][INFO] - Instantiating callbacks...
[2025-03-04 14:13:01,295][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2025-03-04 14:13:01,299][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2025-03-04 14:13:01,300][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2025-03-04 14:13:01,301][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2025-03-04 14:13:01,304][__main__][INFO] - Instantiating loggers...
[2025-03-04 14:13:01,304][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2025-03-04 14:13:33,478][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2025-03-04 14:13:34,753][__main__][INFO] - Logging hyperparameters!
[2025-03-04 14:13:34,757][__main__][INFO] - Starting training!
[2025-03-04 14:16:48,913][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2025-03-04 14:16:48,920][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2025-03-04 14:16:48,990][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2025-03-04 14:16:48,995][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2025-03-04 14:16:49,295][__main__][INFO] - Instantiating callbacks...
[2025-03-04 14:16:49,296][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2025-03-04 14:16:49,299][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2025-03-04 14:16:49,301][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2025-03-04 14:16:49,303][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2025-03-04 14:16:49,305][__main__][INFO] - Instantiating loggers...
[2025-03-04 14:16:49,305][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2025-03-04 14:17:21,488][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2025-03-04 14:17:22,796][__main__][INFO] - Logging hyperparameters!
[2025-03-04 14:17:22,800][__main__][INFO] - Starting training!
[2025-03-04 14:17:30,702][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1016, in _run_stage
    self.fit_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 201, in run
    self.advance()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 354, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 133, in run
    self.advance(data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 218, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 185, in run
    self._optimizer_step(kwargs.get("batch_idx", 0), closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 260, in _optimizer_step
    call._call_lightning_module_hook(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 144, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\module.py", line 1256, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\optimizer.py", line 155, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 225, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\plugins\precision\precision_plugin.py", line 114, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\optim\optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\optim\ys.py", line 123, in step
    p.data.addcdiv_(exp_avg, denom, value=-step_size)
TypeError: addcdiv_(): argument 'value' must be Number, not Tensor
[2025-03-04 14:17:30,710][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2025-03-04_14-16-48
[2025-03-04 14:17:30,711][src.utils.utils][INFO] - Closing wandb!
[2025-03-04 14:17:52,673][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2025-03-04 14:17:52,683][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2025-03-04 14:17:52,794][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2025-03-04 14:17:52,801][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2025-03-04 14:17:53,162][__main__][INFO] - Instantiating callbacks...
[2025-03-04 14:17:53,163][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2025-03-04 14:17:53,167][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2025-03-04 14:17:53,169][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2025-03-04 14:17:53,170][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2025-03-04 14:17:53,172][__main__][INFO] - Instantiating loggers...
[2025-03-04 14:17:53,173][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2025-03-04 14:18:25,720][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2025-03-04 14:18:27,010][__main__][INFO] - Logging hyperparameters!
[2025-03-04 14:18:27,015][__main__][INFO] - Starting training!
[2025-03-04 14:20:45,098][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2025-03-04 14:20:45,105][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2025-03-04 14:20:45,202][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2025-03-04 14:20:45,208][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2025-03-04 14:20:45,511][__main__][INFO] - Instantiating callbacks...
[2025-03-04 14:20:45,512][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2025-03-04 14:20:45,516][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2025-03-04 14:20:45,517][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2025-03-04 14:20:45,518][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2025-03-04 14:20:45,521][__main__][INFO] - Instantiating loggers...
[2025-03-04 14:20:45,521][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2025-03-04 14:22:34,661][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2025-03-04 14:22:34,669][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2025-03-04 14:22:34,758][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2025-03-04 14:22:34,765][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2025-03-04 14:22:35,059][__main__][INFO] - Instantiating callbacks...
[2025-03-04 14:22:35,060][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2025-03-04 14:22:35,063][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2025-03-04 14:22:35,065][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2025-03-04 14:22:35,066][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2025-03-04 14:22:35,068][__main__][INFO] - Instantiating loggers...
[2025-03-04 14:22:35,069][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2025-03-04 14:23:07,436][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2025-03-04 14:23:08,725][__main__][INFO] - Logging hyperparameters!
[2025-03-04 14:23:08,730][__main__][INFO] - Starting training!
[2025-03-04 14:32:33,684][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2025-03-04 14:32:33,691][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2025-03-04 14:32:33,778][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2025-03-04 14:32:33,783][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2025-03-04 14:32:33,883][__main__][INFO] - Instantiating callbacks...
[2025-03-04 14:32:33,883][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2025-03-04 14:32:33,887][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2025-03-04 14:32:33,888][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2025-03-04 14:32:33,889][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2025-03-04 14:32:33,891][__main__][INFO] - Instantiating loggers...
[2025-03-04 14:32:33,891][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2025-03-04 14:33:06,077][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2025-03-04 14:33:07,355][__main__][INFO] - Logging hyperparameters!
[2025-03-04 14:33:07,360][__main__][INFO] - Starting training!
[2025-03-04 14:33:15,254][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1016, in _run_stage
    self.fit_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 201, in run
    self.advance()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 354, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 133, in run
    self.advance(data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 218, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 185, in run
    self._optimizer_step(kwargs.get("batch_idx", 0), closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 260, in _optimizer_step
    call._call_lightning_module_hook(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 144, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\module.py", line 1256, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\optimizer.py", line 155, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 225, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\plugins\precision\precision_plugin.py", line 114, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\optim\optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\optim\ys.py", line 107, in step
    p.data.addcdiv_(-step_size, exp_avg, denom)
TypeError: addcdiv_() takes 2 positional arguments but 3 were given
[2025-03-04 14:33:15,262][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2025-03-04_14-32-33
[2025-03-04 14:33:15,263][src.utils.utils][INFO] - Closing wandb!
[2025-03-04 14:34:31,107][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2025-03-04 14:34:31,114][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2025-03-04 14:34:31,208][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2025-03-04 14:34:31,213][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2025-03-04 14:34:31,308][__main__][INFO] - Instantiating callbacks...
[2025-03-04 14:34:31,309][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2025-03-04 14:34:31,312][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2025-03-04 14:34:31,314][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2025-03-04 14:34:31,315][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2025-03-04 14:34:31,317][__main__][INFO] - Instantiating loggers...
[2025-03-04 14:34:31,318][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2025-03-04 14:35:03,594][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2025-03-04 14:35:04,981][__main__][INFO] - Logging hyperparameters!
[2025-03-04 14:35:04,986][__main__][INFO] - Starting training!
[2025-03-04 14:40:33,255][__main__][INFO] - Starting testing!
[2025-03-04 14:43:28,795][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2025-03-04 14:43:28,801][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2025-03-04 14:43:28,914][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2025-03-04 14:43:28,921][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2025-03-04 14:43:29,027][__main__][INFO] - Instantiating callbacks...
[2025-03-04 14:43:29,028][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2025-03-04 14:43:29,031][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2025-03-04 14:43:29,033][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2025-03-04 14:43:29,034][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2025-03-04 14:43:29,038][__main__][INFO] - Instantiating loggers...
[2025-03-04 14:43:29,038][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2025-03-04 14:44:01,613][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2025-03-04 14:44:03,001][__main__][INFO] - Logging hyperparameters!
[2025-03-04 14:44:03,009][__main__][INFO] - Starting training!
[2025-03-04 14:47:54,610][__main__][INFO] - Starting testing!
[2025-03-04 14:52:48,402][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2025-03-04 14:52:48,410][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2025-03-04 14:52:48,493][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2025-03-04 14:52:48,498][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2025-03-04 14:52:48,595][__main__][INFO] - Instantiating callbacks...
[2025-03-04 14:52:48,596][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2025-03-04 14:52:48,600][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2025-03-04 14:52:48,601][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2025-03-04 14:52:48,602][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2025-03-04 14:52:48,604][__main__][INFO] - Instantiating loggers...
[2025-03-04 14:52:48,604][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2025-03-04 14:53:20,880][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2025-03-04 14:53:22,439][__main__][INFO] - Logging hyperparameters!
[2025-03-04 14:53:22,449][__main__][INFO] - Starting training!
[2025-03-04 14:53:29,853][src.utils.utils][ERROR] - 
Traceback (most recent call last):
  File "D:\pycharmproject\template\src\utils\utils.py", line 65, in wrap
    metric_dict, object_dict = task_func(cfg=cfg)
  File "D:\pycharmproject\template\srcC\train.py", line 83, in train
    trainer.fit(model=model, datamodule=datamodule, ckpt_path=cfg.get("ckpt_path"))
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 529, in fit
    call._call_and_handle_interrupt(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 42, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 568, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 973, in _run
    results = self._run_stage()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\trainer.py", line 1016, in _run_stage
    self.fit_loop.run()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 201, in run
    self.advance()
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\fit_loop.py", line 354, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 133, in run
    self.advance(data_fetcher)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\training_epoch_loop.py", line 218, in advance
    batch_output = self.automatic_optimization.run(trainer.optimizers[0], kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 185, in run
    self._optimizer_step(kwargs.get("batch_idx", 0), closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\loops\optimization\automatic.py", line 260, in _optimizer_step
    call._call_lightning_module_hook(
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\trainer\call.py", line 144, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\module.py", line 1256, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\core\optimizer.py", line 155, in step
    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\strategies\strategy.py", line 225, in optimizer_step
    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\lightning\pytorch\plugins\precision\precision_plugin.py", line 114, in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
  File "D:\Anaconda3\lib\site-packages\torch\optim\optimizer.py", line 280, in wrapper
    out = func(*args, **kwargs)
  File "D:\pycharmproject\template\srcC\optim\ys.py", line 65, in step
    grad_perturbation = agp_lambda * torch.autograd.grad(grad, p, retain_graph=True)[0]
  File "D:\Anaconda3\lib\site-packages\torch\autograd\__init__.py", line 303, in grad
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
[2025-03-04 14:53:29,862][src.utils.utils][INFO] - Output dir: D:\data\CIFAR10_logs\logs\train\runs\2025-03-04_14-52-48
[2025-03-04 14:53:29,862][src.utils.utils][INFO] - Closing wandb!
[2025-03-04 14:54:57,002][src.utils.utils][INFO] - Enforcing tags! <cfg.extras.enforce_tags=True>
[2025-03-04 14:54:57,009][src.utils.utils][INFO] - Printing config tree with Rich! <cfg.extras.print_config=True>
[2025-03-04 14:54:57,086][__main__][INFO] - Instantiating datamodule <srcC.data.CIFAR10_datamodule.CIFAR10DataModule>
[2025-03-04 14:54:57,092][__main__][INFO] - Instantiating model <srcC.models.CIFAR10_module.CIFAR10LitModule>
[2025-03-04 14:54:57,206][__main__][INFO] - Instantiating callbacks...
[2025-03-04 14:54:57,206][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2025-03-04 14:54:57,209][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.EarlyStopping>
[2025-03-04 14:54:57,210][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2025-03-04 14:54:57,212][src.utils.instantiators][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2025-03-04 14:54:57,214][__main__][INFO] - Instantiating loggers...
[2025-03-04 14:54:57,214][src.utils.instantiators][INFO] - Instantiating logger <lightning.pytorch.loggers.wandb.WandbLogger>
[2025-03-04 14:55:29,648][__main__][INFO] - Instantiating trainer <lightning.pytorch.trainer.Trainer>
[2025-03-04 14:55:31,022][__main__][INFO] - Logging hyperparameters!
[2025-03-04 14:55:31,029][__main__][INFO] - Starting training!
[2025-03-04 14:59:58,508][__main__][INFO] - Starting testing!
